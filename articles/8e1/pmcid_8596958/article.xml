<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" id="hbm25656" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Hum Brain Mapp</journal-id>
      <journal-id journal-id-type="iso-abbrev">Hum Brain Mapp</journal-id>
      <journal-id journal-id-type="doi">10.1002/(ISSN)1097-0193</journal-id>
      <journal-id journal-id-type="publisher-id">HBM</journal-id>
      <journal-title-group>
        <journal-title>Human Brain Mapping</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1065-9471</issn>
      <issn pub-type="epub">1097-0193</issn>
      <publisher>
        <publisher-name>John Wiley &amp; Sons, Inc.</publisher-name>
        <publisher-loc>Hoboken, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">34587333</article-id>
      <article-id pub-id-type="pmc">8596958</article-id>
      <article-id pub-id-type="doi">10.1002/hbm.25656</article-id>
      <article-id pub-id-type="publisher-id">HBM25656</article-id>
      <article-categories>
        <subj-group subj-group-type="overline">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="heading">
          <subject>Research Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A deep learning based approach identifies regions more relevant than resting‐state networks to the prediction of general intelligence from resting‐state <styled-content style="fixed-case" toggle="no">fMRI</styled-content>
</article-title>
        <alt-title alt-title-type="left-running-head">Hebling Vieira et al.</alt-title>
      </title-group>
      <contrib-group>
        <contrib id="hbm25656-cr-0001" contrib-type="author" corresp="yes">
          <name>
            <surname>Hebling Vieira</surname>
            <given-names>Bruno</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8770-7396</contrib-id>
          <xref rid="hbm25656-aff-0001" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="hbm25656-aff-0002" ref-type="aff">
<sup>2</sup>
</xref>
          <address>
            <email>bruno.hebling.vieira@usp.br</email>
          </address>
        </contrib>
        <contrib id="hbm25656-cr-0002" contrib-type="author">
          <name>
            <surname>Dubois</surname>
            <given-names>Julien</given-names>
          </name>
          <xref rid="hbm25656-aff-0003" ref-type="aff">
<sup>3</sup>
</xref>
          <xref rid="hbm25656-aff-0004" ref-type="aff">
<sup>4</sup>
</xref>
        </contrib>
        <contrib id="hbm25656-cr-0003" contrib-type="author">
          <name>
            <surname>Calhoun</surname>
            <given-names>Vince D.</given-names>
          </name>
          <xref rid="hbm25656-aff-0002" ref-type="aff">
<sup>2</sup>
</xref>
          <xref rid="hbm25656-aff-0005" ref-type="aff">
<sup>5</sup>
</xref>
          <xref rid="hbm25656-aff-0006" ref-type="aff">
<sup>6</sup>
</xref>
        </contrib>
        <contrib id="hbm25656-cr-0004" contrib-type="author" corresp="yes">
          <name>
            <surname>Garrido Salmon</surname>
            <given-names>Carlos Ernesto</given-names>
          </name>
          <xref rid="hbm25656-aff-0001" ref-type="aff">
<sup>1</sup>
</xref>
          <address>
            <email>garrido@ffclrp.usp.br</email>
          </address>
        </contrib>
      </contrib-group>
      <aff id="hbm25656-aff-0001">
<label>
<sup>1</sup>
</label>
<named-content content-type="organisation-division">InBrain Lab, Departamento de Física</named-content>
<institution>Universidade de São Paulo</institution>
<city>Ribeirão Preto</city>
<country country="BR">Brazil</country>
</aff>
      <aff id="hbm25656-aff-0002">
<label>
<sup>2</sup>
</label>
<institution>Tri‐Institutional Center for Translational Research in Neuroimaging and Data Science (TReNDS), Georgia State University, Georgia Institute of Technology, Emory University</institution>
<city>Atlanta</city>
<named-content content-type="country-part">Georgia</named-content>
<country country="US">USA</country>
</aff>
      <aff id="hbm25656-aff-0003">
<label>
<sup>3</sup>
</label>
<institution>Cedars‐Sinai Medical Center</institution>
<city>Los Angeles</city>
<named-content content-type="country-part">California</named-content>
<country country="US">USA</country>
</aff>
      <aff id="hbm25656-aff-0004">
<label>
<sup>4</sup>
</label>
<institution>Caltech</institution>
<city>Pasadena</city>
<named-content content-type="country-part">California</named-content>
<country country="US">USA</country>
</aff>
      <aff id="hbm25656-aff-0005">
<label>
<sup>5</sup>
</label>
<institution>The Mind Research Network</institution>
<city>Albuquerque</city>
<named-content content-type="country-part">New Mexico</named-content>
<country country="US">USA</country>
</aff>
      <aff id="hbm25656-aff-0006">
<label>
<sup>6</sup>
</label>
<named-content content-type="organisation-division">School of Electrical &amp; Computer Engineering</named-content>
<institution>Georgia Institute of Technology</institution>
<city>Atlanta</city>
<named-content content-type="country-part">Georgia</named-content>
<country country="US">USA</country>
</aff>
      <author-notes>
        <corresp id="correspondenceTo">
<label>*</label>
<bold>Correspondence</bold>
<break/>
Bruno Hebling Vieira and Carlos Ernesto Garrido Salmon, InBrain Lab, Departamento de Física, Universidade de São Paulo, Ribeirão Preto, Brazil.<break/>
Email: <email>bruno.hebling.vieira@usp.br</email> (B. H. V.) and <email>garrido@ffclrp.usp.br</email> (C. E. G. S.)<break/>
</corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>29</day>
        <month>9</month>
        <year>2021</year>
      </pub-date>
      <pub-date pub-type="collection">
        <day>15</day>
        <month>12</month>
        <year>2021</year>
      </pub-date>
      <volume>42</volume>
      <issue seq="80">18</issue>
      <issue-id pub-id-type="doi">10.1002/hbm.v42.18</issue-id>
      <fpage>5873</fpage>
      <lpage>5887</lpage>
      <history>
<date date-type="rev-recd"><day>26</day><month>8</month><year>2021</year></date>
<date date-type="received"><day>28</day><month>6</month><year>2021</year></date>
<date date-type="accepted"><day>27</day><month>8</month><year>2021</year></date>
</history>
      <permissions>
        <!--&#x000a9; 2021 Wiley Periodicals LLC.-->
        <copyright-statement content-type="article-copyright">© 2021 The Authors. <italic toggle="yes">Human Brain Mapping</italic> published by Wiley Periodicals LLC.</copyright-statement>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
        </license>
      </permissions>
      <self-uri content-type="pdf" xlink:href="file:HBM-42-5873.pdf"/>
      <abstract>
        <title>Abstract</title>
        <p>Prediction of cognitive ability latent factors such as general intelligence from neuroimaging has elucidated questions pertaining to their neural origins. However, predicting general intelligence from functional connectivity limit hypotheses to that specific domain, being agnostic to time‐distributed features and dynamics. We used an ensemble of recurrent neural networks to circumvent this limitation, bypassing feature extraction, to predict general intelligence from resting‐state functional magnetic resonance imaging regional signals of a large sample (<italic toggle="no">n</italic> = 873) of Human Connectome Project adult subjects. Ablating common resting‐state networks (RSNs) and measuring degradation in performance, we show that model reliance can be mostly explained by network size. Using our approach based on the temporal variance of saliencies, that is, gradients of outputs with regards to inputs, we identify a candidate set of networks that more reliably affect performance in the prediction of general intelligence than similarly sized RSNs. Our approach allows us to further test the effect of local alterations on data and the expected changes in derived metrics such as functional connectivity and instantaneous innovations.</p>
      </abstract>
      <abstract abstract-type="graphical">
        <p>We predict general intelligence from RS‐fMRI timeseries using a recurrent neural network ensemble in Human Connectome Project data. We propose the selection of networks based on the variance of saliencies per ROI. Resting‐state networks (RSNs) impact on prediction can be explained by their size while with our strategy we find salient networks whose importance exceed that of RSNs.<boxed-text position="anchor" content-type="graphic" id="hbm25656-blkfxd-0001"><graphic xlink:href="HBM-42-5873-g002.jpg" position="anchor" id="jats-graphic-1"/></boxed-text>
</p>
      </abstract>
      <kwd-group kwd-group-type="author-generated">
        <kwd id="hbm25656-kwd-0001">brain‐behavior</kwd>
        <kwd id="hbm25656-kwd-0002">deep learning</kwd>
        <kwd id="hbm25656-kwd-0003">fMRI</kwd>
        <kwd id="hbm25656-kwd-0004">intelligence</kwd>
        <kwd id="hbm25656-kwd-0005">resting‐state</kwd>
      </kwd-group>
      <funding-group>
        <award-group id="funding-0001">
          <funding-source>
            <institution-wrap>
              <institution>Coordenação de Aperfeiçoamento de Pessoal de Nível Superior
</institution>
              <institution-id institution-id-type="doi">10.13039/501100002322</institution-id>
            </institution-wrap>
          </funding-source>
          <award-id>Finance Code 001</award-id>
        </award-group>
        <award-group id="funding-0002">
          <funding-source>
            <institution-wrap>
              <institution>Fundação de Amparo à Pesquisa do Estado de São Paulo
</institution>
              <institution-id institution-id-type="doi">10.13039/501100001807</institution-id>
            </institution-wrap>
          </funding-source>
          <award-id>2017/02752‐0</award-id>
          <award-id>2018/11881‐1</award-id>
        </award-group>
      </funding-group>
      <counts>
        <fig-count count="9"/>
        <table-count count="1"/>
        <page-count count="15"/>
        <word-count count="9549"/>
      </counts>
      <custom-meta-group>
        <custom-meta>
          <meta-name>source-schema-version-number</meta-name>
          <meta-value>2.0</meta-value>
        </custom-meta>
        <custom-meta>
          <meta-name>cover-date</meta-name>
          <meta-value>December 15, 2021</meta-value>
        </custom-meta>
        <custom-meta>
          <meta-name>details-of-publishers-convertor</meta-name>
          <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.0.9 mode:remove_FC converted:17.11.2021</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
    <notes>
      <p content-type="self-citation">
<mixed-citation publication-type="journal" id="hbm25656-cit-9001">
<string-name>
<surname>Hebling Vieira</surname>, <given-names>B.</given-names>
</string-name>, <string-name>
<surname>Dubois</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Calhoun</surname>, <given-names>V. D.</given-names>
</string-name>, &amp; <string-name>
<surname>Garrido Salmon</surname>, <given-names>C. E.</given-names>
</string-name> (<year>2021</year>). <article-title>A deep learning based approach identifies regions more relevant than resting‐state networks to the prediction of general intelligence from resting‐state <styled-content style="fixed-case" toggle="no">fMRI</styled-content>
</article-title>. <source>Human Brain Mapping</source>, <volume>42</volume>(<issue>18</issue>), <fpage>5873</fpage>–<lpage>5887</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25656</pub-id>
</mixed-citation>
</p>
      <fn-group>
        <fn id="hbm25656-note-1002">
          <p>
<bold>Funding information</bold> Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, Grant/Award Number: Finance Code 001; Fundação de Amparo à Pesquisa do Estado de São Paulo, Grant/Award Numbers: 2017/02752‐0, 2018/11881‐1</p>
        </fn>
      </fn-group>
    </notes>
  </front>
  <body id="hbm25656-body-0001">
    <sec id="hbm25656-sec-0001">
      <label>1</label>
      <title>INTRODUCTION</title>
      <p>Intelligence is comprised of a number of distinct mental abilities. According to Colom, Karama, Jung, and Haier (<xref rid="hbm25656-bib-0010" ref-type="bibr">2010</xref>), “Reasoning, problem solving, and learning are crucial facets of human intelligence.” By means of factor analysis, a single factor was found to explain most of the empyrical positive correlation between tests (Spearman, <xref rid="hbm25656-bib-0037" ref-type="bibr">1904</xref>; Thurstone, <xref rid="hbm25656-bib-0041" ref-type="bibr">1940</xref>). Due to its generality, it was named “general intelligence,” “general factor,” or simply “g” (Jensen, <xref rid="hbm25656-bib-0024" ref-type="bibr">1998</xref>; Spearman, <xref rid="hbm25656-bib-0037" ref-type="bibr">1904</xref>). “G” was proposed as an underlying factor that dictates the overall cognitive performance of an individual and is latent to cognitive ability tests.</p>
      <sec id="hbm25656-sec-0002">
        <label>1.1</label>
        <title>Neural bases of intelligence</title>
        <p>The neural bases of intelligence constitute an open scientific question. Recent advances make neuroimaging a fundamental tool to answer this question. For a recent review, see Dizaji et al. (<xref rid="hbm25656-bib-0011" ref-type="bibr">2021</xref>).</p>
        <p>Intra cranial volume (ICV) and intelligence quotients are substantially correlated (Luders, Narr, Thompson, &amp; Toga, <xref rid="hbm25656-bib-0032" ref-type="bibr">2009</xref>). This phenomenon is reproducible in both sexes and across all age groups (McDaniel, <xref rid="hbm25656-bib-0033" ref-type="bibr">2005</xref>). In a meta‐analysis, approximately 10% of the variance of intelligence quotients can be accounted for by differences in brain volumetry alone (McDaniel, <xref rid="hbm25656-bib-0033" ref-type="bibr">2005</xref>). Yet, intelligence depends on verbal, visual‐processing, information encoding and retrieval and executive tasks (Luders et al., <xref rid="hbm25656-bib-0032" ref-type="bibr">2009</xref>). This evokes the importance of specialized regions and networks of the brain. While ICV or gray matter volume account for much of the variation in intelligence, the remaining variance might be explained by other neurobiological factors. Potential candidates include connectivity, neuroanatomy and microstructural properties, and metabolism. Indeed, including cortical gray matter thickness estimates and white matter hyperintensity loads almost doubled the explained variance of general intelligence compared to a model only accounting for brain volume in Ritchie et al. (<xref rid="hbm25656-bib-0036" ref-type="bibr">2015</xref>).</p>
        <p>Previous studies were used to formulate current theories on the neural bases of intelligence. The Parieto Frontal Integration Theory (P‐FIT) proposes the existence of a single network primarily subserving human intelligence, with substantial empirical evidence derived from neuroimaging (Jung &amp; Haier, <xref rid="hbm25656-bib-0025" ref-type="bibr">2007</xref>). The Network Neuroscience Theory (NNT), on the other hand, proposes that fluid and crystallized intelligence and specific skills emerge from the dynamic reorganization of networks (Barbey, <xref rid="hbm25656-bib-0004" ref-type="bibr">2018</xref>). This theory accommodates multiple networks and network dynamics, and relating specific network topologies to fluid and crystalized intelligence. For an overview on other competing theories, see Barbey (<xref rid="hbm25656-bib-0004" ref-type="bibr">2018</xref>).</p>
      </sec>
      <sec id="hbm25656-sec-0003">
        <label>1.2</label>
        <title>Functional magnetic resonance imaging and the biological importance of resting‐state functional connectivity</title>
        <p>Functional magnetic resonance imaging (fMRI) allows the study of cerebral neurophysiology. Even at rest, the brain stays functionally and metabolically active. From the resting‐state‐fMRI signal, it is possible to define resting‐state functional connectivity (RSFC), the temporal coupling between signals in anatomically distinct regions of the brain (Yeo et al., <xref rid="hbm25656-bib-0045" ref-type="bibr">2011</xref>).</p>
        <p>This connectivity between regions can be estimated from relatively simple measures, such as the Pearson correlation coefficients. RSFC alterations have been linked to multiple biological processes, such as brain disorders, aging, and cognition. More importantly, intelligence has also been found to correlate with RSFC and graph theoretical measures, such as local efficiency (Pamplona, Santos Neto, Rosset, Rogers, &amp; Salmon, <xref rid="hbm25656-bib-0035" ref-type="bibr">2015</xref>). More recently, fMRI‐derived data has been used to perform predictive analyses at the individual‐level (Sui, Liu, Lee, Zhang, &amp; Calhoun, <xref rid="hbm25656-bib-0039" ref-type="bibr">2020</xref>).</p>
        <p>Other than estimating temporal coupling between time series, it is possible to define spatially independent components of blood‐oxygen‐level‐dependent (BOLD) contrast fluctuations using data‐driven blind signal separation techniques such as independent component analysis (ICA; Calhoun, Adali, Pearlson, &amp; Pekar, <xref rid="hbm25656-bib-0009" ref-type="bibr">2001</xref>; Calhoun &amp; Adali, <xref rid="hbm25656-bib-0008" ref-type="bibr">2012</xref>). In addition to structured noise components, neural components have been robustly identified with ICA as resting‐state networks (RSNs), with large empirical evidence corroborating their existence. ICA‐derived RSN topologies found validation with other techniques, such as magnetoencephalography (Brookes et al., <xref rid="hbm25656-bib-0007" ref-type="bibr">2011</xref>) or community detection with discrete regions (Ito et al., <xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>). These RSNs are often linked to known cognitive, sensory, or motor processes. The connectivity or topology of RSNs is often used to deduce the cognitive effect of the alterations observed.</p>
      </sec>
      <sec id="hbm25656-sec-0004">
        <label>1.3</label>
        <title>Predicting “g” from resting‐state functional imaging</title>
        <p>Individualized intelligence estimation from RSFC based on machine learning is already in practice (Dubois, Galdi, Paul, &amp; Adolphs, <xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>; Finn et al., <xref rid="hbm25656-bib-0017" ref-type="bibr">2015</xref>). The most successful approaches to the prediction of intelligence are often based on linear modeling with univariate feature filtering. Even though it is not theoretically a required step, it was employed by most of the successful approaches (Dizaji et al., <xref rid="hbm25656-bib-0011" ref-type="bibr">2021</xref>). Under this type of model, when a feature is discarded it can no longer contribute with the predictions, no matter how much it is altered. This implausibility, compounded with our prior knowledge of how the brain works, motivate us to search for other approaches.</p>
        <p>The brain is a complex entity with complex dynamics. The interplay between these dynamics and biological processes such as intelligence can be presumed to be complex as well. The use of aggregate measures, for example, RSFC, can be used to inform highly predictive models with a window for direct interpretability. At the same time, that choice limits the hypotheses about the data. Deep learning can be used to learn predictive representations of data with automatic feature extraction (Abrol et al., <xref rid="hbm25656-bib-0002" ref-type="bibr">2021</xref>). Employing deep learning to learn about the biological bases of “g” opens up new possibilities for interpretation of results, removing the bias due to the choice of features. Human‐level interpretability is more challenging due to the increased complexity of the models. A few strategies exist, however, that allow for insights to be extracted from modeling. We explore two such strategies, namely feature ablation and saliency (Molnar, <xref rid="hbm25656-bib-0034" ref-type="bibr">2019</xref>).</p>
        <p>The effectiveness of deep learning in predictive analyses in cognitive neuroscience over “classical” machine‐learning methods, such as linear, kernel‐based, and tree‐based models, has been a topic of recent debate. While some studies show comparable performance between both paradigms (He et al., <xref rid="hbm25656-bib-0019" ref-type="bibr">2020</xref>), deep learning methods have been shown to outperform classic machine learning as well (Abrol et al., <xref rid="hbm25656-bib-0001" ref-type="bibr">2020</xref>). Deep learning models are able to explore existing nonlinearities in neuroimaging data and automatically extract informative features (Abrol et al., <xref rid="hbm25656-bib-0002" ref-type="bibr">2021</xref>). This allows deep models to often surpass traditional machine learning models in performance. Since highly flexible models are more prone to variance, we control their variance via ensembling, that is, averaging predictions networks trained independently on the same data.</p>
        <p>In this work, we aim to demonstrate that learning from lower level data, that is, timeseries instead of RSFC, can bring further insights into the question of the neuronal bases on intelligence. This type of data requires specialized models, and we opt to use an ensemble of recurrent neural networks (RNNs) for this task. We performed the prediction of “g” from fMRI timeseries obtained from a discrete cortical parcellation. To the best of authors' knowledge, this is the first work to perform this type of analysis. We then interpret the model predictions on unseen data performing ablation and saliency studies. We show that the ablation of single anatomical regions does not degrade performance and that the degradation in performance when ablating RSNs is not greater than when ablating random sets of regions with equal size. We calculate model reliance based on temporal variance of saliencies and show that, when ablating sets of regions ranked by this measure, significant degradation of performance ensues compared with the random sets. We then propagate saliencies to derivative measures and derive neuroscientific insights from the nature of observed saliencies.</p>
      </sec>
    </sec>
    <sec sec-type="methods" id="hbm25656-sec-0005">
      <label>2</label>
      <title>METHODS</title>
      <sec id="hbm25656-sec-0006">
        <label>2.1</label>
        <title>Data and preprocessing</title>
        <p>Original data were provided by the Human Connectome Project (HCP; Essen et al., <xref rid="hbm25656-bib-0015" ref-type="bibr">2013</xref>). Preprocessed and behavior data were provided by Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>). Briefly, test scores from 1,206 subjects were obtained. Tests include seven tasks from the NIH Toolbox for Assessment of Neurological and Behavioral function (dimensional change card sort; flanker inhibitory control and attention; list sorting working memory; picture sequence memory; picture vocabulary; pattern comparison processing speed; oral reading recognition) and three from the Penn Computerized Neurocognitive Battery (Penn progressive matrices; Penn word memory test; variable short Penn line orientation). Twenty‐three subjects with missing or incomplete test scores were excluded (<italic toggle="yes">n</italic> = 1,183). Two subjects that scored 26 or less in the Mini Mental State Examination (MMSE) were also excluded (<italic toggle="yes">n</italic> = 1,181). These subjects were available for factor analysis. Subjects that completed four imaging sessions (<italic toggle="yes">n</italic> = 998) were further filtered by excluding 114 subjects with excessive in‐scanner head movement (<italic toggle="yes">n</italic> = 884). Preprocessing included <italic toggle="yes">z</italic>‐standardization, removal of temporal drifts from white‐matter and CSF signals, regression of mean white‐matter and CSF signals from gray matter, regression of six realignment parameters and their first derivatives, low‐pass filtering with a Gaussian with a <italic toggle="yes">SD</italic> of 720 ms, removal of temporal drifts from the resulting gray matter signal, and finally global‐signal regression. Temporal drift removal was based on third‐degree Legendre polynomials. See Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>) for details. We further removed 11 subjects with less than 1,200 timepoints per session, achieving a final set of 873 subjects with complete imaging and behavioral data.</p>
        <p>Maximum likelihood factor analysis was performed in Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>) including one general factor and four lower‐order factors. The Schmid–Leiman transformation is used to derive loadings for the general factor. Individual scores are obtained by the Thurstone regression method.</p>
        <p>We applied band‐pass filtering (BPF) at [0.008 Hz, 0.09 Hz] to the ROI timeseries. This frequency band corresponds to the expected slow BOLD fluctuations, increasing signal specificity. This is due to the hemodynamic response function attenuation (Sun, Miller, &amp; D'Esposito, <xref rid="hbm25656-bib-0040" ref-type="bibr">2004</xref>), typically up to 0.15 Hz. The major drawback of BPF is that it possibly reduces sensitivity, since higher frequencies might still pertain to the task. Data were then decimated. This entails truncating the spectra at 0.09 Hz and then transforming data back to the time domain. We obtained 155 timepoints per session after decimation. Dividing the session length of 864 s by 155, this leads to a virtual TR of 5.57 s. This additional procedure reduces the data dimensionality without loss of information and removes the long‐range temporal autocorrelation induced by BPF. Individual sessions were kept separate.</p>
      </sec>
      <sec id="hbm25656-sec-0007">
        <label>2.2</label>
        <title>Neural network architecture</title>
        <p>We opted to build upon a simple RNN based on the long‐short term memory (LSTM) (Hochreiter &amp; Schmidhuber, <xref rid="hbm25656-bib-0021" ref-type="bibr">1997</xref>) module. The LSTM captures both short and long‐range information in sequences. It has been shown to work efficiently empirically, including applications in neuroimaging data (Dvornek, Ventola, Pelphrey, &amp; Duncan, <xref rid="hbm25656-bib-0013" ref-type="bibr">2017</xref>).</p>
        <p>To capture complex dynamics and also to better distribute gradients in the network, we used the BiLSTM module (Graves &amp; Schmidhuber, <xref rid="hbm25656-bib-0018" ref-type="bibr">2005</xref>). The BiLSTM consists of two LSTM layers applied in parallel to time‐distributed data. One of the layers transverse the data forward in time, while the other does the same backward. The activations of both are then combined at each timestep by adding both, in our case.</p>
        <p>Our architecture, represented in Figure <xref rid="hbm25656-fig-0001" ref-type="fig">1</xref> consists of two BiLSTM modules and a linear layer with identity activation. The first BiLSTM has 360 inputs (corresponding to each ROI defined in the atlas) and 256 outputs, while the second has 256 inputs and outputs. The activations of the latter are mean‐aggregated, resulting in a 256‐dimensional vector per timeseries. This representation is fed to the linear layer, resulting a single scalar output. Parameters are optimized so that the average value of these scalars best predict “g.” We implemented our architecture in the framework Flux (Innes, <xref rid="hbm25656-bib-0022" ref-type="bibr">2018</xref>), written completely in Julia (Bezanson, Edelman, Karpinski, &amp; Shah, <xref rid="hbm25656-bib-0006" ref-type="bibr">2017</xref>). It has 2,316,545 learnable parameters in total, 632,320 and 525,824 in each LSTM in the first layer and second layers, respectively, and 257 in the affine layer. Based on this architecture, we trained an ensemble of 50 networks trained independently with the same data configuration. Final prediction is obtained by averaging the prediction of each member of the ensemble. For comparison, the convolutional architecture for image classification AlexNet (Krizhevsky, Sutskever, &amp; Hinton, <xref rid="hbm25656-bib-0029" ref-type="bibr">2012</xref>) has almost 61 million trainable parameters.</p>
        <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0001">
          <label>FIGURE 1</label>
          <caption>
            <p>Illustrated architecture of two‐layer bidirectional recurrent neural networks. Matching colors signify shared weights. The inputs <mml:math id="jats-math-1" display="inline" overflow="scroll"><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math> are 360‐dimensional vectors of pseudo‐activity in each ROI. Every LSTM module outputs a 256‐dimensional vector. The outputs <mml:math id="jats-math-2" display="inline" overflow="scroll"><mml:msub><mml:mi>Y</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math> are scalar and are subsequently mean‐pooled. Parameters are optimized to predict “g”</p>
          </caption>
          <graphic xlink:href="HBM-42-5873-g001" position="anchor" id="jats-graphic-3"/>
        </fig>
      </sec>
      <sec id="hbm25656-sec-0008">
        <label>2.3</label>
        <title>Training</title>
        <p>During training, we decorrelated general intelligence from gender, age, brain volume, movement from each resting‐state session, and reconstruction algorithm version, in accordance to Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>). This procedure eliminates possible contributions of no interest from “g,” augmenting specificity. General intelligence was then rescaled to unit variance and centered to zero mean.</p>
        <p>We trained our architecture with backpropagation through time (BPTT; Williams &amp; Peng, <xref rid="hbm25656-bib-0043" ref-type="bibr">1990</xref>). In our learning scheme, each LSTM layer is fed a single batch timepoint, producing its respective output and LSTM cell state. In the next timestep, the layer is again fed the batch input and takes the previous cell state to produce its output. This is repeated until the end of the sequence, when we calculate gradients for the backward pass on the objective function and update the parameters of the network accordingly.</p>
        <p>We used mean‐pooling on the outputs of the forward pass during training. Each sequence had up to 20 timesteps removed during each batch optimization, both at the start and also at the end, providing some variability to the training scheme. The four sessions of resting‐state timecourses were fed separately and in random order for the forward pass of the network and were effectively treated as separate training samples. This subtle difference stimulates the network to obtain the best possible estimate from each session instead of vying to optimize the average between sessions.</p>
        <p>We trained the architecture for 50 epochs using the “ADAM” optimization algorithm (Kingma &amp; Ba, <xref rid="hbm25656-bib-0026" ref-type="bibr">2014</xref>) to perform update iterations. The learning rate was set to 0.0005. We also regularized gradient descent by employing Weight Decay (Krogh &amp; Hertz, <xref rid="hbm25656-bib-0030" ref-type="bibr">1992</xref>) with a decay constant equal to 0.0005. Due to the adaptive nature of “ADAM,” Weight Decay, and <mml:math id="jats-math-3" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="normal">ℒ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math> regularization do not coincide. We used decoupled Weight Decay (Loshchilov &amp; Hutter, <xref rid="hbm25656-bib-0031" ref-type="bibr">2019</xref>) to overcome that limitation.</p>
      </sec>
      <sec id="hbm25656-sec-0009">
        <label>2.4</label>
        <title>Validation</title>
        <p>We performed 10‐fold stratified cross‐validation. We stratified families based on the terciles of family‐averaged “g” and family size, with groups of families with 1, 2, and 3 or more members. This stratification was conducted in order to make validation folds more homogeneous. We computed folds from each stratum of families, and then recombined these folds across strata to ensure a homogenous distribution of “g” across folds. The number of families in each stratum is shown in Table <xref rid="hbm25656-tbl-0001" ref-type="table">1</xref>.</p>
        <table-wrap position="float" id="hbm25656-tbl-0001" content-type="TABLE">
          <label>TABLE 1</label>
          <caption>
            <p>Stratification of families into average “g” terciles and family size</p>
          </caption>
          <table frame="hsides" rules="groups">
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <thead valign="bottom">
              <tr style="border-bottom:solid 1px #000000">
                <th align="left" valign="bottom" rowspan="1" colspan="1"/>
                <th align="left" valign="bottom" rowspan="1" colspan="1"/>
                <th style="border-bottom:solid 1px #000000" align="left" colspan="3" valign="bottom" rowspan="1">Family size</th>
              </tr>
              <tr>
                <th align="left" valign="bottom" rowspan="1" colspan="1"/>
                <th align="left" valign="bottom" rowspan="1" colspan="1"/>
                <th align="left" valign="bottom" rowspan="1" colspan="1">1 (<italic toggle="yes">n</italic> = 115)</th>
                <th align="left" valign="bottom" rowspan="1" colspan="1">2 (<italic toggle="yes">n</italic> = 156)</th>
                <th align="left" valign="bottom" rowspan="1" colspan="1">3+ (<italic toggle="yes">n</italic> = 138)</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td rowspan="3" align="left" valign="top" colspan="1">
<bold>“g” terciles</bold>
</td>
                <td align="left" valign="top" rowspan="1" colspan="1">T1 (<italic toggle="yes">n</italic> = 135)</td>
                <td align="left" valign="top" rowspan="1" colspan="1">53</td>
                <td align="left" valign="top" rowspan="1" colspan="1">47</td>
                <td align="left" valign="top" rowspan="1" colspan="1">35</td>
              </tr>
              <tr>
                <td align="left" valign="top" rowspan="1" colspan="1">T2 (<italic toggle="yes">n</italic> = 135)</td>
                <td align="left" valign="top" rowspan="1" colspan="1">25</td>
                <td align="left" valign="top" rowspan="1" colspan="1">54</td>
                <td align="left" valign="top" rowspan="1" colspan="1">56</td>
              </tr>
              <tr>
                <td align="left" valign="top" rowspan="1" colspan="1">T3 (<italic toggle="yes">n</italic> = 139)</td>
                <td align="left" valign="top" rowspan="1" colspan="1">37</td>
                <td align="left" valign="top" rowspan="1" colspan="1">55</td>
                <td align="left" valign="top" rowspan="1" colspan="1">47</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>The forward pass during validation was virtually exactly the same as in training. The only exception is that outputs were generated by mean‐aggregating outputs from all timepoints within a session, and then averaged across sessions. The general intelligence of validation data was transformed in accordance to the transformation obtained for training data in each fold. This prevents leakage of validation data into training data. The procedure includes decorrelation of confounder variables, rescaling and centering.</p>
      </sec>
      <sec id="hbm25656-sec-0010">
        <label>2.5</label>
        <title>Performance‐evaluation, comparison, and model exploration</title>
        <p>We employed three performance metrics. The MSE (mean squared error) was used during training. To evaluate performance on validation data we computed the coefficient of determination <mml:math id="jats-math-4" display="inline" overflow="scroll"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>MSE</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mi>Var</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mfenced></mml:math>. We also report the squared correlation coefficient <mml:math id="jats-math-5" display="inline" overflow="scroll"><mml:msup><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>Cor</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:math>, to allow for comparison with other works that used this metric exclusively. <mml:math id="jats-math-6" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">y</mml:mi></mml:math> and <mml:math id="jats-math-7" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math> are vectors of true values and predictions, respectively.</p>
        <p>We implemented elastic‐net regression to serve as an alternative model for comparison, predicting “g” from the RSFC, averaged between sessions. See Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>) for more details. Input data were filtered in univariate fashion at each fold. This procedure is based on the correlation between each input feature and “g” for training data. We set the parameter balancing the LASSO and ridge penalties to <mml:math id="jats-math-8" display="inline" overflow="scroll"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>.05</mml:mn></mml:math>. This leads to almost pure ridge regularization. The parameter <mml:math id="jats-math-9" display="inline" overflow="scroll"><mml:mi>λ</mml:mi></mml:math> that controls the tradeoff between the loss function and the penalization was optimized in an inner three‐fold cross‐validation. This inner optimization was based solely on training data at each outer fold. <mml:math id="jats-math-10" display="inline" overflow="scroll"><mml:mi>λ</mml:mi></mml:math> was optimized based on a grid with 50 values. This whole model was validated on the same fold configuration mentioned previously in Section <xref rid="hbm25656-sec-0009" ref-type="sec">2.4</xref>.</p>
        <p>To further understand how the ensemble model behaves, we performed two model‐agnostic exploration strategies: ablation and saliency (Molnar, <xref rid="hbm25656-bib-0034" ref-type="bibr">2019</xref>). This allows us to understand what information the model relies on to reach the performance we assessed in cross‐validation. Both techniques were applied to validation data.</p>
      </sec>
      <sec id="hbm25656-sec-0011">
        <label>2.6</label>
        <title>Model exploration: ablation study on validation sets</title>
        <p>We performed an ablation study on the trained model. Ablation consists in removing information from input and assessing the respective degradation in performance. We ablated anatomically‐defined atlas regions and entire functional networks. This procedure should be able to discern model reliance on individual features and combinations of features. For the networks, we used the network definition established by Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>), which is based on the same MMP cortical atlas we employed.</p>
        <p>To account for the fact that networks have different sizes, we compared the statistics obtained with a distribution of statistics. This distribution was obtained from resampling random networks with matching sizes to the one being tested. This way, we can more certainly state that a change in performance after ablation is not simply due to the removal of nodes. The procedure consists of, given a network with <mml:math id="jats-math-11" display="inline" overflow="scroll"><mml:mi>M</mml:mi></mml:math> nodes, selecting <mml:math id="jats-math-12" display="inline" overflow="scroll"><mml:mi>M</mml:mi></mml:math> nodes at random to be ablated. The statistics associated with the performance are stored and the procedure is repeated for 300 iterations, in our case. We extracted the paired <mml:math id="jats-math-13" display="inline" overflow="scroll"><mml:mi>T</mml:mi></mml:math>‐statistics, comparing the average performance across the 10 folds with and without ablation, with a null‐hypothesis of zero difference. We then define a <italic toggle="yes">p</italic>‐value for each hypothesis tested as the proportion of resampled statistics from the pool that are more extreme than the statistic measured empirically. We used <mml:math id="jats-math-14" display="inline" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>.05</mml:mn></mml:math> as a significance threshold throughout, after correcting for multiple comparisons at the analysis level with the false discovery rate controlling procedure described in Yekutieli and Benjamini (<xref rid="hbm25656-bib-0044" ref-type="bibr">1999</xref>).</p>
      </sec>
      <sec id="hbm25656-sec-0012">
        <label>2.7</label>
        <title>Model exploration: saliency analysis on validation sets</title>
        <p>Saliency measures the degree that a measure is influenced locally by perturbations. It is often defined as the partial derivative of the outputs of a model on its inputs. We chose to study the saliency of the trained models to understand what features of brain resting‐state activity are related to “g.” Since our model is fully differentiable, we can easily compute this measure. For each fold, we obtained the saliencies <mml:math id="jats-math-15" display="inline" overflow="scroll"><mml:msub><mml:mi>∂</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi></mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:math> based on validation, that is, unseen, data. The saliencies have the same dimensionality of input data since our output is scalar.</p>
        <p>Due to efficiency reasons, mean centering and unit scaling and temporal filtering are often performed outside cross‐validation per timeseries. When analyzing saliencies, however, we must take into account this normalization, as it is a constituent part of our model. Otherwise, we might observe, for example, a non‐null gradient at frequency zero, which is not possible under our model, as it assumes all data was standardized. We used the chain rule to propagate derivatives to standardized filtered data <mml:math id="jats-math-16" display="inline" overflow="scroll"><mml:msub><mml:mi>∂</mml:mi><mml:mi mathvariant="bold-italic">S</mml:mi></mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>∂</mml:mi><mml:mi mathvariant="bold-italic">S</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>∂</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi></mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:math>. For more details, see Appendix A. In practice, however, the procedure is performed automatically using automatic differentiation.</p>
        <p>Since we take into account standardization, average saliency of a region is zero across time, by design. We instead summarize reliance on regional timeseries by the temporal variance of saliency, defined, for a region timeseries <mml:math id="jats-math-17" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>, as <mml:math id="jats-math-18" display="inline" overflow="scroll"><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>∂</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub></mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math>.</p>
      </sec>
      <sec id="hbm25656-sec-0013">
        <label>2.8</label>
        <title>Ablating a network defined by the regions with highest saliency</title>
        <p>Regions with high average squared saliency should be approximately the regions the model relies on more for local changes in the estimate of “g.” Thus, it makes sense to use that as a proxy of model reliance, per region. Using the temporal variance of saliency per region, we define 12 networks that have each the size of the 14 RSNs defined in Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>). For each of these “saliency‐based networks” we perform the ablation and permutation study described in Section <xref rid="hbm25656-sec-0011" ref-type="sec">2.6</xref>. To avoid circular analysis, that is, “double‐dipping,” we cross‐validate this procedure. We employ saliencies from training data to define which regions are ablated in validation data.</p>
      </sec>
      <sec id="hbm25656-sec-0014">
        <label>2.9</label>
        <title>Functional connectivity saliency</title>
        <p>Apart from the original data, we also explored saliencies on functional connectivity. Given that saliencies denote a change in input data, we can propagate this to functional connectivity. Since our saliencies preserve the mean and variance of data, we can plug them directly into the definition of functional connectivity. Equation (<xref rid="hbm25656-disp-0001" ref-type="disp-formula">1</xref>) shows the update operator for <mml:math id="jats-math-19" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> and, likewise, <mml:math id="jats-math-20" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math>.<disp-formula id="hbm25656-disp-0001">
<label>(1)</label>
<mml:math id="jats-math-21" display="block" overflow="scroll"><mml:mfenced open="{" close=""><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>'</mml:mo><mml:mo>≔</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>'</mml:mo><mml:mo>≔</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:mi>M</mml:mi></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>M</mml:mi></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac><mml:mi>T</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>≈</mml:mo><mml:mfrac><mml:mi>η</mml:mi><mml:mi>M</mml:mi></mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math>
</disp-formula>In Equation (<xref rid="hbm25656-disp-0001" ref-type="disp-formula">1</xref>), since <mml:math id="jats-math-22" display="inline" overflow="scroll"><mml:mi>η</mml:mi><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:math>, the third term of <mml:math id="jats-math-23" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>'</mml:mo></mml:math> is, again, very small. Therefore, we focus on the second term, <mml:math id="jats-math-24" display="inline" overflow="scroll"><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi></mml:math>. This local change in connectivity is proportional to the covariance between the data and the saliencies. <mml:math id="jats-math-25" display="inline" overflow="scroll"><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">C</mml:mi></mml:math> is the associated alteration in connectivity from changing activity to increasing the “g” estimates in the model.</p>
      </sec>
      <sec id="hbm25656-sec-0015">
        <label>2.10</label>
        <title>Propagating saliency to decorrelated inputs through the zero‐phase component analysis</title>
        <p>When investigating saliency on the original data, one can incorporate possible data generating processes. We assume that RSFC generates the data acting on instantaneous innovations in each region. This can be described as <mml:math id="jats-math-26" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">ZW</mml:mi></mml:math>. <mml:math id="jats-math-27" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> is the centered and unit‐scaled data, where each column is a region and each row is a timepoint, <mml:math id="jats-math-28" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> represents the (uncorrelated) innovations and has the same size of <mml:math id="jats-math-29" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> and <mml:math id="jats-math-30" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math> is a square dewhitening matrix, thus <mml:math id="jats-math-31" display="inline" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">ZW</mml:mi></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:mfenced open="(" close=")"><mml:mi mathvariant="bold-italic">ZW</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math>, because <mml:math id="jats-math-32" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi></mml:math>. <mml:math id="jats-math-33" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">W</mml:mi></mml:math> is then proportional to the square‐root of the correlation matrix, <mml:math id="jats-math-34" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>, thus <mml:math id="jats-math-35" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">XC</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>. Taking the singular value decomposition (SVD) of <mml:math id="jats-math-36" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> results in <mml:math id="jats-math-37" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">USV</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>, then the inverse square root of the correlation is simply <mml:math id="jats-math-38" display="inline" overflow="scroll"><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">VS</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math>. Therefore, we can estimate <mml:math id="jats-math-39" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">XW</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">XVS</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">UV</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>.</p>
        <p>This procedure, called zero‐phase components analysis (ZCA), was described in Krizhevsky (<xref rid="hbm25656-bib-0028" ref-type="bibr">2009</xref>). It results in decorrelated variables that best correspond to the original ones. Or, in other words, each column of <mml:math id="jats-math-40" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> is proportional to the residuals of a least‐squares linear regression that predicts the timeseries of a region based on all other regions. Propagating saliencies to <mml:math id="jats-math-41" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> is then a matter of propagating saliencies through the SVD. For details, refer to Appendix B.</p>
        <p>We propagate saliencies in all subjects, sessions, and folds to <mml:math id="jats-math-42" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> using automatic differentiation. Studying <mml:math id="jats-math-43" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> allows us to understand the model reliance on regional timeseries beyond RSFC. Regions with high temporal variance in <mml:math id="jats-math-44" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> might have low temporal variance in <mml:math id="jats-math-45" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> because their activity is propagated to the rest of the network due to connectivity. Conversely, regions with high temporal variance in <mml:math id="jats-math-46" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> might have low temporal variance in <mml:math id="jats-math-47" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math>, because the model relies on them due to their shared information with other, more informative, regions. Note that, <mml:math id="jats-math-48" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">VS</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">C</mml:mi></mml:math>. If we applied the same SVD differential identities presented here to <mml:math id="jats-math-49" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">C</mml:mi></mml:math>, the same result in Equation (<xref rid="hbm25656-disp-0001" ref-type="disp-formula">1</xref>) would be obtained.</p>
      </sec>
    </sec>
    <sec sec-type="results" id="hbm25656-sec-0016">
      <label>3</label>
      <title>RESULTS</title>
      <sec id="hbm25656-sec-0017">
        <label>3.1</label>
        <title>Confounder variables explain substantial variance of “g”</title>
        <p>As part of our framework, we predict “g” in validation data across folds using the coefficients estimated from training data and perform additional analyses on the residuals of this model. We obtain <mml:math id="jats-math-50" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.138</mml:mn></mml:math>, <mml:math id="jats-math-51" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0279</mml:mn></mml:math> across folds. These confounders include gender, age, brain volume, movement from each resting‐state session, and reconstruction algorithm version. This result is in line with previous works (Dubois et al., <xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>).</p>
      </sec>
      <sec id="hbm25656-sec-0018">
        <label>3.2</label>
        <title>Penalized linear modeling predicts “g” from <styled-content style="fixed-case" toggle="no">RSFC</styled-content>
</title>
        <p>To be able to compare results we implemented the modeling approach presented in Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>) using our validation scheme. Using this approach, we obtained <mml:math id="jats-math-52" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.170</mml:mn></mml:math>, <mml:math id="jats-math-53" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0264</mml:mn></mml:math>, and <mml:math id="jats-math-54" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.178</mml:mn></mml:math>, <mml:math id="jats-math-55" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0242</mml:mn></mml:math> across folds.</p>
      </sec>
      <sec id="hbm25656-sec-0019">
        <label>3.3</label>
        <title>
<styled-content style="fixed-case" toggle="no">RS‐fMRI</styled-content> timeseries predict “g”</title>
        <p>Using our ensemble, we obtained <mml:math id="jats-math-56" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.184</mml:mn></mml:math>, <mml:math id="jats-math-57" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0149</mml:mn></mml:math> and <mml:math id="jats-math-58" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.194</mml:mn></mml:math>, <mml:math id="jats-math-59" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0170</mml:mn></mml:math> across folds on validation data.</p>
        <p>If we were to add the penalized linear model to the ensemble we obtain <mml:math id="jats-math-60" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.197</mml:mn></mml:math>, <mml:math id="jats-math-61" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0184</mml:mn></mml:math> and <mml:math id="jats-math-62" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.206</mml:mn></mml:math>, <mml:math id="jats-math-63" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0203</mml:mn></mml:math>. This small increase in cross‐validated performance is due to the high shared variance between predictions, <mml:math id="jats-math-64" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.622</mml:mn></mml:math>, <mml:math id="jats-math-65" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0193</mml:mn></mml:math>. For comparative purposes, analogous results are obtained when not removing the effect of confounder variables. The performance of the baseline model and the deep ensemble equal <mml:math id="jats-math-66" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.183</mml:mn></mml:math>, <mml:math id="jats-math-67" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0286</mml:mn></mml:math>, and <mml:math id="jats-math-68" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.204</mml:mn></mml:math>, <mml:math id="jats-math-69" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0178</mml:mn></mml:math>, respectively. Both models sharing <mml:math id="jats-math-70" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>mean</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.621</mml:mn></mml:math>, <mml:math id="jats-math-71" display="inline" overflow="scroll"><mml:msubsup><mml:mi>ρ</mml:mi><mml:mtext>stderr</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.0609</mml:mn></mml:math> variance.</p>
        <p>The impact of ensemble size on validation performance is shown in fig:ensemblesizeperformance.</p>
      </sec>
      <sec id="hbm25656-sec-0020">
        <label>3.4</label>
        <title>Ablation of single regions does not affect performance</title>
        <p>We show in Figure 3 how removing one region at a time from the validation data affects performance. Colors represent each network assignment defined in Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>).</p>
      </sec>
      <sec id="hbm25656-sec-0021">
        <label>3.5</label>
        <title>Ablation of RSNs alter performance; their size explains this effect</title>
        <p>We also performed an ablation study deleting one RSN at a time. This is shown in Figure 4. We assessed significance with the paired <italic toggle="yes">T</italic>‐test. We compared the average performance across folds in the baseline with the average performance after ablating each RSN. Results were corrected for multiple comparisons using the procedure described in Yekutieli and Benjamini (<xref rid="hbm25656-bib-0044" ref-type="bibr">1999</xref>).</p>
        <p>However, resampling the distribution of <italic toggle="yes">t</italic>‐statistics with random, equally‐sized networks, we did not observe any significant effect in any performance metric. This implies that the alterations in performance observed in Figure 4 can be explained by the size of RSNs alone.</p>
      </sec>
      <sec id="hbm25656-sec-0022">
        <label>3.6</label>
        <title>Peak regional saliency variance describes a network that when ablated significantly deteriorates model performance</title>
        <p>Figure 5 shows 12 networks, each matching in size the 14 RSNs defined in Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>). These networks are defined from regions that exhibit the largest temporal variance of saliencies across subjects and sessions. Since they are obtained from training data occurrence differs across folds. Only 12 networks are defined because two networks have two regions (PCC and PREM2) and two networks have 15 regions (AUD1 and HIPP). The bigger networks contain all the regions in the smaller ones, that is, smaller networks are subsets of bigger ones.</p>
        <p>When comparing the degradation in performance measured by <mml:math id="jats-math-72" display="inline" overflow="scroll"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math>, after correcting for multiple comparisons, most saliency based networks degraded performance significantly more than equally sized random networks. This is shown in Figure 6.</p>
      </sec>
      <sec id="hbm25656-sec-0023">
        <label>3.7</label>
        <title>Increasing intelligence estimates accompany expected alterations in functional connectivity</title>
        <p>Figure 7 shows the expected value of the change in connectivity <mml:math id="jats-math-73" display="inline" overflow="scroll"><mml:mi mathvariant="italic">δC</mml:mi></mml:math> that accompanies increasing “g.” As in Ferreira et al. (<xref rid="hbm25656-bib-0016" ref-type="bibr">2016</xref>) we choose to categorize saliencies with regards to their average magnitude and the direction of change to ease visualization. Only the highest 5% effects, defined as the average saliencies divided by the respective <italic toggle="yes">SD</italic>, are shown. No correlation was found between the values of average <mml:math id="jats-math-74" display="inline" overflow="scroll"><mml:mi mathvariant="italic">δC</mml:mi></mml:math> and average <mml:math id="jats-math-75" display="inline" overflow="scroll"><mml:mi>C</mml:mi></mml:math>.</p>
      </sec>
      <sec id="hbm25656-sec-0024">
        <label>3.8</label>
        <title>Saliency on whitened timeseries <italic toggle="no">Z</italic> demonstrates the weight of <styled-content style="fixed-case" toggle="no">RSFC</styled-content> on model reliance</title>
        <p>Figure 8 shows the average temporal variance of saliency obtained from whitened data, based on ZCA. Regional contributions are lessened due to the removal of functional connectivity, an expected result.</p>
      </sec>
    </sec>
    <sec id="hbm25656-sec-0025">
      <label>4</label>
      <title>DISCUSSION AND CONCLUSION</title>
      <p>Performance of the ensemble is on par with reported in Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>). Here we employ more restrictive data, including decimation, which should increase specificity at the cost of sensitivity. Training the ensemble with leave‐one‐family‐out cross‐validation would increase computational costs prohibitively. We thus employ 10‐fold CV, which entails less training data in each resample, but displays less variance than leave‐one‐out cross‐validation at the same time (Kohavi &amp; Edu, <xref rid="hbm25656-bib-0027" ref-type="bibr">1993</xref>). Both effects can be noted by the slightly deteriorated performance we observe when validating the penalized linear model. In our data, we retrieve <mml:math id="jats-math-76" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>−</mml:mo><mml:mtext>foldCV</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.170</mml:mn></mml:math>, compared to <mml:math id="jats-math-77" display="inline" overflow="scroll"><mml:msubsup><mml:mi>R</mml:mi><mml:mtext>LOFO</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.206</mml:mn></mml:math> reported in Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>). Increasing the training set size would increase performance for both the ensemble and the penalized linear model.</p>
      <p>Having said that, a modest 8% increase in performance is noted when using our model. This suggests we successfully captured the effects of interest. Given the increased complexity of our model it becomes clear we are possibly reaching the point of capturing most information available in the data. Future works could explore data sampled at other spatial granularities, culminating in using the voxel timeseries themselves. Other timescales can be explored, either fast changing neural oscillations afforded by techniques such as MEG and EEG, or slow changes in activity patterns with longitudinal data.</p>
      <p>We chose to ensemble RNNs to control their variance. Figure <xref rid="hbm25656-fig-0002" ref-type="fig">2</xref> demonstrates that we reach a plateau on performance long before summing 50 models in our ensemble. Roughly, 10 models achieve peak performance in our scenario.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Effect of ensemble size on cross‐validated average <italic toggle="yes">R</italic>
<sup>2</sup>. Ribbon shows the <italic toggle="yes">SE</italic>. Ensemble members were added consecutively in the same order of training</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g007" position="anchor" id="jats-graphic-5"/>
      </fig>
      <p>We choose to assess importance on validation data. An equally valid choice would have been studying training data prior to learning. Dubois et al. (<xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>) applies this approach to study RSNs. For a discussion, see Molnar (<xref rid="hbm25656-bib-0034" ref-type="bibr">2019</xref>).</p>
      <p>As expected, Figure <xref rid="hbm25656-fig-0003" ref-type="fig">3</xref> shows that the removal of no single region is sufficient to substantially disrupt model performance on validation data. This suggests that, as other works have shown, intelligence is distributed across different regions in the brain (Dubois et al., <xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>). This result is compatible with the initial premises of current theories, such as P‐FIT and NNT.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0003">
        <label>FIGURE 3</label>
        <caption>
          <p>Effect of regional ablation on cross‐validated average <italic toggle="yes">R</italic>
<sup>2</sup>. Ribbon shows the standard error. The black line represents the performance measured when using all data. Colors represent each resting state network assignment defined in Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>)</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g008" position="anchor" id="jats-graphic-7"/>
      </fig>
      <p>When looking into RSNs, as shown in Figure <xref rid="hbm25656-fig-0004" ref-type="fig">4</xref>, removing one of the visual, somato‐motor, cingulo‐opercular network (CON), default mode network (DMN), or auditory networks significantly lowers performance on the validation set. These networks have been reported before as important networks for intelligence. However, some of these networks are also among the biggest in the atlas in Ito et al. (<xref rid="hbm25656-bib-0023" ref-type="bibr">2017</xref>). This led us to investigate a direct effect of the amount of information being removed. We found no significant difference in the decrease in performance to the one obtained with resampled random networks of the same size. This means that we have no evidence that the ablation of RSNs reveals specific reliance on these networks for prediction. Rather, the size of RSNs explains this effect.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0004">
        <label>FIGURE 4</label>
        <caption>
          <p>Effect of network ablation on cross‐validated average <italic toggle="yes">R</italic>
<sup>2</sup>. Error bars represent the <italic toggle="yes">SE</italic>. The black line represents the performance measured when using all data. Significance was assessed with the paired <italic toggle="yes">T</italic>‐test, comparing performance in the baseline with the ablation of each network across folds. The Benjamini–Hochberg procedure was employed to correct for multiple comparisons</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g003" position="anchor" id="jats-graphic-9"/>
      </fig>
      <p>As exposed in Section <xref rid="hbm25656-sec-0005" ref-type="sec">2</xref>, by design saliencies sum to zero and their variance is such that, locally, it cancels their covariance with data. This is important for their validity regarding the full model. It also precludes us from exploring such simple measures as average saliency.</p>
      <p>Results point to the fact that saliency is heterogeneous across the cortex. Additionally, we did not detect bilateral patterns. This could be due to redundancies in information between homotopic regions. Ventral visual areas display low saliency overall. Wernicke's and Broca's areas are highlighted with high saliency temporal variance.</p>
      <p>The role of DMN deactivation in cognition has been studied elsewhere. See Anticevic et al. (<xref rid="hbm25656-bib-0003" ref-type="bibr">2012</xref>) for a review.</p>
      <p>To define a measure of regional reliance, we employ the temporal variance of saliencies. Saliencies can be interpreted as the change in data that leads to an increase in the output. Thus, regions with high temporal variance of saliencies are also the ones being most altered.</p>
      <p>Ablation of the “networks” shown in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref> demonstrated significant reductions in performance when compared with resampled random networks, shown in Figure <xref rid="hbm25656-fig-0006" ref-type="fig">6</xref>. These networks were obtained by keeping only the regions with highest average saliency temporal variance in cross‐validated fashion. Anatomically, the selected regions cover many parts of the brain. Many regions in the frontal lobe are included, including orbitofrontal, ventrolateral, and dorsolateral prefrontal and the pregenual anterior cingulate. Ventral occipital and ventral temporal areas are present in both hemispheres, as well as ventral anterior insula regions and inferior parietal regions. Dorsomedial frontal and posterior temporal regions are selected in the right hemisphere but lacking in the left. In the left, superior temporal regions are featured prominently. Very few regions have their homotopic counterpart featured.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0005">
        <label>FIGURE 5</label>
        <caption>
          <p>Regions with top saliency temporal variance in training data across folds. Occurrence goes from 0 to 10. This selection is used to ablate regions in validation data. The number of regions encompassed by the “networks” is shown in white next to each row</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g004" position="anchor" id="jats-graphic-11"/>
      </fig>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0006">
        <label>FIGURE 6</label>
        <caption>
          <p>Comparison of degradation in performance by ablation of networks. Paired <italic toggle="yes">T</italic>‐test statistic comparing ablated results with original performance across folds. The gray ribbon represents a randomization‐based distribution using random networks with the same number of nodes of corresponding RSNs. Downward‐pointing triangles represent the test statistics obtained when ablating networks defined in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref>. Upward‐pointing triangles represent the test statistics obtained when ablating RSNs. Significant effects when compared with the randomization‐based distribution are shown in green. Nonsignificant results are shown in red. RSNs are ordered largest to smallest, from left to right</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g006" position="anchor" id="jats-graphic-13"/>
      </fig>
      <p>These regions populate several RSNs. All regions of the DMN are included with the exception of the angular gyrii. Fronto‐parietal network (FPN), ventral visual and ventral and dorsal attention, right medial somatomator regions, and left auditory regions are contemplated. Several regions, however, lie on the frontier between major networks. We might speculate that the individual extent of RSNs could contribute to the prediction of “g.”</p>
      <p>The selections of networks in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref> include prominent P‐FIT areas. Notably, prefrontal, parietal, and temporal associative areas are included. However, bilateral visual associative regions are missing. We also do not verify any prominence of the left hemisphere over the right one, as theorized in the P‐FIT. This can be an artifact of the number of regions selected or the interindividual variability in functional localization. Of particular interest is the confounding effect of information shared through connectivity. This effect will be further discussed below.</p>
      <p>Comparing the highly modularized functional connectivity between RSNs with the average functional connectivity saliency shown in Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7</xref>, it is clear that the latter does not share the same modules as the first. This implies no simple alignment with intra‐ or internetwork connectivity. Instead, we observe that a small number of regions exhibit marked saliency to connectivity with specific networks. These are easily spotted as blue or orange lines in Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7</xref>, respectively average increasing and decreasing of functional connectivity with increasing “g.” In the case of increasing connectivity with a whole network, this would indicate that a region is more integrated with that network with increasing “g.” Likewise with decreasing functional connectivity, said region would be more segregated from that specific network. The CON exhibits marked loss of intranetwork connectivity with increasing “g.” Some visual, DMN, and FPN regions show uniform decreasing connectivity to it as well.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0007">
        <label>FIGURE 7</label>
        <caption>
          <p>Average derivative of functional connectivity in the direction of increasing “g,” standardized by the <italic toggle="yes">SD</italic> across samples. (a) Increased magnitude of positive correlations (IMPC). (b) Decreased magnitude of positive correlations (DMPC). (c) Decreased magnitude of negative correlations (DMNC). (d) Increased magnitude of negative correlations (IMNC). Highest 5% effects are shown. Opacity increases with effect magnitude</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g009" position="anchor" id="jats-graphic-15"/>
      </fig>
      <p>Comparing the top row with the bottom row in Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7</xref>, it becomes evident that, among the 5% highest effects, positive correlations are more prevalent than negative correlations. Positive correlations are expected to occur in intranetwork connections, while negative correlations occur in internetwork connections. In Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7a</xref>, DMN and FPN regions show evidence of increased coupling in the direction of increasing “g.” In Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7b</xref>, on the other hand, other DMN and FPN regions become decoupled. In the latter, CON intranetwork connections become weaker in the direction of increasing “g.” In the case of negative correlation shown in Figure <xref rid="hbm25656-fig-0007" ref-type="fig">7c,d</xref>, which occur mostly between RSNs, bilateral CON, DMN, and FPN connectivities are highlighted. The role of networks such as the CON, DMN, and FPN RSNs in intelligence was previously studied (Dubois et al., <xref rid="hbm25656-bib-0012" ref-type="bibr">2018</xref>; Hearne, Mattingley, &amp; Cocchi, <xref rid="hbm25656-bib-0020" ref-type="bibr">2016</xref>). These three networks, in particular, are coupled: the CON is associated with switching levels of activity between DMN and FPN (Sridharan, Levitin, &amp; Menon, <xref rid="hbm25656-bib-0038" ref-type="bibr">2008</xref>).</p>
      <p>After disambiguating the role of spontaneous activity, that is, temporal innovations, in the saliencies, other regions emerge in Figure <xref rid="hbm25656-fig-0008" ref-type="fig">8</xref>. In special, the inferior parietal lobules have high saliency in this regard, including the angular gyri and the supramarginal gyri. These two regions are involved in language streams and several higher order cognitive functions. They are directly connected to the frontal lobe by the superior longitudinal fasciculus, including dorsolateral prefrontal cortices. P‐FIT places high importance to this area of the cortex, especially the left angular gyrus (Jung &amp; Haier, <xref rid="hbm25656-bib-0025" ref-type="bibr">2007</xref>). Again, the left primary auditory cortex has high saliency, the sole primary sensorial region to be so, which is also accommodated within P‐FIT.</p>
      <fig position="float" fig-type="FIGURE" id="hbm25656-fig-0008">
        <label>FIGURE 8</label>
        <caption>
          <p>Average temporal variance on saliency calculated over whitened data, that is, multivariate uncorrelated innovations</p>
        </caption>
        <graphic xlink:href="HBM-42-5873-g005" position="anchor" id="jats-graphic-17"/>
      </fig>
      <p>On the other hand, areas such as the insulas, parahippocampal and entorhinal cortices, and the subgenual area are less pronounced in Figure <xref rid="hbm25656-fig-0008" ref-type="fig">8</xref>. These areas are absent in P‐FIT (Jung &amp; Haier, <xref rid="hbm25656-bib-0025" ref-type="bibr">2007</xref>), further corroborating our results.</p>
      <p>We can hypothesize that regions that were highlighted in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref> but not in Figure <xref rid="hbm25656-fig-0008" ref-type="fig">8</xref> could attain high saliency due to their connectivity. The converse is also true, where regions highlighted in Figure <xref rid="hbm25656-fig-0008" ref-type="fig">8</xref> but not in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref>, under this hypothesis, would be so due to their connectivity.</p>
      <p>The smallest “networks” containing two or four regions do not display significance in Figure <xref rid="hbm25656-fig-0006" ref-type="fig">6</xref>. Low concordance was obtained across folds for these, as can be seen in Figure <xref rid="hbm25656-fig-0005" ref-type="fig">5</xref>. This adds to the point that intelligence cannot be attributed to single regions. It is very likely that the choice of “networks” that degrade performance more than random, equally‐sized, ones is not unique. While other “networks” can possibly attain the same effect, the point is that RSNs are not, in fact, specific to intelligence.</p>
      <p>Our model has enough parameters to approximately interpolate training data. This does not lead to overfitting, as can be seen in the results, however. This might be due to the double‐descent phenomenon (Belkin, Hsu, &amp; Mitra, <xref rid="hbm25656-bib-0005" ref-type="bibr">2018</xref>). According to this theory, when model capacity surpasses the interpolation threshold, the solution space of the problem is enlarged, leading to flatter solutions on parameter space. We can hypothesize that even larger, more expressive, networks will not degrade performance, pointing toward possible improvements with increasing computational resources.</p>
      <p>Abrol et al. (<xref rid="hbm25656-bib-0002" ref-type="bibr">2021</xref>) demonstrates systematically that deep models benefit from representation learning, extracting more informative features from data than hand‐crafted ones. Previous benchmarks comparing deep models with traditional machine learning could be thus biased against deep models. Applying deep neural networks to lower level data, that is, timeseries instead of hand‐crafted features such as RSFC, can result in superior performance. We verified this in our comparison against a strong benchmark, based on penalized linear modeling.</p>
      <p>Since RNNs are very flexible nonlinear models we cannot rule out the possibility that the RNNs could have learned a proxy measure of RSFC directly from data. RSFC is largely driven by a small number of high amplitude, small time‐scale, coactivity events (Esfahlani et al., <xref rid="hbm25656-bib-0014" ref-type="bibr">2020</xref>), thus also reflecting dynamically rich information.</p>
      <p>Obtaining maximal performance was not our main objective. Instead, we aimed at providing a differentiable model relating RS‐fMRI activity to “g” and extracting relevant neuroscientific insights from it. The study of brain dynamics in relation to human intelligence warrants the use of models that use that information. In special, the leading linear modeling approaches are based on univariate filtering. Univariate filtering is nondifferentiable, precluding us from relying on gradients for the study of model reliance.</p>
      <p>We demonstrated how one can decouple instant innovations from functional connectivity contributions using ZCA. It is possible to embed this knowledge into networks. Future works could then fuse models working on the functional‐connectivity domain and the time activity domain simultaneously. Such models could better differentiate the roles of dynamics in interindividual variations in cognition.</p>
      <p>We successfully reproduced several findings from the literature pertaining to brain biology in the context of general intelligence. We built a model that predicts “g” from time‐distributed BOLD fMRI activity. This model attains slightly increased performance in this task, without filtering. Studying its reliance on different parts of input information allows us to retrieve neuroscientific insights. We also present a method based on propagating saliencies from data to derivative measures, such as functional connectivity. Using it, we disambiguate the contributions of instantaneous innovations to model reliance, for example. Combining an ablation‐based approach with a saliency based one allowed us to identify a set of regions that degrade performance significantly more than equally sized RSNs.</p>
    </sec>
    <sec sec-type="COI-statement" id="hbm25656-sec-0027">
      <title>CONFLICT OF INTEREST</title>
      <p>The authors declare no potential conflict of interest.</p>
    </sec>
  </body>
  <back>
    <ack id="hbm25656-sec-0026">
      <title>ACKNOWLEDGMENTS</title>
      <p>This study was financed by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior—Brasil (CAPES)—Finance Code 001 and FAPESP (The São Paulo Research Foundation; grants 2017/02752‐0 and 2018/11881‐1). Data were provided by the Human Connectome Project, WU‐Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University. Authors also thank James Townsend, and Lyndon White, Peifan Wu, and Julia AutoDiff community for helpful comments on automatic differentiation of the singular value decomposition.</p>
    </ack>
    <sec sec-type="data-availability" id="hbm25656-sec-5020">
      <title>DATA AVAILABILITY STATEMENT</title>
      <p>All data, imaging, demographical or behavioral, used is provided by the Human Connectome Project main study, HCP Young Adult. Data and details can be obtained at their site, pending approval. See <ext-link xlink:href="https://www.humanconnectome.org/" ext-link-type="uri" specific-use="software is-supplemented-by">https://www.humanconnectome.org/</ext-link>.</p>
      <p>Neural networks were implemented in Flux v0.9.0. See <ext-link xlink:href="https://github.com/FluxML/Flux.jl/tree/v0.9.0" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/FluxML/Flux.jl/tree/v0.9.0</ext-link>. Additional code for analyses was implemented in Julia v1.3.0. See <ext-link xlink:href="https://github.com/JuliaLang/julia/tree/v1.3.0" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/JuliaLang/julia/tree/v1.3.0</ext-link>. Code will be shared upon request.</p>
    </sec>
    <ref-list id="hbm25656-bibl-0001" content-type="cited-references">
      <title>REFERENCES</title>
      <ref id="hbm25656-bib-0001">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0001">
<string-name>
<surname>Abrol</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Fu</surname>, <given-names>Z.</given-names>
</string-name>, <string-name>
<surname>Salman</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Silva</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Du</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<surname>Plis</surname>, <given-names>S.</given-names>
</string-name>, &amp; <string-name>
<surname>Calhoun</surname>, <given-names>V.</given-names>
</string-name> (<year>2020</year>). <article-title>Hype versus hope: Deep learning encodes more predictive and robust brain imaging representations than standard machine learning. <italic toggle="yes">bioRxiv</italic> (preprint)</article-title>. doi: <pub-id pub-id-type="doi">10.1101/2020.04.14.041582</pub-id>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0002">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0002">
<string-name>
<surname>Abrol</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Fu</surname>, <given-names>Z.</given-names>
</string-name>, <string-name>
<surname>Salman</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Silva</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Du</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<surname>Plis</surname>, <given-names>S.</given-names>
</string-name>, &amp; <string-name>
<surname>Calhoun</surname>, <given-names>V.</given-names>
</string-name> (<year>2021</year>). <article-title>Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning</article-title>. <source>Nature Communications</source>, <volume>12</volume>, <fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-20655-6</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0003">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0003">
<string-name>
<surname>Anticevic</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Cole</surname>, <given-names>M. W.</given-names>
</string-name>, <string-name>
<surname>Murray</surname>, <given-names>J. D.</given-names>
</string-name>, <string-name>
<surname>Corlett</surname>, <given-names>P. R.</given-names>
</string-name>, <string-name>
<surname>Wang</surname>, <given-names>X.‐j.</given-names>
</string-name>, &amp; <string-name>
<surname>Krystal</surname>, <given-names>J. H.</given-names>
</string-name> (<year>2012</year>). <article-title>The role of default network deactivation in cognition and disease</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>, <fpage>584</fpage>–<lpage>592</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2012.10.008</pub-id>
<pub-id pub-id-type="pmid">23142417</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0004">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0004">
<string-name>
<surname>Barbey</surname>, <given-names>A. K.</given-names>
</string-name> (<year>2018</year>). <article-title>Network neuroscience theory of human intelligence</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>22</volume>, <fpage>8</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2017.10.001</pub-id>
<pub-id pub-id-type="pmid">29167088</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0005">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0005">
<string-name>
<surname>Belkin</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Hsu</surname>, <given-names>D.</given-names>
</string-name>, &amp; <string-name>
<surname>Mitra</surname>, <given-names>P. P.</given-names>
</string-name> (<year>2018</year>). <article-title>Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate</article-title>. <source>Advances in Neural Information Processing Systems (NeurIPS)</source>, <volume>31</volume>, <fpage>2300</fpage>–<lpage>2311</lpage>. arXiv:1806.05161.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0006">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0006">
<string-name>
<surname>Bezanson</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Edelman</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Karpinski</surname>, <given-names>S.</given-names>
</string-name>, &amp; <string-name>
<surname>Shah</surname>, <given-names>V. B.</given-names>
</string-name> (<year>2017</year>). <article-title>Julia: A fresh approach to numerical computing</article-title>. <source>SIAM Review</source>, <volume>59</volume>, <fpage>65</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1137/141000671</pub-id> arXiv:1411.1607.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0007">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0007">
<string-name>
<surname>Brookes</surname>, <given-names>M. J.</given-names>
</string-name>, <string-name>
<surname>Woolrich</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Luckhoo</surname>, <given-names>H.</given-names>
</string-name>, <string-name>
<surname>Price</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Hale</surname>, <given-names>J. R.</given-names>
</string-name>, <string-name>
<surname>Stephenson</surname>, <given-names>M. C.</given-names>
</string-name>, … <string-name>
<surname>Morris</surname>, <given-names>P. G.</given-names>
</string-name> (<year>2011</year>). <article-title>Investigating the electrophysiological basis of resting state networks using magnetoencephalography</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>108</volume>, <fpage>16783</fpage>–<lpage>16788</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1112685108</pub-id>
<pub-id pub-id-type="pmid">21930901</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0008">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0008">
<string-name>
<surname>Calhoun</surname>, <given-names>V. D.</given-names>
</string-name>, &amp; <string-name>
<surname>Adali</surname>, <given-names>T.</given-names>
</string-name> (<year>2012</year>). <article-title>Multisubject independent component analysis of fMRI: A decade of intrinsic networks, default mode, and neurodiagnostic discovery</article-title>. <source>IEEE Reviews in Biomedical Engineering</source>, <volume>5</volume>, <fpage>60</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1109/RBME.2012.2211076</pub-id> arXiv:15334406.<pub-id pub-id-type="pmid">23231989</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0009">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0009">
<string-name>
<surname>Calhoun</surname>, <given-names>V. D.</given-names>
</string-name>, <string-name>
<surname>Adali</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Pearlson</surname>, <given-names>G. D.</given-names>
</string-name>, &amp; <string-name>
<surname>Pekar</surname>, <given-names>J. J.</given-names>
</string-name> (<year>2001</year>). <article-title>A method for making group inferences from functional MRI data using independent component analysis</article-title>. <source>Human Brain Mapping</source>, <volume>14</volume>, <fpage>140</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.1048</pub-id>
<pub-id pub-id-type="pmid">11559959</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0010">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0010">
<string-name>
<surname>Colom</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Karama</surname>, <given-names>S.</given-names>
</string-name>, <string-name>
<surname>Jung</surname>, <given-names>R. E.</given-names>
</string-name>, &amp; <string-name>
<surname>Haier</surname>, <given-names>R. J.</given-names>
</string-name> (<year>2010</year>). <article-title>Human intelligence and brain networks</article-title>. <source>Dialogues in Clinical Neuroscience</source>, <volume>12</volume>, <fpage>489</fpage>–<lpage>501</lpage>. <pub-id pub-id-type="doi">10.31887/DCNS.2010.12.4/rcolom</pub-id>
<pub-id pub-id-type="pmid">21319494</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0011">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0011">
<string-name>
<surname>Dizaji</surname>, <given-names>A. S.</given-names>
</string-name>, <string-name>
<surname>Vieira</surname>, <given-names>B. H.</given-names>
</string-name>, <string-name>
<surname>Khodaei</surname>, <given-names>M.‐R.</given-names>
</string-name>, <string-name>
<surname>Ashrafi</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Parham</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<surname>Hossein‐Zadeh</surname>, <given-names>G.‐A.</given-names>
</string-name>, … <string-name>
<surname>Soltanian‐Zadeh</surname>, <given-names>H.</given-names>
</string-name> (<year>2021</year>). <article-title>Linking brain biology to intellectual endowment: A review on the associations between human intelligence and neuroimaging data</article-title>. <source>Basic and Clinical Neuroscience</source>, <volume>12</volume>, <fpage>1</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.32598/bcn.12.1.574.1</pub-id>
<pub-id pub-id-type="pmid">33995924</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0012">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0012">
<string-name>
<surname>Dubois</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Galdi</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Paul</surname>, <given-names>L. K.</given-names>
</string-name>, &amp; <string-name>
<surname>Adolphs</surname>, <given-names>R.</given-names>
</string-name> (<year>2018</year>). <article-title>A distributed brain network predicts general intelligence from resting‐state human neuroimaging data</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>373</volume>, <elocation-id>20170284</elocation-id>. <pub-id pub-id-type="doi">10.1098/rstb.2017.0284</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0013">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0013">
<string-name>
<surname>Dvornek</surname>, <given-names>N. C.</given-names>
</string-name>, <string-name>
<surname>Ventola</surname>, <given-names>P.</given-names>
</string-name>, <string-name>
<surname>Pelphrey</surname>, <given-names>K. A.</given-names>
</string-name>, <string-name>
<surname>Duncan</surname>, <given-names>J. S.</given-names>
</string-name> (<year>2017</year>). <article-title>Identifying autism from resting‐state fMRI using long short‐term memory networks. In Q. Wang, Y. Shi, &amp; K. Suzuki (Eds.), <italic toggle="yes">Machine learning in medical imaging</italic>. Lecture Notes in Computer Science (pp. 362–370). Springer International Publishing</article-title>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-67389-9_42</pub-id> .</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0014">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0014">
<string-name>
<surname>Esfahlani</surname>, <given-names>F. Z.</given-names>
</string-name>, <string-name>
<surname>Jo</surname>, <given-names>Y.</given-names>
</string-name>, <string-name>
<surname>Faskowitz</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Byrge</surname>, <given-names>L.</given-names>
</string-name>, <string-name>
<surname>Kennedy</surname>, <given-names>D. P.</given-names>
</string-name>, <string-name>
<surname>Sporns</surname>, <given-names>O.</given-names>
</string-name>, &amp; <string-name>
<surname>Betzel</surname>, <given-names>R. F.</given-names>
</string-name> (<year>2020</year>). <article-title>High‐amplitude cofluctuations in cortical activity drive functional connectivity</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>117</volume>, <fpage>28393</fpage>–<lpage>28401</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2005531117</pub-id>
<pub-id pub-id-type="pmid">33093200</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0015">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0015">
<string-name>
<surname>Essen</surname>, <given-names>D. C. V.</given-names>
</string-name>, <string-name>
<surname>Smith</surname>, <given-names>S. M.</given-names>
</string-name>, <string-name>
<surname>Barch</surname>, <given-names>D. M.</given-names>
</string-name>, <string-name>
<surname>Behrens</surname>, <given-names>T. E.</given-names>
</string-name>, <string-name>
<surname>Yacoub</surname>, <given-names>E.</given-names>
</string-name>, &amp; <string-name>
<surname>Ugurbil</surname>, <given-names>K.</given-names>
</string-name> (<year>2013</year>). <article-title>The WU‐Minn human connectome project: An overview</article-title>. <source>NeuroImage</source>, <volume>80</volume>, <fpage>62</fpage>–<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id> arXiv:NIHMS150003.<pub-id pub-id-type="pmid">23684880</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0016">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0016">
<string-name>
<surname>Ferreira</surname>, <given-names>L. K.</given-names>
</string-name>, <string-name>
<surname>Regina</surname>, <given-names>A. C. B.</given-names>
</string-name>, <string-name>
<surname>Kovacevic</surname>, <given-names>N.</given-names>
</string-name>, <string-name>
<surname>Martin</surname>, <given-names>M. D. G. M.</given-names>
</string-name>, <string-name>
<surname>Santos</surname>, <given-names>P. P.</given-names>
</string-name>, <string-name>
<surname>Carneiro</surname>, <given-names>C. D. G.</given-names>
</string-name>, … <string-name>
<surname>Busatto</surname>, <given-names>G. F.</given-names>
</string-name> (<year>2016</year>). <article-title>Aging effects on whole‐brain functional connectivity in adults free of cognitive and psychiatric disorders</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>, <fpage>3851</fpage>–<lpage>3865</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhv190</pub-id>
<pub-id pub-id-type="pmid">26315689</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0017">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0017">
<string-name>
<surname>Finn</surname>, <given-names>E. S.</given-names>
</string-name>, <string-name>
<surname>Shen</surname>, <given-names>X.</given-names>
</string-name>, <string-name>
<surname>Scheinost</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Rosenberg</surname>, <given-names>M. D.</given-names>
</string-name>, <string-name>
<surname>Huang</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Chun</surname>, <given-names>M. M.</given-names>
</string-name>, … <string-name>
<surname>Constable</surname>, <given-names>R. T.</given-names>
</string-name> (<year>2015</year>). <article-title>Functional connectome fingerprinting: Identifying individuals using patterns of brain connectivity</article-title>. <source>Nature Neuroscience</source>, <volume>18</volume>, <fpage>1664</fpage>–<lpage>1671</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4135</pub-id> arXiv:15334406.<pub-id pub-id-type="pmid">26457551</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0018">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0018">
<string-name>
<surname>Graves</surname>, <given-names>A.</given-names>
</string-name>, &amp; <string-name>
<surname>Schmidhuber</surname>, <given-names>J.</given-names>
</string-name> (<year>2005</year>). <article-title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures</article-title>. <source>Neural Networks</source>, <volume>18</volume>, <fpage>602</fpage>–<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2005.06.042</pub-id>
<pub-id pub-id-type="pmid">16112549</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0019">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0019">
<string-name>
<surname>He</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Kong</surname>, <given-names>R.</given-names>
</string-name>, <string-name>
<surname>Holmes</surname>, <given-names>A. J.</given-names>
</string-name>, <string-name>
<surname>Nguyen</surname>, <given-names>M.</given-names>
</string-name>, <string-name>
<surname>Sabuncu</surname>, <given-names>M. R.</given-names>
</string-name>, <string-name>
<surname>Eickhoff</surname>, <given-names>S. B.</given-names>
</string-name>, … <string-name>
<surname>Yeo</surname>, <given-names>B. T.</given-names>
</string-name> (<year>2020</year>). <article-title>Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behavior and demographics</article-title>. <source>NeuroImage</source>, <volume>206</volume>, <elocation-id>116276</elocation-id>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116276</pub-id>
<pub-id pub-id-type="pmid">31610298</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0020">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0020">
<string-name>
<surname>Hearne</surname>, <given-names>L. J.</given-names>
</string-name>, <string-name>
<surname>Mattingley</surname>, <given-names>J. B.</given-names>
</string-name>, &amp; <string-name>
<surname>Cocchi</surname>, <given-names>L.</given-names>
</string-name> (<year>2016</year>). <article-title>Functional brain networks related to individual differences in human intelligence at rest</article-title>. <source>Scientific Reports</source>, <volume>6</volume>, <elocation-id>32328</elocation-id>. <pub-id pub-id-type="doi">10.1038/srep32328</pub-id>
<pub-id pub-id-type="pmid">27561736</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0021">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0021">
<string-name>
<surname>Hochreiter</surname>, <given-names>S.</given-names>
</string-name>, &amp; <string-name>
<surname>Schmidhuber</surname>, <given-names>J.</given-names>
</string-name> (<year>1997</year>). <article-title>Long short‐term memory</article-title>. <source>Neural Computation</source>, <volume>9</volume>, <fpage>1735</fpage>–<lpage>1780</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0022">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0022">
<string-name>
<surname>Innes</surname>, <given-names>M.</given-names>
</string-name> (<year>2018</year>). <article-title>Flux: Elegant machine learning with Julia</article-title>. <source>Journal of Open Source Software</source>, <volume>3</volume>, <elocation-id>602</elocation-id>. <pub-id pub-id-type="doi">10.21105/joss.00602</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0023">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0023">
<string-name>
<surname>Ito</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Kulkarni</surname>, <given-names>K. R.</given-names>
</string-name>, <string-name>
<surname>Schultz</surname>, <given-names>D. H.</given-names>
</string-name>, <string-name>
<surname>Mill</surname>, <given-names>R. D.</given-names>
</string-name>, <string-name>
<surname>Chen</surname>, <given-names>R. H.</given-names>
</string-name>, <string-name>
<surname>Solomyak</surname>, <given-names>L. I.</given-names>
</string-name>, &amp; <string-name>
<surname>Cole</surname>, <given-names>M. W.</given-names>
</string-name> (<year>2017</year>). <article-title>Cognitive task information is transferred between brain regions via resting‐state network topology</article-title>. <source>Nature Communications</source>, <volume>8</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-017-01000-w</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0024">
        <mixed-citation publication-type="book" id="hbm25656-cit-0024">
<string-name>
<surname>Jensen</surname>, <given-names>A. R.</given-names>
</string-name> (<year>1998</year>). <part-title>The Discovery of g</part-title>. <source>The g factor: The science of mental ability, Human Evolution, Behavior, and Intelligence</source>, (1st ed., pp. 18–44). Westport, Connecticut and London: Praeger. <pub-id pub-id-type="doi">10.1007/s13398-014-0173-7.2</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0025">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0025">
<string-name>
<surname>Jung</surname>, <given-names>R. E.</given-names>
</string-name>, &amp; <string-name>
<surname>Haier</surname>, <given-names>R. J.</given-names>
</string-name> (<year>2007</year>). <article-title>The Parieto‐frontal integration theory (P‐FIT) of intelligence: Converging neuroimaging evidence</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>30</volume>, <fpage>135</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1017/S0140525X07001185</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0026">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0026">
<string-name>
<surname>Kingma</surname>, <given-names>D.P.</given-names>
</string-name>, <string-name>
<surname>Ba</surname>, <given-names>J.</given-names>
</string-name> (<year>2014</year>). <article-title>Adam: A method for stochastic optimization, <italic toggle="yes">arXiv Preprint</italic>. pp. 1–15</article-title>. arXiv:1412.6980.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0027">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0027">
<string-name>
<surname>Kohavi</surname>, <given-names>R.</given-names>
</string-name>, &amp; <string-name>
<surname>Edu</surname>, <given-names>S.</given-names>
</string-name> (<year>1993</year>). <article-title>A study of cross‐validation and bootstrap for accuracy estimation and model selection</article-title>. <source>Proceedings of the 14th International Joint Conference on Artificial Intelligence</source>, <volume>2</volume>, <fpage>1137</fpage>–<lpage>1143</lpage>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0028">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0028">
<string-name>
<surname>Krizhevsky</surname>, <given-names>A.</given-names>
</string-name> (<year>2009</year>). <article-title>Learning multiple layers of features from tiny images</article-title>. Retrieved from <ext-link xlink:href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" ext-link-type="uri">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</ext-link>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0029">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0029">
<string-name>
<surname>Krizhevsky</surname>, <given-names>A.</given-names>
</string-name>, <string-name>
<surname>Sutskever</surname>, <given-names>I.</given-names>
</string-name>, &amp; <string-name>
<surname>Hinton</surname>, <given-names>G. E.</given-names>
</string-name> (<year>2012</year>). <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>2</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0030">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0030">
<string-name>
<surname>Krogh</surname>, <given-names>A.</given-names>
</string-name>, &amp; <string-name>
<surname>Hertz</surname>, <given-names>J. A.</given-names>
</string-name> (<year>1992</year>). <article-title>A simple weight decay can improve generalization</article-title>. <source>Advances in Neural Information Processing Systems</source>, <volume>4</volume>, <fpage>950</fpage>–<lpage>957</lpage>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0031">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0031">
<string-name>
<surname>Loshchilov</surname>, <given-names>I.</given-names>
</string-name>, <string-name>
<surname>Hutter</surname>, <given-names>F.</given-names>
</string-name> (<year>2019</year>). <article-title>
<italic toggle="yes">Decoupled weight decay regularization</italic>. 7th International Conference on Learning Representations, ICLR 2019. arXiv:1711.05101</article-title>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0032">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0032">
<string-name>
<surname>Luders</surname>, <given-names>E.</given-names>
</string-name>, <string-name>
<surname>Narr</surname>, <given-names>K. L.</given-names>
</string-name>, <string-name>
<surname>Thompson</surname>, <given-names>P. M.</given-names>
</string-name>, &amp; <string-name>
<surname>Toga</surname>, <given-names>A. W.</given-names>
</string-name> (<year>2009</year>). <article-title>Neuroanatomical correlates of intelligence</article-title>. <source>Intelligence</source>, <volume>37</volume>, <fpage>156</fpage>–<lpage>163</lpage>. <pub-id pub-id-type="doi">10.1016/j.intell.2008.07.002</pub-id>
<pub-id pub-id-type="pmid">20160919</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0033">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0033">
<string-name>
<surname>McDaniel</surname>, <given-names>M. A.</given-names>
</string-name> (<year>2005</year>). <article-title>Big‐brained people are smarter: A meta‐analysis of the relationship between in vivo brain volume and intelligence</article-title>. <source>Intelligence</source>, <volume>33</volume>, <fpage>337</fpage>–<lpage>346</lpage>. <pub-id pub-id-type="doi">10.1016/j.intell.2004.11.005</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0034">
        <mixed-citation publication-type="book" id="hbm25656-cit-0034">
<string-name>
<surname>Molnar</surname>, <given-names>C.</given-names>
</string-name> (<year>2019</year>). Model‐agnostic methods. In Interpretable machine learning: A guide for making black box models explainable. Munich, Germany. Retrieved from <ext-link xlink:href="https://christophm.github.io/interpretable-ml-book/" ext-link-type="uri">https://christophm.github.io/interpretable-ml-book/</ext-link>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0035">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0035">
<string-name>
<surname>Pamplona</surname>, <given-names>G. S. P.</given-names>
</string-name>, <string-name>
<surname>Santos Neto</surname>, <given-names>G. S.</given-names>
</string-name>, <string-name>
<surname>Rosset</surname>, <given-names>S. R. E.</given-names>
</string-name>, <string-name>
<surname>Rogers</surname>, <given-names>B. P.</given-names>
</string-name>, &amp; <string-name>
<surname>Salmon</surname>, <given-names>C. E. G.</given-names>
</string-name> (<year>2015</year>). <article-title>Analyzing the association between functional connectivity of the brain and intellectual performance</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>9</volume>, <elocation-id>61</elocation-id>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00061</pub-id>
<pub-id pub-id-type="pmid">25713528</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0036">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0036">
<string-name>
<surname>Ritchie</surname>, <given-names>S. J.</given-names>
</string-name>, <string-name>
<surname>Booth</surname>, <given-names>T.</given-names>
</string-name>, <string-name>
<surname>Hernández</surname>, <given-names>M. D. C. V.</given-names>
</string-name>, <string-name>
<surname>Corley</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Maniega</surname>, <given-names>S. M.</given-names>
</string-name>, <string-name>
<surname>Gow</surname>, <given-names>A. J.</given-names>
</string-name>, … <string-name>
<surname>Deary</surname>, <given-names>I. J.</given-names>
</string-name> (<year>2015</year>). <article-title>Beyond a bigger brain: Multivariable structural brain imaging and intelligence</article-title>. <source>Intelligence</source>, <volume>51</volume>, <fpage>47</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1016/j.intell.2015.05.001</pub-id>
<pub-id pub-id-type="pmid">26240470</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0037">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0037">
<string-name>
<surname>Spearman</surname>, <given-names>C.</given-names>
</string-name> (<year>1904</year>). <article-title>"general intelligence," objectively determined and measured</article-title>. <source>The American Journal of Psychology</source>, <volume>15</volume>, <fpage>201</fpage>. <pub-id pub-id-type="doi">10.2307/1412107</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0038">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0038">
<string-name>
<surname>Sridharan</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Levitin</surname>, <given-names>D. J.</given-names>
</string-name>, &amp; <string-name>
<surname>Menon</surname>, <given-names>V.</given-names>
</string-name> (<year>2008</year>). <article-title>A critical role for the right fronto‐insular cortex in switching between central‐executive and default‐mode networks</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>105</volume>, <fpage>12569</fpage>–<lpage>12574</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0800005105</pub-id>
<pub-id pub-id-type="pmid">18723676</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0039">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0039">
<string-name>
<surname>Sui</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Liu</surname>, <given-names>M. X.</given-names>
</string-name>, <string-name>
<surname>Lee</surname>, <given-names>J. H.</given-names>
</string-name>, <string-name>
<surname>Zhang</surname>, <given-names>J.</given-names>
</string-name>, &amp; <string-name>
<surname>Calhoun</surname>, <given-names>V.</given-names>
</string-name> (<year>2020</year>). <article-title>Deep learning methods and applications in neuroimaging</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>339</volume>, <elocation-id>108718</elocation-id>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108718</pub-id>
<pub-id pub-id-type="pmid">32272117</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0040">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0040">
<string-name>
<surname>Sun</surname>, <given-names>F. T.</given-names>
</string-name>, <string-name>
<surname>Miller</surname>, <given-names>L. M.</given-names>
</string-name>, &amp; <string-name>
<surname>D'Esposito</surname>, <given-names>M.</given-names>
</string-name> (<year>2004</year>). <article-title>Measuring interregional functional connectivity using coherence and partial coherence analyses of fMRI data</article-title>. <source>NeuroImage</source>, <volume>21</volume>, <fpage>647</fpage>–<lpage>658</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.09.056</pub-id>
<pub-id pub-id-type="pmid">14980567</pub-id></mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0041">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0041">
<string-name>
<surname>Thurstone</surname>, <given-names>L. L.</given-names>
</string-name> (<year>1940</year>). <article-title>Current issues in factor analysis</article-title>. <source>Psychological Bulletin</source>, <volume>37</volume>, <fpage>189</fpage>–<lpage>236</lpage>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0042">
        <mixed-citation publication-type="miscellaneous" id="hbm25656-cit-0042">
<string-name>
<surname>Townsend</surname>, <given-names>J.</given-names>
</string-name> (<year>2016</year>). <article-title>Differentiating the singular value decomposition</article-title>. Available from <ext-link xlink:href="https://j-towns.github.io/papers/svd-derivative.pdf" ext-link-type="uri">https://j-towns.github.io/papers/svd-derivative.pdf</ext-link>.</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0043">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0043">
<string-name>
<surname>Williams</surname>, <given-names>R. J.</given-names>
</string-name>, &amp; <string-name>
<surname>Peng</surname>, <given-names>J.</given-names>
</string-name> (<year>1990</year>). <article-title>An efficient gradient‐based algorithm for on‐line training of recurrent network trajectories</article-title>. <source>Neural Computation</source>, <volume>2</volume>, <fpage>490</fpage>–<lpage>501</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1990.2.4.490</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0044">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0044">
<string-name>
<surname>Yekutieli</surname>, <given-names>D.</given-names>
</string-name>, &amp; <string-name>
<surname>Benjamini</surname>, <given-names>Y.</given-names>
</string-name> (<year>1999</year>). <article-title>Resampling‐based false discovery rate controlling multiple test procedures for correlated test statistics</article-title>. <source>Journal of Statistical Planning and Inference</source>, <volume>82</volume>, <fpage>171</fpage>–<lpage>196</lpage>. <pub-id pub-id-type="doi">10.1016/s0378-3758(99)00041-5</pub-id>
</mixed-citation>
      </ref>
      <ref id="hbm25656-bib-0045">
        <mixed-citation publication-type="journal" id="hbm25656-cit-0045">
<string-name>
<surname>Yeo</surname>, <given-names>B. T. T.</given-names>
</string-name>, <string-name>
<surname>Krienen</surname>, <given-names>F. M.</given-names>
</string-name>, <string-name>
<surname>Sepulcre</surname>, <given-names>J.</given-names>
</string-name>, <string-name>
<surname>Sabuncu</surname>, <given-names>M. R.</given-names>
</string-name>, <string-name>
<surname>Lashkari</surname>, <given-names>D.</given-names>
</string-name>, <string-name>
<surname>Hollinshead</surname>, <given-names>M.</given-names>
</string-name>, … <string-name>
<surname>Buckner</surname>, <given-names>R. L.</given-names>
</string-name> (<year>2011</year>). <article-title>The organization of the human cerebral cortex estimated by intrinsic functional connectivity</article-title>. <source>Journal of Neurophysiology</source>, <volume>106</volume>, <fpage>1125</fpage>–<lpage>1165</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00338.2011</pub-id>
<pub-id pub-id-type="pmid">21653723</pub-id></mixed-citation>
      </ref>
    </ref-list>
    <app-group>
      <app id="hbm25656-app-0001" content-type="Appendix">
        <title>PROPAGATING DERIVATIVES TO NONSTANDARDIZED DATA</title>
        <p>Implicitly, our model <mml:math id="jats-math-78" display="inline" overflow="scroll"><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:math> includes standardization and BPF. For efficiency, we performed these operations <italic toggle="yes">ante‐hoc</italic>. When calculating saliencies, however, the model is agnostic to those operations. In special, saliencies are not, in principle, forbidden of inducing mean‐ and scale‐shifts, or to access forbidden frequencies below 0.008 Hz. The full model is, actually <mml:math id="jats-math-79" display="inline" overflow="scroll"><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>. <mml:math id="jats-math-80" display="inline" overflow="scroll"><mml:mi mathvariant="script">H</mml:mi><mml:mfenced open="(" close=")"><mml:mo>⋅</mml:mo></mml:mfenced></mml:math> represents BPF while <mml:math id="jats-math-81" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi><mml:mfenced open="(" close=")"><mml:mo>⋅</mml:mo></mml:mfenced></mml:math> represents temporal standardization. The ordering of the operations is not arbitrary, however. Since BPF does not preserve variance, it must be nested under standardization.</p>
        <p>To retrieve meaningful saliencies we use the chain rule to arrive at the results shown in Equation (<xref rid="hbm25656-disp-0002" ref-type="disp-formula">A1</xref>). We introduce the identities <mml:math id="jats-math-82" display="inline" overflow="scroll"><mml:mi mathvariant="script">H</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="script">H</mml:mi><mml:mfenced open="(" close=")"><mml:mi>X</mml:mi></mml:mfenced></mml:math> and <mml:math id="jats-math-83" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="script">H</mml:mi></mml:mfenced></mml:math> to simplify notation.<disp-formula id="hbm25656-disp-0002">
<label>(A1)</label>
<mml:math id="jats-math-84" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="script">S</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">H</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">N</mml:mi><mml:mfenced open="(" close=")"><mml:mi mathvariant="script">S</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:mfrac></mml:math>
</disp-formula>For any given region <mml:math id="jats-math-85" display="inline" overflow="scroll"><mml:mi>i</mml:mi></mml:math> we can express its timeseries as the column matrix <mml:math id="jats-math-86" display="inline" overflow="scroll"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub></mml:mfenced></mml:math>. Its filtered version is <mml:math id="jats-math-87" display="inline" overflow="scroll"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="italic">Dx</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>. <mml:math id="jats-math-88" display="inline" overflow="scroll"><mml:mi>D</mml:mi></mml:math> is the discrete Fourier transform (DFT) matrix with zeros for the elements of filtered frequencies. We can see from Equation (<xref rid="hbm25656-disp-0003" ref-type="disp-formula">A2</xref>) that its derivatives are easy to compute.<disp-formula id="hbm25656-disp-0003">
<label>(A2)</label>
<mml:math id="jats-math-89" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>D</mml:mi></mml:math>
</disp-formula>The standardized version of <mml:math id="jats-math-90" display="inline" overflow="scroll"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> is <mml:math id="jats-math-91" display="inline" overflow="scroll"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>, shown in Equation (<xref rid="hbm25656-disp-0004" ref-type="disp-formula">A3</xref>).<disp-formula id="hbm25656-disp-0004">
<label>(A3)</label>
<mml:math id="jats-math-92" display="block" overflow="scroll"><mml:mfenced open="{" close=""><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mi>t</mml:mi><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mi>t</mml:mi><mml:mi>M</mml:mi></mml:munderover><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math>
</disp-formula>We can therefore express its partial derivatives regarding the elements in <mml:math id="jats-math-93" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math>.<disp-formula id="hbm25656-disp-0005">
<label>(A4)</label>
<mml:math id="jats-math-94" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>∂</mml:mi><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfenced open="(" close=")"><mml:mfrac><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="hbm25656-disp-0006">
<label>(A5)</label>
<mml:math id="jats-math-95" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:math>
</disp-formula>
<disp-formula id="hbm25656-disp-0007">
<label>(A6)</label>
<mml:math id="jats-math-96" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math>
</disp-formula>Combining Equations (<xref rid="hbm25656-disp-0005" ref-type="disp-formula">A4</xref>), (<xref rid="hbm25656-disp-0006" ref-type="disp-formula">A5</xref>), and (<xref rid="hbm25656-disp-0007" ref-type="disp-formula">A6</xref>) results in Equation (<xref rid="hbm25656-disp-0008" ref-type="disp-formula">A7</xref>).<disp-formula id="hbm25656-disp-0008">
<label>(A7)</label>
<mml:math id="jats-math-97" display="block" overflow="scroll"><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">it</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="italic">ik</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:math>
</disp-formula>These tensor derivatives can be flattened then recomposed into respective Jacobian matrices in Equation (<xref rid="hbm25656-disp-0002" ref-type="disp-formula">A1</xref>).</p>
      </app>
      <app id="hbm25656-app-0002" content-type="Appendix">
        <title>PROPAGATING GRADIENTS TO SVD COMPONENTS</title>
        <p>The ZCA of a matrix <mml:math id="jats-math-98" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">USV</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math> is given by <mml:math id="jats-math-99" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">UV</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>. Using the differential <mml:math id="jats-math-100" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math> it becomes simple to show that <mml:math id="jats-math-101" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">UV</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math>. Using the results obtained in (Townsend, <xref rid="hbm25656-bib-0042" ref-type="bibr">2016</xref>), <mml:math id="jats-math-102" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">V</mml:mi></mml:math> and <mml:math id="jats-math-103" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">U</mml:mi></mml:math> can be given in function of <mml:math id="jats-math-104" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> and <mml:math id="jats-math-105" display="inline" overflow="scroll"><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">X</mml:mi></mml:math> as shown in Equation (<xref rid="hbm25656-disp-0009" ref-type="disp-formula">B1</xref>).<disp-formula id="hbm25656-disp-0009">
<label>(B1)</label>
<mml:math id="jats-math-106" display="block" overflow="scroll"><mml:mfenced open="{" close=""><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>∘</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">XVS</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">SV</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">UU</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">XVS</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mo>∘</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>S</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">XV</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">US</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">VV</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">US</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">I</mml:mi><mml:mo>∘</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">XV</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math>
</disp-formula>The square matrix <mml:math id="jats-math-107" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">F</mml:mi></mml:math> is given in terms on <mml:math id="jats-math-108" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">S</mml:mi></mml:math> in Equation (<xref rid="hbm25656-disp-0010" ref-type="disp-formula">B2</xref>).<disp-formula id="hbm25656-disp-0010">
<label>(B2)</label>
<mml:math id="jats-math-109" display="block" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close=""><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math>
</disp-formula>
</p>
      </app>
    </app-group>
  </back>
</article>
