<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Entropy (Basel)</journal-id>
      <journal-id journal-id-type="iso-abbrev">Entropy (Basel)</journal-id>
      <journal-id journal-id-type="publisher-id">entropy</journal-id>
      <journal-title-group>
        <journal-title>Entropy</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1099-4300</issn>
      <publisher>
        <publisher-name>MDPI</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">35626516</article-id>
      <article-id pub-id-type="pmc">9141633</article-id>
      <article-id pub-id-type="doi">10.3390/e24050631</article-id>
      <article-id pub-id-type="publisher-id">entropy-24-00631</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Multivariate Gaussian Copula Mutual Information to Estimate Functional Connectivity with Less Random Architecture</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2466-7689</contrib-id>
          <name>
            <surname>Ashrafi</surname>
            <given-names>Mahnaz</given-names>
          </name>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Soltanian-Zadeh</surname>
            <given-names>Hamid</given-names>
          </name>
          <xref rid="c1-entropy-24-00631" ref-type="corresp">*</xref>
        </contrib>
      </contrib-group>
      <contrib-group>
        <contrib contrib-type="editor">
          <name>
            <surname>Bajic</surname>
            <given-names>Dragana</given-names>
          </name>
          <role>Academic Editor</role>
        </contrib>
      </contrib-group>
      <aff id="af1-entropy-24-00631">Control and Intelligent Processing Center of Excellence (CIPCE), School of Electrical and Computer Engineering, University of Tehran, Tehran 1439957131, Iran; <email>mahnaz.ashrafi@ut.ac.ir</email></aff>
      <author-notes>
        <corresp id="c1-entropy-24-00631"><label>*</label>Correspondence: <email>hszadeh@ut.ac.ir</email></corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>29</day>
        <month>4</month>
        <year>2022</year>
      </pub-date>
      <pub-date pub-type="collection">
        <month>5</month>
        <year>2022</year>
      </pub-date>
      <volume>24</volume>
      <issue>5</issue>
      <elocation-id>631</elocation-id>
      <history>
        <date date-type="received">
          <day>18</day>
          <month>3</month>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>4</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2022 by the authors.</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Recognition of a brain region’s interaction is an important field in neuroscience. Most studies use the Pearson correlation to find the interaction between the regions. According to the experimental evidence, there is a nonlinear dependence between the activities of different brain regions that is ignored by Pearson correlation as a linear measure. Typically, the average activity of each region is used as input because it is a univariate measure. This dimensional reduction, i.e., averaging, leads to a loss of spatial information across voxels within the region. In this study, we propose using an information-theoretic measure, multivariate mutual information (mvMI), as a nonlinear dependence to find the interaction between regions. This measure, which has been recently proposed, simplifies the mutual information calculation complexity using the Gaussian copula. Using simulated data, we show that the using this measure overcomes the mentioned limitations. Additionally using the real resting-state fMRI data, we compare the level of significance and randomness of graphs constructed using different methods. Our results indicate that the proposed method estimates the functional connectivity more significantly and leads to a smaller number of random connections than the common measure, Pearson correlation. Moreover, we find that the similarity of the estimated functional networks of the individuals is higher when the proposed method is used.</p>
      </abstract>
      <kwd-group>
        <kwd>mutual information</kwd>
        <kwd>functional connectivity</kwd>
        <kwd>resting-state fMRI</kwd>
        <kwd>linear correlation</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>Cognitive Sciences and Technologies Council</funding-source>
          <award-id>4776</award-id>
        </award-group>
        <award-group>
          <funding-source>Iran National Science Foundation</funding-source>
          <award-id>70145039</award-id>
        </award-group>
        <funding-statement>This work was supported in part by the Cognitive Sciences and Technologies Council, Tehran, Iran, under Grant No. 4776, and Iran National Science Foundation, Tehran, Iran, under Grant No. 70145039.</funding-statement>
      </funding-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro" id="sec1-entropy-24-00631">
      <title>1. Introduction</title>
      <p>A brain network is defined by its nodes and edges. Different regions are considered nodes. Edges are the interactions between different regions. Each region consists of many voxels, and each voxel has a time series derived from fMRI data. There are different methods for defining the edges or interactions between different nodes. Functional connectivity (FC) is a well-known approach that produces a functional brain network. Analytically, FC is defined as the statistical interaction between the temporal activities of two distinct regions. Interactions between the regions support different cognitive processes [<xref rid="B1-entropy-24-00631" ref-type="bibr">1</xref>]. Deviation from the FC structure of normal individuals is considered a biomarker for some diseases such as schizophrenia [<xref rid="B2-entropy-24-00631" ref-type="bibr">2</xref>], bipolar depressive disorders [<xref rid="B3-entropy-24-00631" ref-type="bibr">3</xref>], and attention deficit disorder [<xref rid="B4-entropy-24-00631" ref-type="bibr">4</xref>]. Therefore, using FC as a biomarker has been widely considered recently.</p>
      <p>Various methods have been proposed to estimate the FC between two regions. Some measures such as Granger causality (GC) and transfer entropy calculate the directed connectivity between two regions, while others such as Pearson correlation, independent component analysis (ICA), and coherence measures calculate undirectional interactions [<xref rid="B5-entropy-24-00631" ref-type="bibr">5</xref>,<xref rid="B6-entropy-24-00631" ref-type="bibr">6</xref>]. Moreover, some new measures such as interaction information defined based on mutual information have been introduced to estimate FC [<xref rid="B7-entropy-24-00631" ref-type="bibr">7</xref>]. Among the undirected measures focused on in this study, Pearson correlation (PCor) is the most common approach. While the calculation of this measure is straightforward, it has some disadvantages. Recent studies have found that PCor is not sufficient to characterize the statistical dependence between two regions [<xref rid="B8-entropy-24-00631" ref-type="bibr">8</xref>]. There are two distinct limitations to the linear correlation methods which can detect some spurious connections. Using this measure, signals from two anatomically separated brain regions may appear correlated and the regions may appear functionally connected [<xref rid="B9-entropy-24-00631" ref-type="bibr">9</xref>,<xref rid="B10-entropy-24-00631" ref-type="bibr">10</xref>,<xref rid="B11-entropy-24-00631" ref-type="bibr">11</xref>,<xref rid="B12-entropy-24-00631" ref-type="bibr">12</xref>]. However, a strong correlation between two regions may not guarantee that there is a functional connection between the underlying neurons. For instance, external or common inputs can lead to the correlation.</p>
      <p>Linear correlation has two important disadvantages as an FC measure. First, the linear correlation is a bivariate measure that needs a single time series for each region as an input. Typically, multiple time series of voxels within each region are reduced to a single time series by averaging across voxels or by taking the first principal component. Techniques such as multivoxel pattern analysis and representation similarity analysis have shown that there are relative patterns across voxels within each region [<xref rid="B13-entropy-24-00631" ref-type="bibr">13</xref>,<xref rid="B14-entropy-24-00631" ref-type="bibr">14</xref>]. This reduction discards the spatial information distributed in thousands of signals within the voxels in a given region [<xref rid="B15-entropy-24-00631" ref-type="bibr">15</xref>,<xref rid="B16-entropy-24-00631" ref-type="bibr">16</xref>]. For instance, it has been shown that some information about mental processes and cognitive states is detected by multivariate pattern analysis, while average activity discards this information [<xref rid="B17-entropy-24-00631" ref-type="bibr">17</xref>,<xref rid="B18-entropy-24-00631" ref-type="bibr">18</xref>]. The second limitation of PCor is that the linear correlation cannot detect the nonlinear dependencies between brain regions. Previous studies have shown that there are nonlinear dependencies between time series during the resting state. This nonlinear analysis of fMRI signals performs better than linear correlation [<xref rid="B19-entropy-24-00631" ref-type="bibr">19</xref>,<xref rid="B20-entropy-24-00631" ref-type="bibr">20</xref>,<xref rid="B21-entropy-24-00631" ref-type="bibr">21</xref>,<xref rid="B22-entropy-24-00631" ref-type="bibr">22</xref>]. Su et al. [<xref rid="B23-entropy-24-00631" ref-type="bibr">23</xref>] have found that considering nonlinear dependencies between fMRI signals can better discriminate schizophrenic patients from healthy subjects. Moreover, individual cognitive differences can be predicted better by taking into account the nonlinear properties of region interactions [<xref rid="B24-entropy-24-00631" ref-type="bibr">24</xref>].</p>
      <p>Recently, using information-theoretic quantities such as mutual information and transfer entropy has become more attractive in analyzing neuroimaging data [<xref rid="B25-entropy-24-00631" ref-type="bibr">25</xref>]. One application of these quantities is to find the interaction between neurons [<xref rid="B26-entropy-24-00631" ref-type="bibr">26</xref>,<xref rid="B27-entropy-24-00631" ref-type="bibr">27</xref>] or brain regions [<xref rid="B28-entropy-24-00631" ref-type="bibr">28</xref>]. For instance, a recent paper has proposed using a new measure, interaction information, to estimate the FC using the mutual information. Although this method takes into account the nonlinear interaction, it ignores the spatial information within voxels by taking the average across voxels [<xref rid="B7-entropy-24-00631" ref-type="bibr">7</xref>]. Generally, an outstanding advantage of the information-theoretic quantities such as mutual information is that they do not rely on the prior assumptions about the relationship between the time series. On the other hand, linear correlation implicitly assumes a multivariate normal distribution for random variables.</p>
      <p>In this paper, we propose using an information-theoretic measure as a functional connectivity metric that can overcome the two mentioned limitations. To this end, both nonlinear interaction and multidimensional signals have been considered. Here, we call this measure multivariate mutual information (mvMI). This measure is defined by mutual information and estimated by using the Gaussian copula notion. It is worth noting that we use the “multivariate” term as we use all voxel activities within each region to estimate the functional connectivity rather than using the average activity. Mutual information can be calculated by means of different approaches. Because of the complexity of computation and implementation, mutual information has gained less attention in the functional connectivity area. Using a copula as a statistical concept can simplify the <italic toggle="yes">MI</italic> estimation for experimental data. Moreover, we show that by using a copula, mutual information can be extended to calculate the dependence between two multidimensional variables. This property can overcome the limitation of PCor which needs univariate inputs. Here, we use Gaussian copula mutual information which was proposed by Ince and colleagues to estimate mutual information [<xref rid="B29-entropy-24-00631" ref-type="bibr">29</xref>]. We start by presenting the theoretical background of the proposed measure, multivariate <italic toggle="yes">MI</italic> (mvMI). Then, we generate simulation data and define different scenarios to evaluate the performance of the proposed measure in the face of the mentioned limitations. The simulation results indicate that mvMI can detect nonlinear and multidimensional dependences. Moreover, it is less sensitive to additive noise. Then, we apply the proposed measure to real resting-state fMRI data. We compare the proposed measure with linear correlation in terms of significance level and randomness. We show that the mvMI-based functional network architecture is closer to the well-known topology of the brain, i.e., small-world architecture. As a complex network that has a highly efficient small-world organization, the small-world organization of the brain supports efficient information flow at low wiring and energy cost [<xref rid="B30-entropy-24-00631" ref-type="bibr">30</xref>]. Brain dysfunctions and diseases lead to a deviation from the small-world architecture, shifting towards a random structure.</p>
      <p>In other words, using mvMI as an estimator of FC leads to a nonrandom topology. Finally, we measure the similarity of the FC matrices obtained for different subjects using different FC measures. The results show that the similarity between subjects in the mvMI approach is larger than that in the conventional PCor approach.</p>
    </sec>
    <sec id="sec2-entropy-24-00631">
      <title>2. Materials and Methods</title>
      <sec sec-type="subjects" id="sec2dot1-entropy-24-00631">
        <title>2.1. Participants</title>
        <p>Fifty-five young healthy right-handed individuals were recruited from the academic community and the local population living in Tehran, Iran. Participants’ age was in the range of 18–26 years (22 females and 23 males). They were university undergraduate paid volunteers. Participants completed a brief questionnaire including questions regarding medical or psychiatric disorders. Ethical approval for the study was obtained from the Iran University of Medical Sciences, Tehran, Iran.</p>
      </sec>
      <sec id="sec2dot2-entropy-24-00631">
        <title>2.2. MRI Data Acquisition and Preprocessing</title>
        <p>All MRI data were acquired on a Siemens 3 Tesla scanner with a 64-channel head coil. Structural images were acquired using a magnetization-prepared rapid acquisition with gradient echo (MPRAGE) pulse sequence with the following parameters: TR/TE = 2500/3.18 ms, flip angle = 8 degrees, voxel size = 1 mm isotropic, and field of view = 244 mm. Echo-planar images sensitive to BOLD contrast were acquired using the following parameters: TR/TE = 2000/30 ms, flip angle = 80 degrees, slice thickness = 4 mm, and voxel size = 4 × 3 × 3 <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>mm</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. During the resting-state fMRI scan, the participants were asked to remain awake with their eyes open and not to think about anything in particular.</p>
        <p>Functional MRI data were preprocessed using the CONN functional connectivity toolbox in MATLAB 2017b (<uri xlink:href="https://web.conn-toolbox.org/">https://web.conn-toolbox.org/</uri> accessed on 15 January 2021) [<xref rid="B31-entropy-24-00631" ref-type="bibr">31</xref>]. During the preprocessing steps, the functional images were slice-timing corrected, realigned, normalized (in the 2 mm Montreal Neurological Institute (MNI) space), and smoothed. The Artifact Detection Tool was used to detect outliers (&gt;3 SD and &gt;0.5 mm) for subsequent scrubbing regression. The structural images were segmented into gray matter, white matter (WM), and cerebral spinal fluid (CSF) and normalized to the MNI space. Then, linear regression using WM and CSF signals, linear trend, and subject motion (six rotation/translation motion parameters and six first-order temporal derivatives) was conducted to remove confounding effects. The residual blood-oxygen-level-dependent (BOLD) time series was band-pass filtered (0.01–0.1 Hz).</p>
      </sec>
      <sec id="sec2dot3-entropy-24-00631">
        <title>2.3. Information-Theoretic Estimation of Functional Connectivity</title>
        <sec id="sec2dot3dot1-entropy-24-00631">
          <title>2.3.1. Information Theory Quantities</title>
          <p>Information theory is a mathematical framework for the quantification, storage, and communication of information. Entropy, mostly known as the Shannon entropy, is a key quantity in information theory defined as the amount of uncertainty involved in the value of a random variable. Let <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> be two continuous random variables having marginal pdf <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> respectively. Denote by <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the joint pdf of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>. The Shannon entropy of variable <italic toggle="yes">X</italic> is defined as
<disp-formula id="FD1-entropy-24-00631"><label>(1)</label><mml:math id="mm5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>The joint entropy of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> is defined as
<disp-formula id="FD2-entropy-24-00631"><label>(2)</label><mml:math id="mm6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Mutual information is another fundamental quantity used to calculate the amount of information about one random variable by observing other random variables. For two discrete random variables, <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, mutual information is given by the following formula:<disp-formula id="FD3-entropy-24-00631"><label>(3)</label><mml:math id="mm7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mo>∫</mml:mo><mml:mi>y</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mfenced><mml:mi>y</mml:mi></mml:mfenced></mml:mrow></mml:mfrac><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Mutual information can be rewritten based on Shannon entropy as
<disp-formula id="FD4-entropy-24-00631"><label>(4)</label><mml:math id="mm8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mi>Y</mml:mi></mml:mfenced><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        </sec>
        <sec id="sec2dot3dot2-entropy-24-00631">
          <title>2.3.2. Information-Theoretic Quantities of Gaussian Variables</title>
          <p>For the multivariate Gaussian random variables, there is a closed-form solution for entropy:<disp-formula id="FD5-entropy-24-00631"><label>(5)</label><mml:math id="mm9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mfenced><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mfenced close="]" open="("><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">k</italic> are the covariance matrix and dimensionality of <italic toggle="yes">X</italic>, respectively. Using this closed-form expression of entropy leads to an exact definition of mutual information. Based on Equations (4) and (5), the mutual information of two random variables with Gaussian distribution can be calculated as follows [<xref rid="B32-entropy-24-00631" ref-type="bibr">32</xref>]:<disp-formula id="FD6-entropy-24-00631"><label>(6)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mfrac><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm12" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the covariance matrices of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, respectively, and <inline-formula><mml:math id="mm14" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the joint covariance matrix of variables <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>. For two univariate random variables, <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, Equation (6) can be rewritten based on the Pearson correlation between <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, i.e., <italic toggle="yes">Corr</italic>(<italic toggle="yes">X,Y</italic>), as follows:<disp-formula id="FD7-entropy-24-00631"><label>(7)</label><mml:math id="mm15" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        </sec>
        <sec id="sec2dot3dot3-entropy-24-00631">
          <title>2.3.3. Copulas</title>
          <p>A copula is another approach for determining the dependency between random variables. The copula is a multivariate cumulative distribution function for which the marginal probability distribution of each variable is uniform in the interval [0, 1]. Sklar’s theorem states that a multivariate cumulative distribution function (CDF), <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, can be defined by its marginal CDF, <inline-formula><mml:math id="mm17" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and a copula <italic toggle="yes">C</italic>:
<disp-formula id="FD8-entropy-24-00631"><label>(8)</label><mml:math id="mm18" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>The theorem indicates that the copula is unique if the marginals <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are continuous [<xref rid="B33-entropy-24-00631" ref-type="bibr">33</xref>]. Notably, a copula can be considered as the joint CDF of the random vector <inline-formula><mml:math id="mm20" overflow="scroll"><mml:mrow><mml:mrow><mml:mfenced><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, where each element is derived using the following transformation:<disp-formula id="FD9-entropy-24-00631"><label>(9)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Thus, the copula can be defined as
<disp-formula id="FD10-entropy-24-00631"><label>(10)</label><mml:math id="mm22" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msubsup><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>F</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>The joint probability density function of random variables can be written as
<disp-formula id="FD11-entropy-24-00631"><label>(11)</label><mml:math id="mm23" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mrow><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mi>C</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is the copula density. Using Equation (3), mutual information can be rewritten as follows:<disp-formula id="FD12-entropy-24-00631"><label>(12)</label><mml:math id="mm25" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∫</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover></mml:mstyle></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mspace linebreak="newline"/><mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Thus, the <italic toggle="yes">MI</italic> is equivalent to the negative of copula entropy. <italic toggle="yes">MI</italic> between two random variables <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> with transformation <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mfenced><mml:mi>x</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mfenced><mml:mi>y</mml:mi></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> can be obtained as
<disp-formula id="FD15-entropy-24-00631"><label>(13)</label><mml:math id="mm27" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Thus, the <italic toggle="yes">MI</italic> between two random variables can be obtained independently of the marginal distribution of the variables. For more details about the copula and its properties, see [<xref rid="B34-entropy-24-00631" ref-type="bibr">34</xref>].</p>
        </sec>
        <sec id="sec2dot3dot4-entropy-24-00631">
          <title>2.3.4. Estimating <italic toggle="yes">MI</italic> Using Gaussian Copula</title>
          <p>Calculating the mutual information through (3) is computationally complex. In other words, as estimation of the joint probability density function (pdf) of non-Gaussian distributed data is hard, the estimation of mutual information is also difficult. Thus, little attention has been paid to this method as an estimator of FC. In the previous section, we illustrated that copulas offer a natural approach for estimating mutual information, independently of the marginal distributions. As the copula entropy and thus the <italic toggle="yes">MI</italic> do not depend on the marginal distributions, without loss of generality, the marginals can be transformed to the standard Gaussian distributions. Since the copula is mostly unknown, Robin Ince and colleagues used the Gaussian approximation [<xref rid="B29-entropy-24-00631" ref-type="bibr">29</xref>]. Here, we use the Gaussian copula as an estimation for the unknown copula.</p>
          <p>The Gaussian copula entropy for two random variables <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> can be derived as follows [<xref rid="B32-entropy-24-00631" ref-type="bibr">32</xref>]:<disp-formula id="FD16-entropy-24-00631"><label>(14)</label><mml:math id="mm28" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>I</mml:mi><mml:mfenced><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mfenced><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">r</italic> is the correlation between the transformed Gaussian random variables. It is obvious that if the real copula is not Gaussian, the <italic toggle="yes">MI</italic> derived using (14) is not accurate. Since for a given mean and covariance matrix, the joint Gaussian distribution has the maximum entropy [<xref rid="B35-entropy-24-00631" ref-type="bibr">35</xref>], the Gaussian copula also has the maximum entropy. As <italic toggle="yes">MI</italic> is the negative copula entropy, the Gaussian copula provides a lower bound for the true <italic toggle="yes">MI</italic> [<xref rid="B36-entropy-24-00631" ref-type="bibr">36</xref>].</p>
        </sec>
        <sec id="sec2dot3dot5-entropy-24-00631">
          <title>2.3.5. Multivariate Mutual Information in Neuroimaging</title>
          <p>FC is defined as the statistical dependence between each pair of brain regions. Each region consists of many voxels, and each voxel has a time series that is derived from the fMRI data. Most studies summarize each region’s activity in a single time series obtained by taking the average across voxels. This dimensional reduction from the voxel dimension to a one-dimensional signal leads to the loss of spatial information between voxels. The main reason for this dimension reduction is that most FC quantities such as Pearson correlation’s inputs should be one-dimensional. Mutual information as a dependence quantity has the capability to estimate the dependence between two multidimensional random variables. In the previous section, we showed that <italic toggle="yes">MI</italic> can be calculated by means of the copula concept which is independent of the marginal distribution (Equation (12)). For a given covariance matrix <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow></mml:math></inline-formula>, the Gaussian copula can be written as
<disp-formula id="FD17-entropy-24-00631"><label>(15)</label><mml:math id="mm30" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mfenced><mml:mi>u</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mi>Σ</mml:mi></mml:msub><mml:mfenced><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the inverse cumulative distribution function of a standard normal distribution and <inline-formula><mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Φ</mml:mi><mml:mi>Σ</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the joint cumulative distribution function of a multivariate normal distribution with zero mean and correlation matrix equal to <inline-formula><mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow></mml:math></inline-formula>. The density function can be written as
<disp-formula id="FD18-entropy-24-00631"><label>(16)</label><mml:math id="mm34" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>Σ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced close="]" open="["><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mfenced><mml:mfenced close=")" open="["><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>Equation (4) is the entropy of a multivariate normal distribution. We can use this equation to find the entropy of the Gaussian copula.</p>
          <p>Here, to find the interaction between two regions which are multidimensional variables, we transform the marginal distribution of the variables to standard normal distribution. This transformation is performed since the <italic toggle="yes">MI</italic> is independent of the marginal distribution. Then, we use (4) to find the copula entropy of transformed variables, which is equal to <italic toggle="yes">MI</italic> based on (12).</p>
        </sec>
      </sec>
      <sec id="sec2dot4-entropy-24-00631">
        <title>2.4. Functional Connectivity Measures</title>
        <p>Linear correlation methods such as Pearson correlation are the simplest approaches for calculating FC. For two time series, <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> with n time points, the Pearson correlation is defined as follows:<disp-formula id="FD19-entropy-24-00631"><label>(17)</label><mml:math id="mm35" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mfenced><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mfenced><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm36" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm37" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are the mean values of <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic>, respectively. This measure can only detect the linear dependence between two univariate time series. Each region contains many signals related to different voxels. Before calculating the correlation between two regions, it is necessary to reduce each region’s activation to a single time series. Taking the average across voxels within each region is the most common method for this dimension reduction in functional connectivity studies. The Pearson correlation which uses the average signal is called PCor. If the voxel activities in a region are homogeneous, this approach is the best choice. However, as the homogeneity in the regions decreases, an average time series is not a good representative of the regional activity. Singular value decomposition is an alternative method, whose advantage arises when the regions are not homogeneous. As the second measure of FC, SVD, we summarize each region’s activity using the temporal singular vector corresponding to the largest singular value of the regional activity. <italic toggle="yes">MI</italic> is a more general measure. As a multivariate estimator, one important feature of <italic toggle="yes">MI</italic> is that it is capable of finding the statistical dependence between two multidimensional variables. Thus, for calculating the interaction between two regions with multiple voxels, it is not necessary to reduce the voxel time series within each region to a single time series, e.g., the average time series.</p>
        <p>Another advantage of <italic toggle="yes">MI</italic> over PCor is that it is a nonlinear dependence estimator. To pinpoint which aspect of mvMI leads to a difference from PCor, we use both univariate and multivariate versions of <italic toggle="yes">MI</italic> in our investigation. For the univariate version, which we name uvMI, we use <italic toggle="yes">MI</italic> to estimate the association between the average time series of two brain regions. For the multivariate version, named mvMI, we use multiple time series from each region to quantify the FC between two regions. To calculate the mvMI, we start with a principal component analysis over each region’s time series and select the first 5 principal components of each region. Indeed, by applying principal component analysis (PCA) to each region’s time series, we select the most important components as a representation of that region’s activities. Then, FC between two regions is estimated by calculating the <italic toggle="yes">MI</italic> between the 5 selected principal components of the regions.</p>
        <p>Unlike the PCor, which has a value within the range of −1 to 1, the mutual information’s value is more open-ended and can range from 0 for complete independence to infinity for complete dependence. To have a fair comparison, we rescale both <italic toggle="yes">MI</italic> measures to the [0, 1] range using a power transformation. As the <italic toggle="yes">MI</italic> is a positive measure, the absolute values of PCor have been used throughout this article.</p>
      </sec>
      <sec id="sec2dot5-entropy-24-00631">
        <title>2.5. Simulation Design</title>
        <p>We propose using <italic toggle="yes">MI</italic> as an FC metric because of two advantages of <italic toggle="yes">MI</italic> over Pearson correlation, the capabilities of detecting nonlinear and multidimensional dependence. In the previous section, a computationally efficient approximation of <italic toggle="yes">MI</italic> was presented. To evaluate if the approximation preserves the considered advantages, we use simulated data. We compare the performance of <italic toggle="yes">MI</italic>-based quantities and Pearson correlation through different dependencies between two simulated regions.</p>
        <p>We simulate the time series of two distinct regions containing 100 and 150 voxels with 500 time points. The time series of the first region is generated using a multivariate normal distribution. The time series of the second region is calculated from those of the first region using a given mapping function <italic toggle="yes">f</italic>. This function controls the interaction between the two regions. It can be a nonlinear function such as a power function. By changing <italic toggle="yes">f</italic>, the performance of <italic toggle="yes">MI</italic> for linear and nonlinear interactions can be evaluated. To evaluate the performance of different measures by the simulated data, we estimate the null distribution of each measure by calculating the FC value between the regions of the null data, which is obtained by shuffling the time points randomly for each voxel. Then, we calculate the distance between each FC value and the 95th percentile of the null distribution. This distance is considered as the performance quantity.</p>
        <sec>
          <title>Simulation Scenarios</title>
          <p>In order to compare the performance of the mvMI measure with Pearson, four different simulated scenarios have been defined. In each scenario, we consider four different measures of FC, two Pearson correlation-based measures, and the two mutual information-based measures. Through different scenarios, we assess the performance of the proposed measure (mvMI) from different aspects. In each scenario, a different mapping function f is defined to generate the time series in the second region, based on the first region’s time series. Let <italic toggle="yes">X<sub>t</sub></italic> and <italic toggle="yes">Y<sub>t</sub></italic> be two vectors containing the <italic toggle="yes">N<sub>x</sub></italic> and <italic toggle="yes">N<sub>y</sub></italic> values which represent the voxel activation within two regions at time point <italic toggle="yes">t</italic>. For each scenario, the second region’s activation, <italic toggle="yes">Y<sub>t</sub></italic>, is defined as a function of the first one, <italic toggle="yes">X<sub>t</sub></italic>:
<disp-formula id="FD20-entropy-24-00631"><label>(18)</label><mml:math id="mm38" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">T</italic> is the mapping matrix and <italic toggle="yes">f</italic> is the function that determines the relation between the two regions. For example, for a linear voxel-to-voxel mapping from region 1 to region 2, <italic toggle="yes">f</italic> is a multiplication function and <italic toggle="yes">T</italic> is an <italic toggle="yes">N<sub>x</sub></italic> × <italic toggle="yes">N<sub>y</sub></italic> mapping matrix in which <italic toggle="yes">N<sub>x</sub></italic> × <italic toggle="yes">N<sub>x</sub></italic> elements are an identity matrix and the remaining elements are just random noise An independent Gaussian noise is added to both <italic toggle="yes">X<sub>t</sub></italic> and <italic toggle="yes">Y<sub>t</sub></italic> as the measurement noise, <inline-formula><mml:math id="mm39" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD21-entropy-24-00631"><label>(19)</label><mml:math id="mm40" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>For instance, to compare the performance of different measures in detecting nonlinear interaction, we use the elementwise power function as a nonlinear function. Indeed, the second region’s activity at time <italic toggle="yes">t</italic> <inline-formula><mml:math id="mm41" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is derived as
<disp-formula id="FD22-entropy-24-00631"><label>(20)</label><mml:math id="mm42" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p>The homogeneity of voxels in a region is an important property that directly affects the average time series. In the situation where all voxels within a region have the same activation, reducing the ROI activation to a single time series does not lead to any loss of information. As the homogeneity of the ROI activation decreases, a greater amount of information will be lost by taking the average. We use the covariance matrix as a parameter to control the first region’s homogeneity level. In each scenario, we consider three different covariance matrices for the first region’s time series which demonstrates the homogeneity level of that region. We consider two extreme covariance matrices, constant and identity. We use the constant matrix to generate homogeneous activity, i.e., the time series are highly positively correlated. For identity one, the time series within each region will be uncorrelated or independent. One middle condition is taken into account which is constituted of two constant positive and negative values which indicate positively correlated and anticorrelated, respectively. These covariance matrices are illustrated in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>. Through these scenarios, the performances of different FC measures, PCor, SVD uvMI, and mvMI, are examined. Nonlinearity, multivariate dependencies, and sensitivity to the structured noise are the main properties that are investigated through these scenarios.</p>
        </sec>
      </sec>
      <sec id="sec2dot6-entropy-24-00631">
        <title>2.6. Real Data Analysis</title>
        <p>The brain contains functional communities called resting-state networks. These networks show high-level within-community functional interaction and low-level interaction strengths between communities. The connections can be divided into within- and between-network categories. If the two nodes linked by the connection are located in the same network, the connection is considered a within-network connection; otherwise, the connection is considered a between-network connection. At first, we compare the functional brain networks derived using different estimators by calculating the correlation between different functional connectivity matrices for within and between connections.</p>
        <p>Using different approaches, we compare the performance of the proposed estimator (mvMI) with the other measures. For each connection and estimator, it is examined whether the connections are significant or not. By shuffling the time points of voxel time series within each region, the null distribution is obtained. By comparing the true value of each connection with the 95th percentile threshold of the null distribution, the connection is characterized as significant or insignificant. If the true value is greater than the 95th percentile level, it will be considered a significant connection.</p>
        <p>As a property of a normal functional network, the randomness level of each measure’s connectivity matrix is measured. Previous studies have illustrated that the brain network has a nonrandom pattern. Deviation from this pattern leads to known neuropathologies [<xref rid="B37-entropy-24-00631" ref-type="bibr">37</xref>]. For each connectivity measure, the nonrandomness level is determined using the Networkx, a graph theory package (<uri xlink:href="https://networkx.org/">https://networkx.org/</uri> accessed on 15 December 2021). The main idea is to quantify how close a connectivity matrix is to a random matrix containing random elements. There is a baseline for functional brain networks which is dominated by common resting-state networks across participants. Indeed, recent studies have indicated that resting-state functional brain networks share certain similar patterns including the connectivity weights and the spatial distribution of resting-state networks [<xref rid="B38-entropy-24-00631" ref-type="bibr">38</xref>,<xref rid="B39-entropy-24-00631" ref-type="bibr">39</xref>]. For instance, the default mode network (DMN) and the frontoparietal network are two well-known networks thought to be activated during the resting state. These networks do not vary significantly across different healthy subjects [<xref rid="B40-entropy-24-00631" ref-type="bibr">40</xref>]. To investigate the similarity of functional networks between different subjects, the correlation between each subject’s connectivity matrix and the average connectivity matrix is computed.</p>
      </sec>
    </sec>
    <sec sec-type="results" id="sec3-entropy-24-00631">
      <title>3. Results</title>
      <p>In this section, the results of the comparison of the proposed estimator (mvMI) with the most common measure (PCor) on both the simulated and real rs-fMRI data are provided.</p>
      <sec id="sec3dot1-entropy-24-00631">
        <title>3.1. Simulation Results</title>
        <p>In this section, we present the simulation results for four different scenarios. It is worth noting that for all scenarios we use the simulation data and covariance matrices explained in <xref rid="sec2dot5-entropy-24-00631" ref-type="sec">Section 2.5</xref>. The first four scenarios differ in <italic toggle="yes">f</italic> and <italic toggle="yes">T</italic> definitions, while they have the same covariance matrices. In the last scenario, the robustness of each measure to noise is evaluated. In all scenarios, we obtain FC 100 times to avoid random results. Illustrated results are the average performance across 100 repetitions.</p>
        <sec id="sec3dot1dot1-entropy-24-00631">
          <title>3.1.1. Linear Interaction between Two Regions</title>
          <p>In this section, we report the performance of different measures in detecting the linear interaction between two simulated brain regions. To implement this scenario, we generate the first region’s time series using a multivariate normal distribution with zero mean and different covariance matrices; for more details, see <xref rid="sec2dot5-entropy-24-00631" ref-type="sec">Section 2.5</xref>. Using Equation (19) the second region time series are generated. In this example, <italic toggle="yes">T</italic> is an <italic toggle="yes">N<sub>x</sub></italic> × <italic toggle="yes">N<sub>y</sub></italic> matrix whose first <italic toggle="yes">N<sub>x</sub></italic> × <italic toggle="yes">N<sub>x</sub></italic> elements are an identity matrix that simulates the linear interaction between two regions and whose other elements are random noise. <xref rid="entropy-24-00631-f002" ref-type="fig">Figure 2</xref> shows the performance of different measures. Each color is related to a special covariance matrix. As expected, all measures can detect the linear interaction with the positively correlated activities (blue bars).</p>
          <p>For the uncorrelated activities, mvMI outperforms the others, while SVD has the worst performance. Regardless of the covariance matrix, PCor and uvMI have the same performance, since both use the average regional activity. This result confirms that taking the average, which is used in PCor and uvMI, is a good representative of a region when all activities are correlated (blue bars). However, for the nonhomogeneous activity (orange and yellow bars), taking the average leads to performance reduction.</p>
        </sec>
        <sec id="sec3dot1dot2-entropy-24-00631">
          <title>3.1.2. Nonlinear Interaction between Two Regions</title>
          <p>In this scenario, we evaluate the nonlinear capability of all measures. To simulate this condition, we use a power function. Data for the second region are generated using Equation (20). In this scenario, we use the same matrix for <italic toggle="yes">T</italic> as in the previous scenario and a different power function as <italic toggle="yes">f</italic>. We again consider three different covariance matrices. Regardless of the homogeneity of the activities, i.e., covariance matrices, mvMI has the highest performance and detects the nonlinear interaction between two regions. However, uvMI can detect the nonlinear interaction only for the positively correlated condition (blue bar). While <italic toggle="yes">MI</italic> is a nonlinear dependency, detecting the nonlinear FC between two regions by uvMI is dependent on the homogeneity of the activities. As we expected, both PCor and SVD as linear measures fail to detect the nonlinear interaction (see <xref rid="entropy-24-00631-f003" ref-type="fig">Figure 3</xref>).</p>
        </sec>
        <sec id="sec3dot1dot3-entropy-24-00631">
          <title>3.1.3. Multivariate Interaction between Two Regions</title>
          <p>Here, we simulate a multivariate dependence between two regions using a multivariate normal distribution. In other words, <italic toggle="yes">f</italic> and <italic toggle="yes">T</italic> in Equation (20) are the multiplication function and an <italic toggle="yes">N<sub>x</sub></italic> × <italic toggle="yes">N<sub>y</sub></italic> matrix whose elements are generated by a multivariate normal distribution, respectively. Based on <xref rid="entropy-24-00631-f004" ref-type="fig">Figure 4</xref>, mvMI finds the multivariate interaction in all three conditions, while both PCor and uvMI, which are univariate measures that use average regional activities, cannot detect the multivariate interaction. SVD recognizes the interaction for positively correlated and mixed covariance matrices. This measure fails to detect the interaction when the activities are independent (orange bar).</p>
        </sec>
        <sec id="sec3dot1dot4-entropy-24-00631">
          <title>3.1.4. Additive Structural Noise</title>
          <p>In this scenario, additional noise is added to the time series of the second region. The noise power is the same for all voxels. Here we simulate a linear interaction with three different covariance matrices similar to previous scenarios. This kind of noise can be present in the real data due to various reasons such as subject movement or changes in alertness across different time points. <xref rid="entropy-24-00631-f005" ref-type="fig">Figure 5</xref> illustrates that additional noise has the least effect on mvMI, i.e., mvMI is more robust to noise than the other measures. The other three measures, especially PCor and SVD, cannot detect the interaction.</p>
        </sec>
      </sec>
      <sec id="sec3dot2-entropy-24-00631">
        <title>3.2. Real Data Results</title>
        <sec id="sec3dot2dot1-entropy-24-00631">
          <title>3.2.1. Comparing the Connectivity Matrices Derived Using Different Measures</title>
          <p>The connectivity matrices obtained for the 400-ROI Schaefer parcellation [<xref rid="B41-entropy-24-00631" ref-type="bibr">41</xref>] of one participant using PCor, uvMI, and mvMI are shown in <xref rid="entropy-24-00631-f006" ref-type="fig">Figure 6</xref>. The ROIs are ordered according to seven resting-state functional networks similar to Yeo atlas networks [<xref rid="B42-entropy-24-00631" ref-type="bibr">42</xref>]. Between-network connections are sparser for the mutual information measures, uvMI and mvMI, than PCor. Moreover, uvMI estimated some weak connections within networks. Since the correlation between PCor and SVD is very high, more than 0.9 for all participants, the SVD measure is excluded from real data analysis.</p>
          <p>To further analyze network-based FC structures revealed by different measures, the connection weights of the average matrices across participants are displayed in <xref rid="entropy-24-00631-f007" ref-type="fig">Figure 7</xref>, where within- and between-network connections for the default mode network have been separated. There are some spurious connections in correlation-based connectivity matrices [<xref rid="B43-entropy-24-00631" ref-type="bibr">43</xref>]. Thresholding is a common approach for cleaning these connections. However, thresholding also removes some real connections. Here, thresholding has removed some between- and within-network connections of the default mode network (DMN), which leads to a reduction in median value. The DMN is a set of regions that exhibit greater activity during the resting state [<xref rid="B10-entropy-24-00631" ref-type="bibr">10</xref>]. Some diseases such as schizophrenia (SZ) and autism spectrum disorder (ASD) lead to reduced interactions within the DMN [<xref rid="B40-entropy-24-00631" ref-type="bibr">40</xref>,<xref rid="B44-entropy-24-00631" ref-type="bibr">44</xref>].</p>
          <p>To quantify the general similarity of different measures, we compute the rank correlation between each pair of connectivity matrices generated by different measures. The average rank correlation coefficients, as a similarity metric, across all participants between mvMI and PCor and between mvMI and uvMI are 0.24 and 0.35, respectively. These values indicate that FC obtained from mvMI is more different from the PCor measure than from the uvMI measure. To better recognize the similarity of measures within each network separately, we calculate the correlation between each pair of measures for connections within each network. <xref rid="entropy-24-00631-f008" ref-type="fig">Figure 8</xref> shows the average correlation across all repetitions for mvMI with uvMI and PCor. The similarity between mvMI and PCor is higher for connections that connect the nodes inside the networks, except for the limbic network. The similarity between mvMI and PCor for visual and limbic networks is less than the others.</p>
        </sec>
        <sec id="sec3dot2dot2-entropy-24-00631">
          <title>3.2.2. Comparing Insignificant Connections and Nonrandom Architecture of Functional Networks Obtained Using Different Measures</title>
          <p>In the previous section, it has been demonstrated that the connectivity matrix obtained using mvMI is different from PCor and uvMI across some connections, especially for visual and limbic networks. In this section, the significance of the connections is investigated. First, the null distribution of each connection is derived using the permutation test. For each connection, we randomly shuffle the time points of the time series within regions connected by the given connection 100 times. The connections are divided into significant and insignificant categories according to comparing the true value of the connection with the 95th percentile value of the null distribution.</p>
          <p><xref rid="entropy-24-00631-f009" ref-type="fig">Figure 9</xref> illustrates the average number of insignificant connections. The connection weights have significant values when using mvMI. Comparing <xref rid="entropy-24-00631-f008" ref-type="fig">Figure 8</xref>b and <xref rid="entropy-24-00631-f009" ref-type="fig">Figure 9</xref>c shows that for the PCor measure, cells with insignificant values (blue colored) in <xref rid="entropy-24-00631-f009" ref-type="fig">Figure 9</xref>c are those which are less similar to mvMI (yellow colored) in <xref rid="entropy-24-00631-f008" ref-type="fig">Figure 8</xref>b. Therefore, the insignificant connections obtained using PCor are those that are different from mvMI. This means that mvMI makes a difference in functional connections whose PCor values are insignificant. As the FC architecture of a healthy individual has a nonrandom structure that supports different cognitive functions, we evaluate the randomness level of the proposed method, mvMI. For each participant and FC measure, the sum of nonrandomness values of all edges within the functional network is calculated. The nonrandomness of an edge tends to be small when the two nodes linked by that edge are from two different communities [<xref rid="B45-entropy-24-00631" ref-type="bibr">45</xref>]. The values for mvMI, uvMI, and PCor are 336.6, 70.07, and 27.68, respectively. The results denote that the mvMI connectivity matrix has a smaller number of random connections compared to the other two measures.</p>
        </sec>
        <sec id="sec3dot2dot3-entropy-24-00631">
          <title>3.2.3. Functional Network Similarity between Subjects</title>
          <p>As the average time series of each region is related to the homogeneity of voxel activation within that region, the representative time series of each ROI may be different for different subjects. This leads to less similarity between different subjects’ functional networks obtained using PCor. <xref rid="entropy-24-00631-f010" ref-type="fig">Figure 10</xref> shows that the similarity of the functional network between subjects for mvMI is higher than that for uvMI and PCor. As the similarity index of uvMI is larger than that of PCor, it can be concluded that both nonlinear and multivariate properties of mvMI lead to an increase in similarity between subjects.</p>
          <p>Next, we examined the similarity for different resting-state networks in <xref rid="entropy-24-00631-f011" ref-type="fig">Figure 11</xref>, both within and between networks. For PCor, the connectivity structure within networks is more similar across subjects than between-network connections. In the previous section, it has been illustrated that between-network connections have a more random organization than the within-network connections. This randomness yields less similarity between subjects. To test the effect of thresholding on the between-participant similarity value, we have added thresholded PCor to our investigation. Although thresholding has improved the similarity for between-network connections, it is still less than the mvMI similarity value for all networks. PCor has the lowest similarity value for the visual and default mode networks.</p>
        </sec>
      </sec>
    </sec>
    <sec sec-type="discussion" id="sec4-entropy-24-00631">
      <title>4. Discussion</title>
      <p>In summary, we investigated multivariate Gaussian copula mutual information as an estimator of FC. This estimator can be used as an alternative to the widely used measure PCor, which has two important limitations. These limitations motivated us to use mvMI, which is a more robust estimator than the linear correlation methods [<xref rid="B46-entropy-24-00631" ref-type="bibr">46</xref>]. As a linear measure, PCor misses nonlinear dependences. Another limitation of PCor is that it can only utilize the univariate time series as inputs. Consequently, the time series within each region should be reduced to a single time series using methods such as averaging. This dimension reduction causes a loss of voxel-level spatial information. Using simulated data, we compared the performance of mvMI with PCor from different aspects and showed that, in contrast to PCor, mvMI detects both linear and nonlinear interactions. Moreover, in situations where the ROIs have inhomogeneous activity, we showed that mvMI detected connectivity ignored by PCor. In addition, the sensitivity of FC measures to additive Gaussian noise was examined. The results illustrated less noise sensitivity of mvMI compared to the other measures.</p>
      <p>We started our investigation on real data by comparing the connectivity matrices derived using different FC estimators. We evaluated the similarity between mvMI and PCor across different resting-state networks. The connections were divided into within- and between-network connections. All FC measures were similar in estimating within-network connections. For between-network connections, they behaved differently, and mvMI outperformed the others, especially for the visual and limbic networks. To better recognize which features of mvMI lead to this capability, we included the univariate version of <italic toggle="yes">MI</italic> (uvMI) in our investigations.</p>
      <p>We verified the significance of each connection by comparing the FC value with the 95th percentile of the null distribution. We found that for between-network connections in which mvMI and PCor are considerably different, PCor connections were insignificant. For example, across visual and limbic networks, PCor was less similar to mvMI. Most insignificant connections of PCor were in these networks (see <xref rid="entropy-24-00631-f008" ref-type="fig">Figure 8</xref>b and <xref rid="entropy-24-00631-f009" ref-type="fig">Figure 9</xref>c). As an example, the similarity between PCor and mvMI for connections between the DMN and the frontoparietal network was high, and the number of insignificant PCor connections between these networks was small. In other words, the number of insignificant connections of PCor was directly correlated with its similarity with mvMI. The insignificant connections were those that were more different from mvMI. One approach to remove the insignificant PCor-based connections between networks is thresholding. In our investigation, we analyzed the performance of thresholding with mvGCM. Thresholding removed the insignificant connections but also removed some significant connections. Actually, some weak connections may be important in explaining the cognitive differences of individuals. For example, it has been declared that IQ variance is mostly explained by moderately weak, long-distance connections, with only a smaller contribution of stronger connections [<xref rid="B46-entropy-24-00631" ref-type="bibr">46</xref>]. Thus, thresholding methods may destroy some useful information.</p>
      <p>According to previous studies, a functional brain network has a nonrandom structure with a specific architecture that supports different cognition functions [<xref rid="B47-entropy-24-00631" ref-type="bibr">47</xref>]. For example, the brain has a small-world topology of short path length and high clustering. Deviation from this nonrandom topology is considered a biomarker of some diseases. It has been shown that there is an alteration in the small-world organization of the functional network of the brain of some patients. In other words, there is a deviation toward a random structure [<xref rid="B48-entropy-24-00631" ref-type="bibr">48</xref>]. In addition, there is an association between the randomness level and ADHD disorder, where there is an increase in randomness during childhood and early adulthood [<xref rid="B49-entropy-24-00631" ref-type="bibr">49</xref>]. To better understand these disorders, characterization of the nonrandomness of brain connectivity has gained attention in recent years [<xref rid="B34-entropy-24-00631" ref-type="bibr">34</xref>]. Therefore, we assessed whether the proposed method generated a random structure or not. We found that the nonrandomness level of mvMI is greater than that of the other measures.</p>
      <p>The resting-state networks, especially the DMN, are consistent across different subjects. We measured the similarity of functional networks across subjects. We found that the similarity of the connections estimated for different subjects using mvMI was the highest for both within- and between-network connections and was approximately 0.8. This result is consistent with that of a recent paper which has declared that the shared pattern of functional networks across different subjects generates an intersubject similarity of 0.822 ± 0.061 [<xref rid="B38-entropy-24-00631" ref-type="bibr">38</xref>].</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>The authors are grateful for the assistance provided by Robin Ince (Institute of Psychology &amp; Neuroscience, University of Glasgow, Glasgow, Scotland) in providing the GCMI codes and helpful comments on using the codes.</p>
    </ack>
    <fn-group>
      <fn>
        <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
      </fn>
    </fn-group>
    <notes>
      <title>Author Contributions</title>
      <p>Supervision and editing, H.S.-Z.; Writing—original draft, M.A. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Institutional Review Board Statement</title>
      <p>The study was conducted according to the guilelines of the Declaration of Helsinki, and approved by the Institutional Review Board of the local Ethics Committee of the Iran University of the Med-ical Science (IR.IUMS.REC.1396.8827, date of approval 24 April 2018).</p>
    </notes>
    <notes notes-type="data-availability">
      <title>Data Availability Statement</title>
      <p>The fMRI data used in this study are available on request from the corresponding author. The data are not publicly available due to the nature of the research in which the data were acquired where the participants did not agree for their data to be shared publicly.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="B1-entropy-24-00631">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Damaraju</surname><given-names>E.</given-names></name>
<name><surname>Allen</surname><given-names>E.A.</given-names></name>
<name><surname>Belger</surname><given-names>A.</given-names></name>
<name><surname>Ford</surname><given-names>J.M.</given-names></name>
<name><surname>McEwen</surname><given-names>S.</given-names></name>
<name><surname>Mathalon</surname><given-names>D.H.</given-names></name>
<name><surname>Mueller</surname><given-names>B.A.</given-names></name>
<name><surname>Pearlson</surname><given-names>G.D.</given-names></name>
<name><surname>Potkin</surname><given-names>S.G.</given-names></name>
<name><surname>Preda</surname><given-names>A.</given-names></name>
<etal/>
</person-group>
          <article-title>Dynamic functional connectivity analysis reveals transient states of dysconnectivity in schizophrenia</article-title>
          <source>NeuroImage Clin.</source>
          <year>2014</year>
          <volume>5</volume>
          <fpage>298</fpage>
          <lpage>308</lpage>
          <pub-id pub-id-type="doi">10.1016/j.nicl.2014.07.003</pub-id>
          <pub-id pub-id-type="pmid">25161896</pub-id>
        </element-citation>
      </ref>
      <ref id="B2-entropy-24-00631">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>He</surname><given-names>H.</given-names></name>
<name><surname>Yu</surname><given-names>Q.</given-names></name>
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Vergara</surname><given-names>V.</given-names></name>
<name><surname>Victor</surname><given-names>T.A.</given-names></name>
<name><surname>Drevets</surname><given-names>W.C.</given-names></name>
<name><surname>Savitz</surname><given-names>J.B.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Sui</surname><given-names>J.</given-names></name>
<name><surname>Calhoun</surname><given-names>V.D.</given-names></name>
</person-group>
          <article-title>Resting-state functional network connectivity in prefrontal regions differs between unmedicated patients with bipolar and major depressive disorders</article-title>
          <source>J. Affect. Disord.</source>
          <year>2016</year>
          <volume>190</volume>
          <fpage>483</fpage>
          <lpage>493</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jad.2015.10.042</pub-id>
          <pub-id pub-id-type="pmid">26551408</pub-id>
        </element-citation>
      </ref>
      <ref id="B3-entropy-24-00631">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Cao</surname><given-names>X.</given-names></name>
<name><surname>Cao</surname><given-names>Q.</given-names></name>
<name><surname>Long</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>L.</given-names></name>
<name><surname>Sui</surname><given-names>M.</given-names></name>
<name><surname>Zhu</surname><given-names>C.</given-names></name>
<name><surname>Zuo</surname><given-names>X.</given-names></name>
<name><surname>Zang</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
</person-group>
          <article-title>Abnormal resting-state functional connectivity patterns of the putamen in medication-naive children with attention deficit hyperactivity disorder</article-title>
          <source>Brain Res.</source>
          <year>2009</year>
          <volume>1303</volume>
          <fpage>195</fpage>
          <lpage>206</lpage>
          <pub-id pub-id-type="doi">10.1016/j.brainres.2009.08.029</pub-id>
          <pub-id pub-id-type="pmid">19699190</pub-id>
        </element-citation>
      </ref>
      <ref id="B4-entropy-24-00631">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Mohanty</surname><given-names>R.</given-names></name>
<name><surname>Sethares</surname><given-names>W.A.</given-names></name>
<name><surname>Nair</surname><given-names>V.A.</given-names></name>
<name><surname>Prabhakaran</surname><given-names>V.</given-names></name>
</person-group>
          <article-title>Rethinking measures of functional connectivity via feature extraction</article-title>
          <source>Sci. Rep.</source>
          <year>2020</year>
          <volume>10</volume>
          <fpage>1</fpage>
          <lpage>7</lpage>
          <pub-id pub-id-type="doi">10.1038/s41598-020-57915-w</pub-id>
          <pub-id pub-id-type="pmid">31913322</pub-id>
        </element-citation>
      </ref>
      <ref id="B5-entropy-24-00631">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Wang</surname><given-names>H.E.</given-names></name>
<name><surname>Bénar</surname><given-names>C.G.</given-names></name>
<name><surname>Quilichini</surname><given-names>P.P.</given-names></name>
<name><surname>Friston</surname><given-names>K.J.</given-names></name>
<name><surname>Jirsa</surname><given-names>V.K.</given-names></name>
<name><surname>Bernard</surname><given-names>C.</given-names></name>
</person-group>
          <article-title>A systematic framework for functional connectivity measures</article-title>
          <source>Front. Neurosci.</source>
          <year>2014</year>
          <volume>8</volume>
          <fpage>405</fpage>
          <pub-id pub-id-type="doi">10.3389/fnins.2014.00405</pub-id>
          <pub-id pub-id-type="pmid">25538556</pub-id>
        </element-citation>
      </ref>
      <ref id="B6-entropy-24-00631">
        <label>6.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
<name><surname>Wu</surname><given-names>X.</given-names></name>
<name><surname>Yao</surname><given-names>L.</given-names></name>
<name><surname>Long</surname><given-names>Z.Y.</given-names></name>
<name><surname>Lu</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>K.C.</given-names></name>
</person-group>
          <article-title>Functional connectivity in the resting brain: An analysis based on ICA</article-title>
          <source>Proceedings of the International Conference on Neural Information Processing</source>
          <conf-loc>Hong Kong, China</conf-loc>
          <conf-date>3 October 2006</conf-date>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
          <year>2006</year>
          <fpage>175</fpage>
          <lpage>182</lpage>
        </element-citation>
      </ref>
      <ref id="B7-entropy-24-00631">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Li</surname><given-names>Q.</given-names></name>
</person-group>
          <article-title>Functional connectivity inference from fMRI data using multivariate information measures</article-title>
          <source>Neural Netw.</source>
          <year>2021</year>
          <volume>146</volume>
          <fpage>85</fpage>
          <lpage>97</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2021.11.016</pub-id>
          <pub-id pub-id-type="pmid">34847461</pub-id>
        </element-citation>
      </ref>
      <ref id="B8-entropy-24-00631">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Damoiseaux</surname><given-names>J.S.</given-names></name>
<name><surname>Rombouts</surname><given-names>S.A.</given-names></name>
<name><surname>Barkhof</surname><given-names>F.</given-names></name>
<name><surname>Scheltens</surname><given-names>P.</given-names></name>
<name><surname>Stam</surname><given-names>C.J.</given-names></name>
<name><surname>Smith</surname><given-names>S.M.</given-names></name>
<name><surname>Beckmann</surname><given-names>C.F.</given-names></name>
</person-group>
          <article-title>Consistent resting-state networks across healthy subjects</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2006</year>
          <volume>103</volume>
          <fpage>13848</fpage>
          <lpage>13853</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.0601417103</pub-id>
          <pub-id pub-id-type="pmid">16945915</pub-id>
        </element-citation>
      </ref>
      <ref id="B9-entropy-24-00631">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Greicius</surname><given-names>M.D.</given-names></name>
<name><surname>Krasnow</surname><given-names>B.</given-names></name>
<name><surname>Reiss</surname><given-names>A.L.</given-names></name>
<name><surname>Menon</surname><given-names>V.</given-names></name>
</person-group>
          <article-title>Functional connectivity in the resting brain: A network analysis of the default mode hypothesis</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2003</year>
          <volume>100</volume>
          <fpage>253</fpage>
          <lpage>258</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.0135058100</pub-id>
          <pub-id pub-id-type="pmid">12506194</pub-id>
        </element-citation>
      </ref>
      <ref id="B10-entropy-24-00631">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Shirer</surname><given-names>W.R.</given-names></name>
<name><surname>Ryali</surname><given-names>S.</given-names></name>
<name><surname>Rykhlevskaia</surname><given-names>E.</given-names></name>
<name><surname>Menon</surname><given-names>V.</given-names></name>
<name><surname>Greicius</surname><given-names>M.D.</given-names></name>
</person-group>
          <article-title>Decoding subject-driven cognitive states with whole-brain connectivity patterns</article-title>
          <source>Cereb. Cortex</source>
          <year>2012</year>
          <volume>22</volume>
          <fpage>158</fpage>
          <lpage>165</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhr099</pub-id>
          <pub-id pub-id-type="pmid">21616982</pub-id>
        </element-citation>
      </ref>
      <ref id="B11-entropy-24-00631">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Salvador</surname><given-names>R.</given-names></name>
<name><surname>Suckling</surname><given-names>J.</given-names></name>
<name><surname>Coleman</surname><given-names>M.R.</given-names></name>
<name><surname>Pickard</surname><given-names>J.D.</given-names></name>
<name><surname>Menon</surname><given-names>D.</given-names></name>
<name><surname>Bullmore</surname><given-names>E.D.</given-names></name>
</person-group>
          <article-title>Neurophysiological architecture of functional magnetic resonance images of human brain</article-title>
          <source>Cereb. Cortex</source>
          <year>2005</year>
          <volume>15</volume>
          <fpage>1332</fpage>
          <lpage>1342</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhi016</pub-id>
          <pub-id pub-id-type="pmid">15635061</pub-id>
        </element-citation>
      </ref>
      <ref id="B12-entropy-24-00631">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kriegeskorte</surname><given-names>N.</given-names></name>
<name><surname>Mur</surname><given-names>M.</given-names></name>
<name><surname>Bandettini</surname><given-names>P.A.</given-names></name>
</person-group>
          <article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title>
          <source>Front. Syst. Neurosci.</source>
          <year>2008</year>
          <volume>2</volume>
          <fpage>4</fpage>
          <pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id>
          <pub-id pub-id-type="pmid">19104670</pub-id>
        </element-citation>
      </ref>
      <ref id="B13-entropy-24-00631">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Norman</surname><given-names>K.A.</given-names></name>
<name><surname>Polyn</surname><given-names>S.M.</given-names></name>
<name><surname>Detre</surname><given-names>G.J.</given-names></name>
<name><surname>Haxby</surname><given-names>J.V.</given-names></name>
</person-group>
          <article-title>Beyond mind-reading: Multi-voxel pattern analysis of fMRI data</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2006</year>
          <volume>10</volume>
          <fpage>424</fpage>
          <lpage>430</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id>
          <pub-id pub-id-type="pmid">16899397</pub-id>
        </element-citation>
      </ref>
      <ref id="B14-entropy-24-00631">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Geerligs</surname><given-names>L.</given-names></name>
<name><surname>Henson</surname><given-names>R.N.</given-names></name>
</person-group>
          <article-title>Functional connectivity and structural covariance between regions of interest can be measured more accurately using multivariate distance correlation</article-title>
          <source>NeuroImage</source>
          <year>2016</year>
          <volume>135</volume>
          <fpage>16</fpage>
          <lpage>31</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.047</pub-id>
          <pub-id pub-id-type="pmid">27114055</pub-id>
        </element-citation>
      </ref>
      <ref id="B15-entropy-24-00631">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Sundaram</surname><given-names>P.</given-names></name>
<name><surname>Luessi</surname><given-names>M.</given-names></name>
<name><surname>Bianciardi</surname><given-names>M.</given-names></name>
<name><surname>Stufflebeam</surname><given-names>S.</given-names></name>
<name><surname>Hämäläinen</surname><given-names>M.</given-names></name>
<name><surname>Solo</surname><given-names>V.</given-names></name>
</person-group>
          <article-title>Individual Resting-State Brain Networks Enabled by Massive Multivariate Conditional Mutual Information</article-title>
          <source>IEEE Trans. Med. Imaging</source>
          <year>2019</year>
          <volume>39</volume>
          <fpage>1957</fpage>
          <lpage>1966</lpage>
          <pub-id pub-id-type="doi">10.1109/TMI.2019.2962517</pub-id>
          <pub-id pub-id-type="pmid">31880547</pub-id>
        </element-citation>
      </ref>
      <ref id="B16-entropy-24-00631">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Haxby</surname><given-names>J.V.</given-names></name>
<name><surname>Connolly</surname><given-names>A.C.</given-names></name>
<name><surname>Guntupalli</surname><given-names>J.S.</given-names></name>
</person-group>
          <article-title>Decoding neural representational spaces using multivariate pattern analysis</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <year>2014</year>
          <volume>37</volume>
          <fpage>435</fpage>
          <lpage>456</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id>
          <pub-id pub-id-type="pmid">25002277</pub-id>
        </element-citation>
      </ref>
      <ref id="B17-entropy-24-00631">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Tong</surname><given-names>F.</given-names></name>
<name><surname>Pratte</surname><given-names>M.S.</given-names></name>
</person-group>
          <article-title>Decoding patterns of human brain activity</article-title>
          <source>Annu. Rev. Psychol.</source>
          <year>2012</year>
          <volume>63</volume>
          <fpage>483</fpage>
          <lpage>509</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100412</pub-id>
          <pub-id pub-id-type="pmid">21943172</pub-id>
        </element-citation>
      </ref>
      <ref id="B18-entropy-24-00631">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Hlinka</surname><given-names>J.</given-names></name>
<name><surname>Paluš</surname><given-names>M.</given-names></name>
<name><surname>Vejmelka</surname><given-names>M.</given-names></name>
<name><surname>Mantini</surname><given-names>D.</given-names></name>
<name><surname>Corbetta</surname><given-names>M.</given-names></name>
</person-group>
          <article-title>Functional connectivity in resting-state fMRI: Is linear correlation sufficient?</article-title>
          <source>Neuroimage</source>
          <year>2011</year>
          <volume>54</volume>
          <fpage>2218</fpage>
          <lpage>2225</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.08.042</pub-id>
          <pub-id pub-id-type="pmid">20800096</pub-id>
        </element-citation>
      </ref>
      <ref id="B19-entropy-24-00631">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Mišić</surname><given-names>B.</given-names></name>
<name><surname>Sporns</surname><given-names>O.</given-names></name>
</person-group>
          <article-title>From regions to connections and networks: New bridges between brain and behavior</article-title>
          <source>Curr. Opin. Neurobiol.</source>
          <year>2016</year>
          <volume>40</volume>
          <fpage>1</fpage>
          <lpage>7</lpage>
          <pub-id pub-id-type="doi">10.1016/j.conb.2016.05.003</pub-id>
          <pub-id pub-id-type="pmid">27209150</pub-id>
        </element-citation>
      </ref>
      <ref id="B20-entropy-24-00631">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Xie</surname><given-names>X.</given-names></name>
<name><surname>Cao</surname><given-names>Z.</given-names></name>
<name><surname>Weng</surname><given-names>X.</given-names></name>
</person-group>
          <article-title>Spatiotemporal nonlinearity in resting-state fMRI of the human brain</article-title>
          <source>Neuroimage</source>
          <year>2008</year>
          <volume>40</volume>
          <fpage>1672</fpage>
          <lpage>1685</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.01.007</pub-id>
          <pub-id pub-id-type="pmid">18316208</pub-id>
        </element-citation>
      </ref>
      <ref id="B21-entropy-24-00631">
        <label>21.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
<name><surname>Deshpande</surname><given-names>G.</given-names></name>
<name><surname>LaConte</surname><given-names>S.</given-names></name>
<name><surname>Peltier</surname><given-names>S.</given-names></name>
<name><surname>Hu</surname><given-names>X.</given-names></name>
</person-group>
          <article-title>Connectivity analysis of human functional MRI data: From linear to nonlinear and static to dynamic</article-title>
          <source>Proceedings of the International Workshop on Medical Imaging and Virtual Reality</source>
          <conf-loc>Shanghai, China</conf-loc>
          <conf-date>17 August 2006</conf-date>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
          <year>2006</year>
          <fpage>17</fpage>
          <lpage>24</lpage>
        </element-citation>
      </ref>
      <ref id="B22-entropy-24-00631">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Jeong</surname><given-names>S.O.</given-names></name>
<name><surname>Kang</surname><given-names>J.</given-names></name>
<name><surname>Pae</surname><given-names>C.</given-names></name>
<name><surname>Eo</surname><given-names>J.</given-names></name>
<name><surname>Park</surname><given-names>S.M.</given-names></name>
<name><surname>Son</surname><given-names>J.</given-names></name>
<name><surname>Park</surname><given-names>H.J.</given-names></name>
</person-group>
          <article-title>Empirical Bayes estimation of pairwise maximum entropy model for nonlinear brain state dynamics</article-title>
          <source>NeuroImage</source>
          <year>2021</year>
          <volume>244</volume>
          <fpage>118618</fpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118618</pub-id>
          <pub-id pub-id-type="pmid">34571159</pub-id>
        </element-citation>
      </ref>
      <ref id="B23-entropy-24-00631">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Su</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>L.</given-names></name>
<name><surname>Shen</surname><given-names>H.</given-names></name>
<name><surname>Feng</surname><given-names>G.</given-names></name>
<name><surname>Hu</surname><given-names>D.</given-names></name>
</person-group>
          <article-title>Discriminative analysis of non-linear brain connectivity in schizophrenia: An fMRI Study</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2013</year>
          <volume>7</volume>
          <fpage>702</fpage>
          <pub-id pub-id-type="doi">10.3389/fnhum.2013.00702</pub-id>
          <pub-id pub-id-type="pmid">24155713</pub-id>
        </element-citation>
      </ref>
      <ref id="B24-entropy-24-00631">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kumar</surname><given-names>S.</given-names></name>
<name><surname>Yoo</surname><given-names>K.</given-names></name>
<name><surname>Rosenberg</surname><given-names>M.D.</given-names></name>
<name><surname>Scheinost</surname><given-names>D.</given-names></name>
<name><surname>Constable</surname><given-names>R.T.</given-names></name>
<name><surname>Zhang</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>C.S.</given-names></name>
<name><surname>Chun</surname><given-names>M.M.</given-names></name>
</person-group>
          <article-title>An information network flow approach for measuring functional connectivity and predicting behavior</article-title>
          <source>Brain Behav.</source>
          <year>2019</year>
          <volume>9</volume>
          <fpage>e01346</fpage>
          <pub-id pub-id-type="doi">10.1002/brb3.1346</pub-id>
          <pub-id pub-id-type="pmid">31286688</pub-id>
        </element-citation>
      </ref>
      <ref id="B25-entropy-24-00631">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Keshmiri</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Entropy and the brain: An overview</article-title>
          <source>Entropy</source>
          <year>2020</year>
          <volume>22</volume>
          <elocation-id>917</elocation-id>
          <pub-id pub-id-type="doi">10.3390/e22090917</pub-id>
          <pub-id pub-id-type="pmid">33286686</pub-id>
        </element-citation>
      </ref>
      <ref id="B26-entropy-24-00631">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Reing</surname><given-names>K.</given-names></name>
<name><surname>Ver Steeg</surname><given-names>G.</given-names></name>
<name><surname>Galstyan</surname><given-names>A.</given-names></name>
</person-group>
          <article-title>Discovering Higher-Order Interactions Through Neural Information Decomposition</article-title>
          <source>Entropy</source>
          <year>2021</year>
          <volume>23</volume>
          <elocation-id>79</elocation-id>
          <pub-id pub-id-type="doi">10.3390/e23010079</pub-id>
        </element-citation>
      </ref>
      <ref id="B27-entropy-24-00631">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Gençağa</surname><given-names>D.</given-names></name>
<name><surname>Şengül Ayan</surname><given-names>S.</given-names></name>
<name><surname>Farnoudkia</surname><given-names>H.</given-names></name>
<name><surname>Okuyucu</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Statistical approaches for the analysis of dependency among neurons under noise</article-title>
          <source>Entropy</source>
          <year>2020</year>
          <volume>22</volume>
          <elocation-id>387</elocation-id>
          <pub-id pub-id-type="doi">10.3390/e22040387</pub-id>
        </element-citation>
      </ref>
      <ref id="B28-entropy-24-00631">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Abazid</surname><given-names>M.</given-names></name>
<name><surname>Houmani</surname><given-names>N.</given-names></name>
<name><surname>Boudy</surname><given-names>J.</given-names></name>
<name><surname>Dorizzi</surname><given-names>B.</given-names></name>
<name><surname>Mariani</surname><given-names>J.</given-names></name>
<name><surname>Kinugawa</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>A comparative study of functional connectivity measures for brain network analysis in the context of AD detection with EEG</article-title>
          <source>Entropy</source>
          <year>2021</year>
          <volume>23</volume>
          <elocation-id>1553</elocation-id>
          <pub-id pub-id-type="doi">10.3390/e23111553</pub-id>
          <pub-id pub-id-type="pmid">34828251</pub-id>
        </element-citation>
      </ref>
      <ref id="B29-entropy-24-00631">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Ince</surname><given-names>R.A.</given-names></name>
<name><surname>Giordano</surname><given-names>B.L.</given-names></name>
<name><surname>Kayser</surname><given-names>C.</given-names></name>
<name><surname>Rousselet</surname><given-names>G.A.</given-names></name>
<name><surname>Gross</surname><given-names>J.</given-names></name>
<name><surname>Schyns</surname><given-names>P.G.</given-names></name>
</person-group>
          <article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>1541</fpage>
          <lpage>1573</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id>
          <pub-id pub-id-type="pmid">27860095</pub-id>
        </element-citation>
      </ref>
      <ref id="B30-entropy-24-00631">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Liao</surname><given-names>X.</given-names></name>
<name><surname>Vasilakos</surname><given-names>A.V.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
</person-group>
          <article-title>Small-world human brain networks: Perspectives and challenges</article-title>
          <source>Neurosci. Biobehav. Rev.</source>
          <year>2017</year>
          <volume>77</volume>
          <fpage>286</fpage>
          <lpage>300</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.03.018</pub-id>
          <pub-id pub-id-type="pmid">28389343</pub-id>
        </element-citation>
      </ref>
      <ref id="B31-entropy-24-00631">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Whitfield-Gabrieli</surname><given-names>S.</given-names></name>
<name><surname>Nieto-Castanon</surname><given-names>A.</given-names></name>
</person-group>
          <article-title>Conn: A functional connectivity toolbox for correlated and anticorrelated brain networks</article-title>
          <source>Brain Connect.</source>
          <year>2012</year>
          <volume>2</volume>
          <fpage>125</fpage>
          <lpage>141</lpage>
          <pub-id pub-id-type="doi">10.1089/brain.2012.0073</pub-id>
          <pub-id pub-id-type="pmid">22642651</pub-id>
        </element-citation>
      </ref>
      <ref id="B32-entropy-24-00631">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Safaai</surname><given-names>H.</given-names></name>
<name><surname>Onken</surname><given-names>A.</given-names></name>
<name><surname>Harvey</surname><given-names>C.D.</given-names></name>
<name><surname>Panzeri</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Information estimation using nonparametric copulas</article-title>
          <source>Phys. Rev. E</source>
          <year>2018</year>
          <volume>98</volume>
          <fpage>053302</fpage>
          <pub-id pub-id-type="doi">10.1103/PhysRevE.98.053302</pub-id>
          <pub-id pub-id-type="pmid">30984901</pub-id>
        </element-citation>
      </ref>
      <ref id="B33-entropy-24-00631">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Giraudo</surname><given-names>M.T.</given-names></name>
<name><surname>Sacerdote</surname><given-names>L.</given-names></name>
<name><surname>Sirovich</surname><given-names>R.</given-names></name>
</person-group>
          <article-title>Non–parametric estimation of mutual information through the entropy of the linkage</article-title>
          <source>Entropy</source>
          <year>2013</year>
          <volume>15</volume>
          <fpage>5154</fpage>
          <lpage>5177</lpage>
          <pub-id pub-id-type="doi">10.3390/e15125154</pub-id>
        </element-citation>
      </ref>
      <ref id="B34-entropy-24-00631">
        <label>34.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
<name><surname>Nelsen</surname><given-names>R.B.</given-names></name>
</person-group>
          <source>An Introduction to Copulas</source>
          <publisher-name>Springer Science &amp; Business Media</publisher-name>
          <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
          <year>2007</year>
        </element-citation>
      </ref>
      <ref id="B35-entropy-24-00631">
        <label>35.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
<name><surname>Cover</surname><given-names>T.M.</given-names></name>
</person-group>
          <source>Elements of Information Theory</source>
          <publisher-name>John Wiley &amp; Sons</publisher-name>
          <publisher-loc>Hoboken, NJ, USA</publisher-loc>
          <year>1999</year>
        </element-citation>
      </ref>
      <ref id="B36-entropy-24-00631">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Calsaverini</surname><given-names>R.S.</given-names></name>
<name><surname>Vicente</surname><given-names>R.</given-names></name>
</person-group>
          <article-title>An information-theoretic approach to statistical dependence: Copula information</article-title>
          <source>EPL (Europhys. Lett.)</source>
          <year>2009</year>
          <volume>88</volume>
          <fpage>68003</fpage>
          <pub-id pub-id-type="doi">10.1209/0295-5075/88/68003</pub-id>
        </element-citation>
      </ref>
      <ref id="B37-entropy-24-00631">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Vergara</surname><given-names>V.M.</given-names></name>
<name><surname>Yu</surname><given-names>Q.</given-names></name>
<name><surname>Calhoun</surname><given-names>V.D.</given-names></name>
</person-group>
          <article-title>A method to assess randomness of functional connectivity matrices</article-title>
          <source>J. Neurosci. Methods</source>
          <year>2018</year>
          <volume>303</volume>
          <fpage>146</fpage>
          <lpage>158</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2018.03.015</pub-id>
          <pub-id pub-id-type="pmid">29601886</pub-id>
        </element-citation>
      </ref>
      <ref id="B38-entropy-24-00631">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Li</surname><given-names>Q.</given-names></name>
<name><surname>Zhao</surname><given-names>Y.</given-names></name>
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Ma</surname><given-names>B.</given-names></name>
<name><surname>Fu</surname><given-names>Z.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Decomposition of individual-specific and individual-shared components from resting-state functional connectivity using a multi-task machine learning method</article-title>
          <source>NeuroImage</source>
          <year>2021</year>
          <volume>238</volume>
          <fpage>118252</fpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118252</pub-id>
          <pub-id pub-id-type="pmid">34116155</pub-id>
        </element-citation>
      </ref>
      <ref id="B39-entropy-24-00631">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Jann</surname><given-names>K.</given-names></name>
<name><surname>Gee</surname><given-names>D.G.</given-names></name>
<name><surname>Kilroy</surname><given-names>E.</given-names></name>
<name><surname>Schwab</surname><given-names>S.</given-names></name>
<name><surname>Smith</surname><given-names>R.X.</given-names></name>
<name><surname>Cannon</surname><given-names>T.D.</given-names></name>
<name><surname>Wang</surname><given-names>D.J.</given-names></name>
</person-group>
          <article-title>Functional connectivity in BOLD and CBF data: Similarity and reliability of resting brain networks</article-title>
          <source>Neuroimage</source>
          <year>2015</year>
          <volume>106</volume>
          <fpage>111</fpage>
          <lpage>122</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.11.028</pub-id>
          <pub-id pub-id-type="pmid">25463468</pub-id>
        </element-citation>
      </ref>
      <ref id="B40-entropy-24-00631">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Du</surname><given-names>Y.</given-names></name>
<name><surname>Fu</surname><given-names>Z.</given-names></name>
<name><surname>Xing</surname><given-names>Y.</given-names></name>
<name><surname>Lin</surname><given-names>D.</given-names></name>
<name><surname>Pearlson</surname><given-names>G.</given-names></name>
<name><surname>Kochunov</surname><given-names>P.</given-names></name>
<name><surname>Hong</surname><given-names>L.E.</given-names></name>
<name><surname>Qi</surname><given-names>S.</given-names></name>
<name><surname>Salman</surname><given-names>M.</given-names></name>
<name><surname>Abrol</surname><given-names>A.</given-names></name>
<etal/>
</person-group>
          <article-title>Evidence of shared and distinct functional and structural brain signatures in schizophrenia and autism spectrum disorder</article-title>
          <source>Commun. Biol.</source>
          <year>2021</year>
          <volume>4</volume>
          <fpage>1</fpage>
          <lpage>6</lpage>
          <pub-id pub-id-type="doi">10.1038/s42003-021-02592-2</pub-id>
          <pub-id pub-id-type="pmid">33398033</pub-id>
        </element-citation>
      </ref>
      <ref id="B41-entropy-24-00631">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Schaefer</surname><given-names>A.</given-names></name>
<name><surname>Kong</surname><given-names>R.</given-names></name>
<name><surname>Gordon</surname><given-names>E.M.</given-names></name>
<name><surname>Laumann</surname><given-names>T.O.</given-names></name>
<name><surname>Zuo</surname><given-names>X.N.</given-names></name>
<name><surname>Holmes</surname><given-names>A.J.</given-names></name>
<name><surname>Eickhoff</surname><given-names>S.B.</given-names></name>
<name><surname>Yeo</surname><given-names>B.T.</given-names></name>
</person-group>
          <article-title>Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI</article-title>
          <source>Cereb. Cortex</source>
          <year>2018</year>
          <volume>28</volume>
          <fpage>3095</fpage>
          <lpage>3114</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhx179</pub-id>
          <pub-id pub-id-type="pmid">28981612</pub-id>
        </element-citation>
      </ref>
      <ref id="B42-entropy-24-00631">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Thomas Yeo</surname><given-names>B.T.</given-names></name>
<name><surname>Krienen</surname><given-names>F.M.</given-names></name>
<name><surname>Sepulcre</surname><given-names>J.</given-names></name>
<name><surname>Sabuncu</surname><given-names>M.R.</given-names></name>
<name><surname>Lashkari</surname><given-names>D.</given-names></name>
<name><surname>Hollinshead</surname><given-names>M.</given-names></name>
<name><surname>Roffman</surname><given-names>J.L.</given-names></name>
<name><surname>Smoller</surname><given-names>J.W.</given-names></name>
<name><surname>Zöllei</surname><given-names>L.</given-names></name>
<name><surname>Polimeni</surname><given-names>J.R.</given-names></name>
<etal/>
</person-group>
          <article-title>The organization of the human cerebral cortex estimated by intrinsic functional connectivity</article-title>
          <source>J. Neurophysiol.</source>
          <year>2011</year>
          <volume>106</volume>
          <fpage>1125</fpage>
          <lpage>1165</lpage>
          <pub-id pub-id-type="doi">10.1152/jn.00338.2011</pub-id>
          <pub-id pub-id-type="pmid">21653723</pub-id>
        </element-citation>
      </ref>
      <ref id="B43-entropy-24-00631">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Power</surname><given-names>J.D.</given-names></name>
<name><surname>Barnes</surname><given-names>K.A.</given-names></name>
<name><surname>Snyder</surname><given-names>A.Z.</given-names></name>
<name><surname>Schlaggar</surname><given-names>B.L.</given-names></name>
<name><surname>Petersen</surname><given-names>S.E.</given-names></name>
</person-group>
          <article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title>
          <source>Neuroimage</source>
          <year>2012</year>
          <volume>59</volume>
          <fpage>2142</fpage>
          <lpage>2154</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id>
          <pub-id pub-id-type="pmid">22019881</pub-id>
        </element-citation>
      </ref>
      <ref id="B44-entropy-24-00631">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Rai</surname><given-names>S.</given-names></name>
<name><surname>Griffiths</surname><given-names>K.R.</given-names></name>
<name><surname>Breukelaar</surname><given-names>I.A.</given-names></name>
<name><surname>Barreiros</surname><given-names>A.R.</given-names></name>
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Boyce</surname><given-names>P.</given-names></name>
<name><surname>Hazell</surname><given-names>P.</given-names></name>
<name><surname>Foster</surname><given-names>S.L.</given-names></name>
<name><surname>Malhi</surname><given-names>G.S.</given-names></name>
<name><surname>Harris</surname><given-names>A.W.</given-names></name>
<etal/>
</person-group>
          <article-title>Default-mode and fronto-parietal network connectivity during rest distinguishes asymptomatic patients with bipolar disorder and major depressive disorder</article-title>
          <source>Transl. Psychiatry</source>
          <year>2021</year>
          <volume>11</volume>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">10.1038/s41398-021-01660-9</pub-id>
          <pub-id pub-id-type="pmid">33414379</pub-id>
        </element-citation>
      </ref>
      <ref id="B45-entropy-24-00631">
        <label>45.</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
<name><surname>Ying</surname><given-names>X.</given-names></name>
<name><surname>Wu</surname><given-names>X.</given-names></name>
</person-group>
          <article-title>On randomness measures for social networks</article-title>
          <source>Proceedings of the 2009 SIAM International Conference on Data Mining</source>
          <conf-loc>Sparks, NV, USA</conf-loc>
          <conf-date>30 April 2009</conf-date>
          <publisher-name>Society for Industrial and Applied Mathematics</publisher-name>
          <publisher-loc>Philadelphia, PA, USA</publisher-loc>
          <year>2009</year>
          <fpage>709</fpage>
          <lpage>720</lpage>
        </element-citation>
      </ref>
      <ref id="B46-entropy-24-00631">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Santarnecchi</surname><given-names>E.</given-names></name>
<name><surname>Galli</surname><given-names>G.</given-names></name>
<name><surname>Polizzotto</surname><given-names>N.R.</given-names></name>
<name><surname>Rossi</surname><given-names>A.</given-names></name>
<name><surname>Rossi</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Efficiency of weak brain connections support general cognitive functioning</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2014</year>
          <volume>35</volume>
          <fpage>4566</fpage>
          <lpage>4582</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.22495</pub-id>
          <pub-id pub-id-type="pmid">24585433</pub-id>
        </element-citation>
      </ref>
      <ref id="B47-entropy-24-00631">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Menon</surname><given-names>S.S.</given-names></name>
<name><surname>Krishnamurthy</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>A study of brain neuronal and functional complexities estimated using multiscale entropy in healthy young adults</article-title>
          <source>Entropy</source>
          <year>2019</year>
          <volume>21</volume>
          <elocation-id>995</elocation-id>
          <pub-id pub-id-type="doi">10.3390/e21100995</pub-id>
        </element-citation>
      </ref>
      <ref id="B48-entropy-24-00631">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Hua</surname><given-names>B.</given-names></name>
<name><surname>Ding</surname><given-names>X.</given-names></name>
<name><surname>Xiong</surname><given-names>M.</given-names></name>
<name><surname>Zhang</surname><given-names>F.</given-names></name>
<name><surname>Luo</surname><given-names>Y.</given-names></name>
<name><surname>Ding</surname><given-names>J.</given-names></name>
<name><surname>Ding</surname><given-names>Z.</given-names></name>
</person-group>
          <article-title>Alterations of functional and structural connectivity in patients with brain metastases</article-title>
          <source>PLoS ONE</source>
          <year>2020</year>
          <volume>15</volume>
          <elocation-id>e0233833</elocation-id>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0233833</pub-id>
          <pub-id pub-id-type="pmid">32470024</pub-id>
        </element-citation>
      </ref>
      <ref id="B49-entropy-24-00631">
        <label>49.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Zuo</surname><given-names>C.</given-names></name>
<name><surname>Xu</surname><given-names>Q.</given-names></name>
<name><surname>Liao</surname><given-names>S.</given-names></name>
<name><surname>Kanji</surname><given-names>M.</given-names></name>
<name><surname>Wang</surname><given-names>D.</given-names></name>
</person-group>
          <article-title>Altered resting functional network topology assessed using graph theory in youth with attention-deficit/hyperactivity disorder</article-title>
          <source>Prog. Neuro-Psychopharmacol. Biol. Psychiatry</source>
          <year>2020</year>
          <volume>98</volume>
          <fpage>109796</fpage>
          <pub-id pub-id-type="doi">10.1016/j.pnpbp.2019.109796</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
  <floats-group>
    <fig position="float" id="entropy-24-00631-f001">
      <label>Figure 1</label>
      <caption>
        <p>Different covariance matrices used to simulate different levels of homogeneity within the first region: (<bold>a</bold>) correlated activities, i.e., constant matrix; (<bold>b</bold>) uncorrelated activities obtained using an identity matrix; (<bold>c</bold>) mixtures of correlated and anticorrelated activities within the first region. These covariance matrices are used in generating the first region’s activities using a multivariate normal distribution.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g001" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f002">
      <label>Figure 2</label>
      <caption>
        <p>We simulate a one-to-one voxel mapping which shows a linear interaction between two regions. Each bar value shows the average performance of detecting the linear interaction across 100 repetitions by using four different FC measures, PCor, SVD, uvMI, and mvMI. Error bars show the standard deviation of the performance across repetitions. The performance is defined as the distance between the FC value derived using each measure and the 95th percentile of the null distribution. The first region’s activities are generated by a multivariate normal distribution with three different covariance matrices shown in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>. For each FC measure, the obtained performance using different covariance matrices is illustrated through different colors. Blue bars are related to the covariance matrix in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>a which simulates the homogeneous or correlated activities within the first region. Inhomogeneous or uncorrelated activity results generated by the covariance matrix in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>b are shown in orange color. Yellow bars are the result of using the covariance matrix in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>c which simulates both correlated and anticorrelated activities within the first region. mvMI can detect the linear interaction for all covariance matrices better than the other measures. For linear interaction, uvMI performs as well as PCor. SVD has a weak performance in detecting interaction when activities within one region are anticorrelated.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g002" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f003">
      <label>Figure 3</label>
      <caption>
        <p>We simulate a nonlinear interaction between two regions. Each bar shows the performance of each FC measure PCor, SVD, uvMI, and mvMI in detecting the nonlinear interaction. Error bars correspond to the standard deviation of the performance across all repetitions. Performance is defined as the distance between the FC value derived using each measure and the 95th percentile of the null distribution. To simulate the nonlinear interaction, the activities within the second region are derived via a second power function of the first region’s activities. For each FC measure, we use three covariance matrices in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref> to generate the first region’s activities. Performances obtained by using different covariance matrices are represented in different colors. The result of using homogeneous or correlated activities (using <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>a covariance matrix) and inhomogeneous or uncorrelated activities (using covariance matrix in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>b) are shown by blue and orange bars, respectively. The performance related to the third covariance matrix (<xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>c), which contains two constant values, positive and negative, has been illustrated by yellow bars. This one simulates a condition where some voxels within each region are positively correlated while the others are anticorrelated, i.e., negatively correlated. Each bar shows the mean performance of each measure across 100 repetitions. Pcor and SVD as two linear measures fail to detect the nonlinear interaction. uvMI as a measure that uses the average activity within each region only detects the nonlinear interaction in the homogeneous condition. The mvMI correctly detects this interaction using different covariance matrices.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g003" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f004">
      <label>Figure 4</label>
      <caption>
        <p>Simulation results of the performance of four different connectivity measures, PCor, SVD, uvMI, and mvMI, in detecting a multivariate interaction. Each bar shows the mean performance of each measure across 100 repetitions. Error bars correspond to the standard deviation of the performance across the different repetitions. Performance is defined as the distance between the value derived using each FC measure and the 95th percentile of the null distribution. Multivariate interaction is simulated by using a transformation matrix whose elements are derived using the multivariate normal distribution shown in panel a. The second region’s activities are derived by multiplying the first region’s activities by this transform matrix. The first region’s activities are generated using a multivariate normal distribution with three different covariance matrices in <xref rid="entropy-24-00631-f001" ref-type="fig">Figure 1</xref>, which are shown in three colors. Blue represents the homogeneous or correlated activities within the first region. Orange bars are related to the result of using inhomogeneous or uncorrelated activities. The third covariance matrix, which contains two constant values, positive and negative, has been illustrated in yellow. This one simulates a mixed condition of correlated and anticorrelated activities. PCor, as a measure that uses the average activity of each region, does not detect this connection. mvMI, as a multivariate measure independent of the regional homogeneity, detects this multivariate connection.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g004" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f005">
      <label>Figure 5</label>
      <caption>
        <p>Performance of four different connectivity measures, PCor, SVD, uvMI, and mvMI, in detecting a linear interaction while the second region contains an additional same noise across all voxels named structural noise. Performance is defined as the distance between the FC value derived using each measure and the 95th percentile of the null distribution. Each bar illustrates the average performance across repetitions of each measure in detecting the FC interaction. Standard deviation of performance across repetitions is represented by the error bar on top of each bar. The first region’s activities are generated using a multivariate normal distribution with three different covariance matrices with different levels of homogeneity shown in different colors. Blue and orange are related to the homogeneous and inhomogeneous activities within the first region, respectively. The yellow bars are related to an intermediate condition, in which some voxels are positively correlated while the others are anticorrelated. All measures except mvMI are sensitive to the noise and cannot detect the linear interaction.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g005" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f006">
      <label>Figure 6</label>
      <caption>
        <p>Functional connectivity matrices derived using different estimators (<bold>a</bold>–<bold>c</bold>) for a subject’s fMRI data parcellated by the 400-ROI Schaefer parcellation. (<bold>d</bold>) Visualization of the 400-parcel parcellation in fslr32k space; parcels were colored to match Yeo 7-network parcellation.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g006" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f007">
      <label>Figure 7</label>
      <caption>
        <p>Average DMN FCs across subjects estimated by different measures, uvMI, mvMI, PCor, and SVD. Moreover, thresholded PCor and SVD with different threshold levels including 10%, 20%, and 30% of the maximum values of FCs are added to the results. Average DMN FC illustration is separated for within- and between-network connections. (<bold>a</bold>) Connections that connect a region within the DMN with a node in other networks (<bold>b</bold>). Connections whose endpoint nodes are within the DMN. PCor and SVD have the highest median values as they have more spurious connections. Thresholding removes some DMN connections and leads to smaller boxes that represent the less diverse connection values.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g007" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f008">
      <label>Figure 8</label>
      <caption>
        <p>Similarity between FCs derived using mvMI with uvMI and PCor for different networks. Similarity is separated for different networks. Diagonal squares illustrate the FC measures’ similarity of connections whose connected nodes are on the same networks. Other squares represent the similarity between different measures considering the connections that connect two nodes of different networks. Similarity is calculated using the rank correlation. (<bold>a</bold>) Similarity between mvMI and uvMI; (<bold>b</bold>) similarity between mvMI and Pearson. Darker squares indicate more similarity between two related FC measures. mvMI and uvMI are more similar for within-network connections. PCor and mvMI are less similar in the visual network, while they are more similar in the DMN.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g008" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f009">
      <label>Figure 9</label>
      <caption>
        <p>Average number of insignificant FCs across all participants for different FC measures. The result is separated for different networks. Diagonal squares show the within-network connections while the others are related to the connections that connect two nodes in different networks. Significance is determined by comparing each FC value with the 95th percentile of the null distribution obtained using the permutation test. mvMI has the lowest number of insignificant connections in comparison with other measures.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g009" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f010">
      <label>Figure 10</label>
      <caption>
        <p>Between-subject similarity of FC matrices derived using different FC estimators, uvMI, mvMI, and PCor. Similarity between subjects is defined as the correlation between each subject’s FC matrix and the average FC matrix across subjects. Each subject correlation value is a point of the box. Each box is related to an FC estimator. FC matrices estimated by mvMI are more similar across different subjects. Using PCor leads to less similarity between subjects.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g010" position="float"/>
    </fig>
    <fig position="float" id="entropy-24-00631-f011">
      <label>Figure 11</label>
      <caption>
        <p>Between-subject similarity separated for within- and between-network connections. For each subject, the similarity is obtained using the correlation between the subject’s FC matrix and the average FC matrix across all subjects. Each bar illustrates the average similarity values across different subjects. (<bold>a</bold>) The result for connections that connect two regions located in the same network, or within-network connections. (<bold>b</bold>) The similarity between subjects for connections that connect two regions of different networks, called between-network connections. Different measures are separated by different colors. Each group of bars is related to a special network. Between-network connections derived using mvMI are more similar across different subjects than the other FC measures. Using PCor as a measure of FC leads to less similarity between different subjects for between-network connections.</p>
      </caption>
      <graphic xlink:href="entropy-24-00631-g011" position="float"/>
    </fig>
  </floats-group>
</article>
