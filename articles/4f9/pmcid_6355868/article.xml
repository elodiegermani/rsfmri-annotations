<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
      <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
      <journal-title-group>
        <journal-title>Scientific Reports</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2045-2322</issn>
      <publisher>
        <publisher-name>Nature Publishing Group UK</publisher-name>
        <publisher-loc>London</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">30705340</article-id>
      <article-id pub-id-type="pmc">6355868</article-id>
      <article-id pub-id-type="publisher-id">37387</article-id>
      <article-id pub-id-type="doi">10.1038/s41598-018-37387-9</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Multi-Channel 3D Deep Feature Learning for Survival Time Prediction of Brain Tumor Patients Using Multi-Modal Neuroimages</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" equal-contrib="yes">
          <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0385-8988</contrib-id>
          <name>
            <surname>Nie</surname>
            <given-names>Dong</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author" equal-contrib="yes">
          <name>
            <surname>Lu</surname>
            <given-names>Junfeng</given-names>
          </name>
          <xref ref-type="aff" rid="Aff3">3</xref>
          <xref ref-type="aff" rid="Aff4">4</xref>
        </contrib>
        <contrib contrib-type="author" equal-contrib="yes">
          <name>
            <surname>Zhang</surname>
            <given-names>Han</given-names>
          </name>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0579-7763</contrib-id>
          <name>
            <surname>Adeli</surname>
            <given-names>Ehsan</given-names>
          </name>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9548-0411</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Jun</given-names>
          </name>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>Zhengda</given-names>
          </name>
          <xref ref-type="aff" rid="Aff3">3</xref>
          <xref ref-type="aff" rid="Aff4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>LuYan</given-names>
          </name>
          <xref ref-type="aff" rid="Aff5">5</xref>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Wang</surname>
            <given-names>Qian</given-names>
          </name>
          <address>
            <email>wang.qian@sjtu.edu.cn</email>
          </address>
          <xref ref-type="aff" rid="Aff5">5</xref>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Wu</surname>
            <given-names>Jinsong</given-names>
          </name>
          <address>
            <email>wjsongc@126.com</email>
          </address>
          <xref ref-type="aff" rid="Aff3">3</xref>
          <xref ref-type="aff" rid="Aff4">4</xref>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Shen</surname>
            <given-names>Dinggang</given-names>
          </name>
          <address>
            <email>dgshen@med.unc.edu</email>
          </address>
          <xref ref-type="aff" rid="Aff2">2</xref>
          <xref ref-type="aff" rid="Aff6">6</xref>
        </contrib>
        <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122483208</institution-id><institution-id institution-id-type="GRID">grid.10698.36</institution-id><institution>Department of Computer Science, </institution><institution>University of North Carolina at Chapel Hill, </institution></institution-wrap>Chapel Hill, NC 27514 USA </aff>
        <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122483208</institution-id><institution-id institution-id-type="GRID">grid.10698.36</institution-id><institution>Department of Radiology and BRIC, </institution><institution>University of North Carolina at Chapel Hill, </institution></institution-wrap>Chapel Hill, NC 27514 USA </aff>
        <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1757 8861</institution-id><institution-id institution-id-type="GRID">grid.411405.5</institution-id><institution>Department of Neurosurgery, </institution><institution>Huashan Hospital, Fudan University, </institution></institution-wrap>Shanghai, 200040 China </aff>
        <aff id="Aff4"><label>4</label>Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention, Shanghai, 200040 China </aff>
        <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0368 8293</institution-id><institution-id institution-id-type="GRID">grid.16821.3c</institution-id><institution>Med-X Research Institute, School of Biomedical Engineering, </institution><institution>Shanghai Jiao Tong University, </institution></institution-wrap>Shanghai, 200030 China </aff>
        <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0840 2678</institution-id><institution-id institution-id-type="GRID">grid.222754.4</institution-id><institution>Department of Brain and Cognitive Engineering, </institution><institution>Korea University, </institution></institution-wrap>Seoul, 02841 Republic of Korea </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>31</day>
        <month>1</month>
        <year>2019</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>31</day>
        <month>1</month>
        <year>2019</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2019</year>
      </pub-date>
      <volume>9</volume>
      <elocation-id>1103</elocation-id>
      <history>
        <date date-type="received">
          <day>19</day>
          <month>3</month>
          <year>2018</year>
        </date>
        <date date-type="accepted">
          <day>13</day>
          <month>11</month>
          <year>2018</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© The Author(s) 2019</copyright-statement>
        <license license-type="OpenAccess">
          <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract id="Abs1">
        <p id="Par1">High-grade gliomas are the most aggressive malignant brain tumors. Accurate pre-operative prognosis for this cohort can lead to better treatment planning. Conventional survival prediction based on clinical information is subjective and could be inaccurate. Recent radiomics studies have shown better prognosis by using carefully-engineered image features from magnetic resonance images (MRI). However, feature engineering is usually time consuming, laborious and subjective. Most importantly, the engineered features cannot effectively encode other predictive but implicit information provided by multi-modal neuroimages. We propose a two-stage learning-based method to predict the overall survival (OS) time of high-grade gliomas patient. At the first stage, we adopt deep learning, a recently dominant technique of artificial intelligence, to automatically extract implicit and high-level features from multi-modal, multi-channel preoperative MRI such that the features are competent of predicting survival time. Specifically, we utilize not only contrast-enhanced T1 MRI, but also diffusion tensor imaging (DTI) and resting-state functional MRI (rs-fMRI), for computing multiple metric maps (including various diffusivity metric maps derived from DTI, and also the frequency-specific brain fluctuation amplitude maps and local functional connectivity anisotropy-related metric maps derived from rs-fMRI) from 68 high-grade glioma patients with different survival time. We propose a multi-channel architecture of 3D convolutional neural networks (CNNs) for deep learning upon those metric maps, from which high-level predictive features are extracted for each individual patch of these maps. At the second stage, those deeply learned features along with the pivotal limited demographic and tumor-related features (such as age, tumor size and histological type) are fed into a support vector machine (SVM) to generate the final prediction result (i.e., long or short overall survival time). The experimental results demonstrate that this multi-model, multi-channel deep survival prediction framework achieves an accuracy of 90.66%, outperforming all the competing methods. This study indicates highly demanded effectiveness on prognosis of deep learning technique in neuro-oncological applications for better individualized treatment planning towards precision medicine.</p>
      </abstract>
      <funding-group>
        <award-group>
          <funding-source>
            <institution-wrap>
              <institution-id institution-id-type="FundRef">https://doi.org/10.13039/100000098</institution-id>
              <institution>U.S. Department of Health &amp;amp; Human Services | NIH | NIH Clinical Center (Clinical Center)</institution>
            </institution-wrap>
          </funding-source>
          <award-id>EB006733</award-id>
          <principal-award-recipient>
            <name>
              <surname>Shen</surname>
              <given-names>Dinggang</given-names>
            </name>
          </principal-award-recipient>
        </award-group>
      </funding-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>© The Author(s) 2019</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>Introduction</title>
      <p id="Par2">Brain tumors are one of the most lethal cancers. High-grade gliomas with World Health Organization (WHO) grades III and IV are the most deadly brain tumors with short overall survival (OS) time. Presurgical prognosis of the high-grade gliomas is highly desired in clinical practice for better treatment planning, but still challenging compared to low-grade gliomas (i.e., WHO grades I and II, for which generally long OS is expected). Presurgical OS prediction is traditionally believed to be affected by numerous factors, such as tumor location, histopathological types, patient’s age, physical status, patient performance status and neurological disability<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>. Although generally corresponding to the short OS<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, recent molecular pathological studies have shown that the higher grade glioma patients with the same tumor histopathology may have significantly different OS<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. These findings indicate that the traditional prognosis prediction based on the simple clinical and demographical information may not be adequately accurate<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. Instead, based on the abundant non-invasive multi-modal neuroimaging data acquired prior to any invasive examination or surgery, a more accurate prognosis model for high-grade gliomas could be established, which is of great clinical importance and could benefit both treatment planning and patient care.</p>
      <p id="Par3">Recently, promising progress has been made using presurgical brain imaging and the radiomics features extracted from these images to study glioma prognosis, or to investigate phenotype-genotype association<sup><xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup> for <italic>indirect</italic> prognostic studies. Among all presurgical neuroimaging modalities, T1-weighted magnetic resonance image (MRI) provides a 3D visualization of the brain structures with high soft-tissue contrast and high spatial resolution. Specifically, contrast-enhanced T1 MRI (where hyper-intensity suggests higher grade) has been widely used for imaging-based presurgical diagnosis and treatment planning. The rich appearance information depicted by this modality has also played an important role in prognostic studies<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. For example, Pope <italic>et al</italic>. extracted 15 features from contrast-enhanced T1 MRI and found that the existence of non-enhancing regions indicated good OS while enhancing regions was not a valuable predictor<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Jain <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> found that, using dynamic susceptibility contrast-enhanced T2*-weighted perfusion MR, increasing relative cerebral blood volume in non-enhancing regions could predict worsen OS. The same group further found that patients with high rCBV and wild-type EGFR mutation had poor overall survival<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. However, Gutman <italic>et al</italic>. suggested that the volume of the enhancing lesions strongly indicated poor survival<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. In a phenotype-genotype association study<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, more image features were found to be helpful for bifurcate survival curves, which included the volume of contrast-enhancing area again. In addition to the contrast-enhanced T1 MRI, diffusion tensor imaging (DTI) and functional MRI (fMRI) could also have prognostic values. DTI measures the anisotropic diffusivity of water molecules. It can be used to indicate edema and capture white matter microstructural alterations, which are helpful for OS evaluation. For instance, Saksena <italic>et al</italic>. found a potential relationship between various DTI metrics and survival time of glioblastoma patients<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Several survival studies have shown that DTI is statistically more effective to help separate glioblastoma patients into short and long survival groups than only using histopathologic information<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Although extensively used for presurgical functional mapping<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, fMRI has not been used for OS prediction yet. FMRI can measure brain function with blood oxygen level dependent (BOLD) signals and, similar to perfusion MRI, fMRI has also been used to characterize relative cerebral blood volume/flow<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Since highly malignant gliomas may have abnormal cerebrovascular reactivity<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, or altered regional blood flow due to neovascularization<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> and/or abnormal metabolism<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. This imaging modality can also be used to predict OS.</p>
      <p id="Par4">The aforementioned imaging-based prognostic studies usually use handcrafted features that are carefully designed and manually extracted by well-trained and experienced clinicians (or automatically extracted using image processing techniques). Although such feature extraction is straightforward, the simplicity of such features prevents the rich information embedded in the multi-modal neuroimages from being fully utilized for OS prediction, because the handcrafted features are extracted based on previous studies or prior knowledge of diseases, and can also be limited to the existing image processing techniques. These feature descriptors could be biased and subjective to human interference. On the contrary, rich imaging phenotype information, which is beyond simple changes in image contrast/intensity, is deeply embedded and could be of essential prognostic value. The functional alternations within/around the visible lesion in MRI should also be extracted to further improve prognostic accuracy. Recently, rich radiomics features have further extended our knowledge on the roles of neuroimages in OS predictability<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. However, the automatically extracted features in these studies are mostly based on the existing image processing algorithms or related with the lesions or their proximity<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. There could be more sophisticated, high-level features that have better OS predictive values. In addition, the simple methodology in previous studies may also prevent the multi-modal images being well-integrated for OS prediction. Most existing works conduct univariate (i.e., independently considering each feature) or multivariate (i.e., jointly considering all features in a linear regression framework) analysis for prognosis<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. The individual-level predictive capability (i.e., predicting OS for a single patient) of these models is limited by the group-level comparisons (e.g., identifying image features as biomarkers for either statistically partitioning patients into the long/short OS groups<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> or statistically better bifurcated of survival curves<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>). In clinical practice, however, survival time is expected to be individually predictable.</p>
      <p id="Par5">Similar to a radiologist who deliberates a prognostic suggestion after carefully review and comparison of all multi-modal images, we propose an objective and accurate computer-aided OS prediction framework for high-grade glioma patients. Our method is powered by popular and effective machine learning techniques, such that it is capable of extracting multi-modal and multi-channel neuroimaging features and effectively fusing them for individual OS prediction. We particularly use deep learning to extract features. With a convolutional neural network (CNN), a hierarchy of appearance features can be synthesized from low level to high level in a layer-by-layer manner<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. Upon the convolutional parameters of the CNN are trained, with which the input raw image patches (i.e., small segments of whole-brain image) can be mapped to fit the target estimates (long/short OS). The mapping yields a highly sophisticated feature representation for the neuroimages, which is the key advantage of CNN compared to other machine learning methods. The CNN has shown superior performance on numerous visual object recognition and image classification studies<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. It has also boosted the development of medical image analysis<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, including applications to tumor diagnosis<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. In this paper, we propose a novel learning based method to predict OS of high-grade glioma patients: (1) We first automatically learn the high-level features for patches from multi-modal images (i.e., using the popularly acquired presurgical images, including contrast-enhanced T1 MRI, DTI and resting-state fMRI (rs-fMRI)) by training a supervised deep learning model at patch level; (2) We then train a binary support vector machine (SVM) model<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> based on the automatically extracted semantic features (i.e., by concatenating patch-level features together to form patient-level features for each patient) to predict the OS for each patient. To well utilize neuroimage information, we calculate multiple diffusivity metric maps as multi-channel maps for DTI; also from rs-fMRI, we derive two types of multi-channel images using frequency information (freq-fMRI) of voxel-wise brain activity and inter-voxel local functional connectivity anisotropy (expressed as “functional tensor”, or fTensor-fMRI), respectively. The flowchart of our OS prediction framework is illustrated by Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Figure 1</label><caption><p>Schematic description of the proposed survival prediction framework for high-grade glioma patients, by using (1) 3D CNN-based deep learning to conduct feature learning and (2) an SVM for final prediction (long or short OS).</p></caption><graphic xlink:href="41598_2018_37387_Fig1_HTML" id="d29e520"/></fig></p>
      <p id="Par6">A preliminary version of this work has been presented at a conference<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Herein, we (i) extend our method by introducing an additional modality (fTensor-fMRI) to enhance the multi-modal multi-channel feature learning by providing more supplementary information, (ii) explore the impact of using the multi-modality information, (iii) investigate the impact of convolutional kernels: comparing 3D convolutional kernels with 2D convolutional kernels, (iv) compare the proposed supervised learned features with unsupervised extracted features on the classification task, and (vi) test on an extra 25-subject dataset.</p>
    </sec>
    <sec id="Sec2">
      <title>Experiments and Results</title>
      <sec id="Sec3">
        <title>Data Acquisition</title>
        <sec id="Sec4">
          <title>Subjects</title>
          <p id="Par7">In this study, we included 68 patients with high-grade gliomas screened by presurgical imaging from the glioma image database (collected during 2010–2015) of Huashan hospital, Shanghai, China. We call this dataset training dataset. We also included another independent dataset with 25 patients (see Validation on Independent Dataset), which is the validation dataset to further validate our model. The inclusion criteria are listed in the following: (1) patients who have primary intracranial tumor but having not received any treatment before multi-modal MRI scan; (2) patients who have all three key imaging modalities (i.e., T1 MRI, rs-fMRI and DTI) with the same imaging parameters (see Imaging Parameters); and (3) with the screening thick-slice contrast-enhanced T1 MRI clearly showing the enhancing lesions (indicating high-grade gliomas). The exclusion criteria are patients (1) with any surgery, radiotherapy, or chemotherapy of brain tumor prior to image acquisitions; (2) with excessive head motion or presence of artifacts in any image. To avoid subjectivity, the T1 images were separately visually evaluated by three raters, only consensus result were used for decision making; (3) with irrelevant death causes (e.g., suicide) during follow-ups which may confound OS estimation; and (4) with inadequate follow-up period to determine the label of long or short OS. Specifically, OS is defined by the duration from the date when the patient received operation (i.e., the starting date of treatment) to the date of death (if applicable). The threshold is chosen to be 650 days and the patients are thus divided into two groups: short OS group and long OS group. This threshold is defined according to the median OS for the adult high-grade glioma patients<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The patients who were alive according to the latest follow-up but already lived longer than 650 days are also labeled as “long OS”. Detailed patient information can be found in Table <xref rid="Tab1" ref-type="table">1</xref>. Informed written consents were acquired from all the participants before imaging. The imaging study was also approved by the local ethical committee at Huashan hospital. The whole study was carried out in accordance with the approved guidelines. The images from a sample subject are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, from which we can see the contrast-enhanced T1 MRI (presented in single channel), and the multi-channel metric maps derived from DTI and rs-fMRI. The detailed multi-channel metric map calculation will be described later.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Statistical information for the recruited patients.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Variables</th><th>Value</th><th>Variables</th><th>Value</th></tr></thead><tbody><tr><td colspan="2">Age (years)</td><td colspan="2">WHO, histological type (%)</td></tr><tr><td>   Mean ± std</td><td>51.4 ± 14.5</td><td>   III, anaplastic astrocytomas</td><td>17 (25)</td></tr><tr><td>   Range</td><td>16–74</td><td>   III, anaplastic oligodendrogliomas</td><td>9 (13.2)</td></tr><tr><td>Males/females (%)</td><td>48/68(70.6)</td><td>   III, anaplastic ependymomas</td><td>1 (1.5)</td></tr><tr><td colspan="2">Hemisphere (%)</td><td>   IV, glioblastoma</td><td>41 (60.3)</td></tr><tr><td>   Left</td><td>50 (73.5)</td><td colspan="2">Tumor size (<italic>mm</italic><sup>3</sup>)</td></tr><tr><td>   Right</td><td>16 (23.5)</td><td>   Mean ± std</td><td>53.9 ± 40.1</td></tr><tr><td>   Bilateral</td><td>2 (3)</td><td>   Range</td><td>1.7 200</td></tr><tr><td colspan="2">Main location (%)</td><td>Preoperative epilepsy (%)</td><td>27/68(39.7)</td></tr><tr><td>   Occipital</td><td>4 (5.9)</td><td colspan="2">Overall survival time(%)</td></tr><tr><td>   Temporal</td><td>20 (29.4)</td><td>   ≤650 days</td><td>29 (42.6)</td></tr><tr><td>   Parietal</td><td>7 (10.3)</td><td>   ≥650 days (dead)</td><td>27 (39.7)</td></tr><tr><td>   Frontal</td><td>31 (45.6)</td><td>   ≥650 days (live)</td><td>12 (17.6)</td></tr><tr><td>   Insula</td><td>6 (8.8)</td><td/><td/></tr></tbody></table></table-wrap><fig id="Fig2"><label>Figure 2</label><caption><p>A sample glioblastoma patient in our dataset. T1 MRI is presented as a single metric map, while DTI and rs-fMRI are presented as multi-channel metric maps.</p></caption><graphic xlink:href="41598_2018_37387_Fig2_HTML" id="d29e749"/></fig></p>
        </sec>
        <sec id="Sec5">
          <title>Imaging parameters</title>
          <p id="Par8">All the multi-modal images are acquired by a 3T MRI scanner (MAGNETOM Verio, Siemens Healthcare, Siemens AG, Germany) at Huashan hospital. The image data collected from each patient subject include T1 MRI (TR, 1900 ms; TE, 2.93 ms; flip angle, 9; FOV, 250 × 250 <italic>mm</italic><sup>2</sup>; matrix size, 256 × 215; slice thickness, 1 mm; acquisition average, 1), DTI (TR, 7600 ms; TE, 91 ms; slice thickness, 3 mm; inter-slice space, 0 mm; b-value, 1000 s/<italic>mm</italic><sup>2</sup>; NEX, 2; FOV, 230 × 230 <italic>mm</italic><sup>2</sup>; matrix size, 128 × 128; voxel size, 1.8 × 1.8 × 3 <italic>mm</italic><sup>3</sup>; number of gradient directions, 20), and rs-fMRI (TR, 2000 ms; TE, 35 ms; flip angle, 90; number of acquisitions, 240 (8 min); slice number, 33; slice thickness, 4 mm; inter-slice gap, 0 mm; FOV, 210 × 210 <italic>mm</italic><sup>2</sup>; matrix size, 64 × 64; voxel size, 3.4 × 3.4 × 4 <italic>mm</italic><sup>3</sup>).</p>
        </sec>
        <sec id="Sec6">
          <title>Treatment</title>
          <p id="Par9">All patients have been treated according to the clinical guideline for adult high-grade gliomas. All cases have achieved the maximal safe tumor resection by the same neurosurgeon (JW, with 20+ years’ experience) with intraoperative neurophysiological monitoring, which ensures the consistency of the surgical treatments across subjects. All tumors have been totally or gross-totally removed according to postsurgical imaging. In our study, the patients received concurrent high conformal radiation therapy and chemotherapy with Temozolomide followed by six cycles of Temozolomide according to Stupp’s regimen<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. For radiotherapy, each patient received fractionated focal irradiation in daily factions of 2 Gy given 5 days per week for 6 weeks, for a total of 60 Gy.</p>
          <p id="Par10">There is no significant difference between two OS groups in preoperative tumor volumes (<italic>p</italic> = 0.55) and extension of resection (<italic>p</italic> = 0.22). There is also no significant difference between two OS groups in the postoperative treatment (<italic>p</italic> = 0.82). Of note, this paper aims to develop a novel method that can preoperatively predict long/short survival outcome. We will show that, given the guideline and suggested following treatment, one can potentially predict the OS from the presurgical imaging data. Accordingly, the prediction of OS can be made for the presurgical planning, as the decision is made prior to the surgery. We by no means advocate that treatment is irrelevant to the final outcome.</p>
        </sec>
      </sec>
      <sec id="Sec7">
        <title>Data Preprocessing</title>
        <p id="Par11">We preprocess the images for convincing quantitative study. Of note, the three modalities of images are spatially co-registered (specifically, we register all other image modalities to T1-weighted image for each subject)<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, but not further registered to the Montreal Neurological Institute (MNI) standard space, to avoid artifacts caused by nonlinear deformation (see details below).</p>
        <sec id="Sec8">
          <title>T1 MRI</title>
          <p id="Par12">For each subject, the tumor as well as its close surrounding area is extracted by a rectangular bounding box. The bounding box is manually drawn, and further verified by a second radiologist to assure that all tumor lesions are included. Since both rs-fMRI and DTI images are co-registered to the T1 MRI of the same subject, the bounding boxes can be applied to other multi-channel metric maps of the same subject.</p>
        </sec>
        <sec id="Sec9">
          <title>DTI</title>
          <p id="Par13">We use PANDA<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> to process DTI data. The main procedures include brain extraction, eddy current correction, and diffusion tensor and diffusivity metric calculation. In particular, we compute 6 diffusivity metric maps: fractional anisotropy (FA), mean diffusivity (MD), the first/second/third eigenvalue of the tensor (<italic>λ</italic><sub>1</sub>, <italic>λ</italic><sub>2</sub>, <italic>λ</italic><sub>3</sub>), and radial diffusivity (RD) (with more details in this paper<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>). Together with B0 (b = 0 s/<italic>mm</italic><sup>2</sup>) map, the 7 metric maps constitute 7 channels, which are co-registered to T1 MRI per subject.</p>
        </sec>
        <sec id="Sec10">
          <title>rs-fMRI</title>
          <p id="Par14">Data preprocessing is performed by DPARSF<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, including removal of the first 5 volumes, slice timing, head motion correction, spatial smoothing, linear trend removal, and regressing out the nuisance covariates that consist of averaged white-matter and cerebrospinal-fluid signals. Frequency-specific BOLD fluctuation power maps are calculated in five non-overlapping frequency bands<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup> (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>), resulting in 5 metric maps (freq-fMRI). Note that these 5 maps mainly focus on grey matter. To also extract functional features in white matter, we propose to use the functional connectivity tensor (fTensor-fMRI)<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup> to provide functional information in white matter. The fTensor-fMRI was originally proposed in the work<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> to measure the structured spatiotemporal relationship among the BOLD signals of neighboring voxels in white matter, which shows the anisotropic pattern that is generally consistent with the diffusion anisotropy derived from DTI in major fiber bundles. The fTensor-fMRI has demonstrated to be able to detect functional changes in white matter, caused by task stimulation<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> and anesthesia<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Since gliomas grow in white matter, which could alter the fTensor-fMRI, we decide to adopt this metric (mainly focusing on the white matter) together with freq-fMRI (mainly focusing on the grey matter) to jointly predict OS. Like the metric maps derived from diffusion tensor for DTI, we also compute <italic>λ</italic><sub>1</sub>, <italic>λ</italic><sub>2</sub>, <italic>λ</italic><sub>3</sub>, FA, MD and RD maps based on fTensor for rs-fMRI, which results in 6 additional metric maps (thus totally 11 metric maps for rs-fMRI). Of note, it is the first time that fTensor-fMRI has been used in clinical applications.</p>
          <p id="Par15">For all channels of the metric maps from the three imaging modalities, the bounding box is applied. Then, the intensities inside the bounding box are normalized per metric map. The 3D metric maps within the bounding box are rescaled to the same dimension (64 × 64 × 64) to facilitate the subsequent deep learning.</p>
        </sec>
      </sec>
      <sec id="Sec11">
        <title>Experimental Settings</title>
        <p id="Par16">As described earlier, we use our 3D CNN architectures, which is implemented by the widely used deep learning framework Caffe<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, to extract features from multi-modal brain images and their multi-channel metric maps in a supervised manner. These features are expected to classify individual image patches according to the survival of the patient. Then, the high-level features of all patches of the patient, as well as the important limited demographic and tumor-related features, are integrated to train the SVM classifier for the survival time prediction of the patient. As mentioned in the Method section, the patch-based features are processed through Principal Component Analysis (PCA) for feature reduction and Sparse Representation (SR)<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> for feature selection. Specifically, the number of the features from fc6 for each metric map is 8 × 256 = 2048 (note that we extract 8 non-overlapping patches uniformly within the bounding box of each metric map for a subject), and the number for fc7 is 8 × 2 = 16. With PCA, we preserve the principal components up to 99% and reduce the feature numbers of fc6 for the T1 MRI, DTI, freq-fMRI and fTensor-fMRI to 9, 14, 12, 16, respectively, as well as the feature numbers of fc7 to 5, 7, 7, 8, respectively. With SR, we set the balance parameter to 0.2 with the SLEP package<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Usually, there are 3, 10, 8, 7 selected features from fc6 for the T1 MRI, DTI, freq-fMRI and fTensor-fMRI, respectively; as for fc7, the corresponding numbers are 3, 5, 4, 4, respectively. For the SVM<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, we chose the L1-regularized logistic regression and set the cost parameter to be 1.</p>
      </sec>
      <sec id="Sec12">
        <title>Experimental Results</title>
        <sec id="Sec13">
          <title>Cross-Validation Experiments</title>
          <p id="Par17">We use 10-fold and 3-fold cross-validation upon 68 patient subjects. That is, for each testing fold, the remaining other folds are used to train both the single-channel CNN (for T1 MRI) and mCNN (for DTI and fMRI), as well as the SVM. The performance measures averaged over all the folds are reported in Table <xref rid="Tab2" ref-type="table">2</xref>, including accuracy (ACC), sensitivity (SEN), specificity (SPE), positive predictive rate (PPR), and negative predictive rate (NPR).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance evaluation of different features and selection/reduction methods with 3-fold and 10-fold cross validation, respectively. *Significantly improved (<italic>p</italic> &lt; 0.05) performance compared with “dtf” only method, as measured by McNemar’s test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>ACC (%)</th><th>SEN (%)</th><th>SPE (%)</th><th>PPR (%)</th><th>NPR (%)</th><th>Cross Validation</th></tr></thead><tbody><tr><td>dtf</td><td>62.96</td><td>66.39</td><td>58.53</td><td>63.18</td><td>65.28</td><td rowspan="7">3-fold</td></tr><tr><td>fc7*</td><td>81.04</td><td>85.87</td><td>77.94</td><td>73.28</td><td>89.42</td></tr><tr><td>fc6-PCA*</td><td>80.78</td><td>84.88</td><td>77.04</td><td>76.03</td><td>85.37</td></tr><tr><td>fc6-SR</td><td>77.11</td><td>85.39</td><td>71.01</td><td>67.25</td><td>86.53</td></tr><tr><td>dtf + fc7*</td><td>
<bold>90.66</bold>
</td><td>96.77</td><td>
<bold>85.04</bold>
</td><td>
<bold>85.82</bold>
</td><td>96.31</td></tr><tr><td>dtf + fc6-PCA*</td><td>
<bold>90.27</bold>
</td><td>
<bold>96.48</bold>
</td><td>84.76</td><td>
<bold>84.98</bold>
</td><td>94.05</td></tr><tr><td>dtf + fc6-SR*</td><td>86.68</td><td>93.51</td><td>82.39</td><td>78.64</td><td>
<bold>95.43</bold>
</td></tr><tr><td>dtf</td><td>63.23</td><td>67.24</td><td>60.25</td><td>60.71</td><td>66.21</td><td rowspan="7">10-fold</td></tr><tr><td>fc7*</td><td>82.35</td><td>86.20</td><td>79.48</td><td>75.76</td><td>88.58</td></tr><tr><td>fc6-PCA*</td><td>80.88</td><td>85.27</td><td>76.92</td><td>74.52</td><td>86.26</td></tr><tr><td>fc6-SR</td><td>77.94</td><td>85.60</td><td>71.80</td><td>69.35</td><td>87.04</td></tr><tr><td>dtf + fc7*</td><td>
<bold>90.46</bold>
</td><td>
<bold>95.82</bold>
</td><td>85.78</td><td>
<bold>84.81</bold>
</td><td>95.59</td></tr><tr><td>dtf + fc6-PCA*</td><td>
<bold>90.17</bold>
</td><td>94.83</td><td>
<bold>85.90</bold>
</td><td>84.33</td><td>
<bold>95.71</bold>
</td></tr><tr><td>dtf + fc6-SR*</td><td>86.76</td><td>93.10</td><td>82.05</td><td>79.41</td><td>94.12</td></tr></tbody></table></table-wrap></p>
          <p id="Par18">Incorporating both deeply learned (with the proposed CNNs) and limited demographic &amp; tumor-related features (dtf) leads to the best classification accuracy of 90.46%. In contrast, using dtf alone, we obtain just an accuracy of 63.23%. Regarding the sensitivity and the specificity, we know that the higher the sensitivity, the lower the chance of misclassifying the short survival patients; on the other hand, the higher the specificity, the lower the chance of misclassifying the long survival patients. The proposed feature extraction method resulted in an approximately 30% higher sensitivity and specificity, compared to the limited demographic and tumor-related features. Interestingly, our model predicts the short survival patients with more confidence than the long survival patients.</p>
          <p id="Par19">To further investigate the effectiveness of our proposed method, we also draw a Kaplan-Meier plot based on the model output (i.e., the hard classification labels) in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. We can see that the survival curves of the two groups are well separated (<italic>p</italic> &lt; 0.0001, log-rank test). The result indicates that our model can well separate subjects with long OS from those with short OS. It should be noted that the K-M plot we plotted is not based on the two values of a single explicit variable nor those of certain combined explicit variables, but the weighted combination of the deep learning features and the subsequent “hard” classification result. Therefore, our K-M plot should be interpreted carefully.<fig id="Fig3"><label>Figure 3</label><caption><p>Kaplan-Meier plot of the two groups’ survival data based on our predicted results. Dotted line indicates the 95% confidence interval.</p></caption><graphic xlink:href="41598_2018_37387_Fig3_HTML" id="d29e1315"/></fig></p>
          <p id="Par20">Besides the deep learned features and the limited demographic &amp; tumor-related features, we also extract the radiomics features using traditional unsupervised feature learning methods, such as Haar features and SIFT features. We have reported the details in Section “Comparison with Unsupervised Feature Extraction Approaches”. In addition, we also compare CNN alone, CNN + SVM (our method) and SVM alone (with the above radiomics features), and you can refer to “Impact of Combining CNN and SVM” for details.</p>
        </sec>
        <sec id="Sec14">
          <title>Validation on Independent Dataset</title>
          <p id="Par21">To further validate the effectiveness of our proposed algorithm on predicting OS for gliomas patients, we test our trained model on a newly collected dataset. This newly collected dataset consists of 25 patients; each of them has the same modalities (channels) with the dataset described in Section Data Acquisition. The statistical information about the 25 patients are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. We preprocessed the data by following the same procedures as described in Section Data Preprocessing.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Statistical information for the newly recruited 25 patients (validation dataset).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Variables</th><th>Value</th><th>Variables</th><th>Value</th></tr></thead><tbody><tr><td colspan="2">Age (years)</td><td colspan="2">WHO, histological type (%)</td></tr><tr><td>   Mean ± std</td><td>50.2 ± 13.0</td><td>   III, anaplastic astrocytomas</td><td>6 (24)</td></tr><tr><td>   Range</td><td>23–68</td><td>   III, anaplastic oligodendrogliomas</td><td>3 (12)</td></tr><tr><td>Males/females (%)</td><td>16/9</td><td>   III, anaplastic ependymomas</td><td>1 (4)</td></tr><tr><td colspan="2">Hemisphere (%)</td><td>   IV, glioblastoma</td><td>15 (60)</td></tr><tr><td>   Left</td><td>16 (64)</td><td colspan="2">Tumor size (<italic>mm</italic><sup>3</sup>)</td></tr><tr><td>   Right</td><td>8 (32)</td><td>   Mean ± std</td><td>35.8 ± 32.3</td></tr><tr><td>   Bilateral</td><td>1 (4)</td><td>   Range</td><td>8.4 99.3</td></tr><tr><td colspan="2">Main location (%)</td><td>   Preoperative epilepsy(%)</td><td>27(39.7)</td></tr><tr><td>   Occipital</td><td>1 (4)</td><td colspan="2">Overall survival time(%)</td></tr><tr><td>   Temporal</td><td>6 (24)</td><td>   ≤ 650 days</td><td>9 (36)</td></tr><tr><td>   Parietal</td><td>2 (8)</td><td>   ≥ 650 days (dead)</td><td>3 (12)</td></tr><tr><td>   Frontal</td><td>14 (56)</td><td>   ≥ 650 days (live)</td><td>13 (52)</td></tr><tr><td>   Insula</td><td>2 (8)</td><td/><td/></tr></tbody></table></table-wrap></p>
          <p id="Par22">With preprocessed data, we adopt the trained neural networks to extract feature representation for all these 25 patients. Then, we apply the trained SVM model for classification based on the extracted features (Note that we only consider limited demographic and tumor-related features and fc7 features as the final features for the SVM model). The experimental results on such an independent dataset are presented in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance comparison across different features for the newly collected 25 patients. *Significantly improved (<italic>p</italic> &lt; 0.05) performance compared with “dtf” only method, as measured by McNemar’s test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>ACC (%)</th><th>SEN (%)</th><th>SPE (%)</th><th>PPR (%)</th><th>NPR (%)</th></tr></thead><tbody><tr><td>dtf</td><td>68</td><td>66.67</td><td>68.75</td><td>54.55</td><td>78.57</td></tr><tr><td>fc7*</td><td>80</td><td>77.78</td><td>81.25</td><td>75</td><td>86.67</td></tr><tr><td>dtf + fc7*</td><td>
<bold>88</bold>
</td><td>
<bold>88.9</bold>
</td><td>
<bold>87.5</bold>
</td><td>
<bold>80</bold>
</td><td>
<bold>93.3</bold>
</td></tr></tbody></table></table-wrap></p>
          <p id="Par23">The performances reported in Table <xref rid="Tab4" ref-type="table">4</xref> are generally consistent with those in Table <xref rid="Tab2" ref-type="table">2</xref>, especially for the accuracy, sensitivity and specificity. This further proves the robustness of the proposed method.</p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec15" sec-type="discussion">
      <title>Discussion</title>
      <sec id="Sec16">
        <title>General Discussion</title>
        <p id="Par24">Accurate pre-operative prognosis for this high-grade glioma can lead to better treatment planning. Conventional survival prediction based on clinical information is prone to be subjective and sometimes could be not accurate enough. In this paper, we propose a multi-modality multi-channel deep learning method to automatically learn feature representations for the imaging data and then a binary SVM model for the final tumor OS classification. Due to the use of powerful deep learning model, we can learn useful features from imaging data automatically. It can thus avoid being subjective if self-designing the features by radiologists, and will be able to explore some useful but hard-to-design features. Furthermore, our proposed deep feature learning method can be adapted to both single-channel and multi-channel imaging data. This is a huge advantage in clinical application as it is common that medical imaging data has uncertain number of channels. However, we by no means aim to underrate and criticize the traditional OS prediction model, but to test the feasibility of deep-learning-based OS prediction model as this type of methods has many advantages such as automatic feature learning, high-level feature learning, better ability to fuse multi-channel images, and so on.</p>
        <p id="Par25">Other studies also used conventional features, or radiomics features, but may include more features, such as KPS (daily living score), resection percentage, and some genomics features (e.g., IDH1 or MGMT). But obtaining these features will involve enormous work and resources. We would like to provide the reasons why we did not include them as below. First, KPS score is based on patients’ self-report, which could be less objective. Moreover, all our patients have KPS scores larger than 90, making this factor count for very little variability of the survival data. Second, resection percentage is a treatment-related factor, which is beyond the scope of this study (i.e., to predict survival time based on presurgical data). Moreover, as described in Sec. “Treatment”, all tumors have been totally or gross-totally removed, indicating that the optimized treatment has been achieved for all the patients. Therefore, our goal can be simply summarized as: to predict OS based on deep learning upon tumor multimodal imaging obtained presurgically, given optimized treatment following the guideline later on. Third, the genomic data is not available for most of the subjects, given the commencement of the study is early (i.e., since year 2010). Collectively, to achieve our preset research goal, and to simply convey our proposed method, i.e., to demonstrate the feasibility of DL-based feature selection for prognosis, we would rather prefer the current comparison strategy.</p>
        <p id="Par26">More importantly, our proposed framework is able to work well on a small dataset. In our study, the minimum unit or a sample is a patch in the feature learning stage, rather than a whole brain image. That is, for each subject, we can extract hundreds of patches (with the same label); therefore, we can eventually have enough samples (i.e., patches) to train the neural networks. In other words, we train the networks at the patch level, rather than a whole-image (or subject) level. After training the neural networks, we then train a SVM with the learned “deep features” (as they have been learned from deep convolutional neural networks) to classify the patients with short or long overall survival (OS) time. Because the features learned from the deep learning framework are more accurate and also at much higher level, the following SVM could have better performance than the SVMs using features extracted by traditional methods.</p>
        <p id="Par27">Moreover, our proposed framework can fuse multi-modality information so that it can fuse more information from different imaging modalities to determine the final classification. The results from additional experiments on this issue are detailed in the Experiments and Results section.</p>
      </sec>
      <sec id="Sec17">
        <title>Contribution of Multi-Modality Information to OS Prediction</title>
        <p id="Par28">We run the same proposed framework for extracting features from each single modality, and train SVM using the extracted features. In this way, we can justify the importance of fusing multi-modal imaging data in predicting OS. The quantitative results are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Among the single modality classification performances, the features from rs-fMRI yield the best performance among all single modalities, i.e., about 82.63% of accuracy (with significant improvement compared with the “dtf” only method, <italic>p</italic> &lt; 0.05, McNemar’s test). However, as it is obvious from the results, we can benefit about 7% improvement of accuracy by fusing multi-modal images, when using our proposed framework.<fig id="Fig4"><label>Figure 4</label><caption><p>OS prediction results using different imaging modalities. “Proposed” denotes the prediction result by combining all modalities together.</p></caption><graphic xlink:href="41598_2018_37387_Fig4_HTML" id="d29e1673"/></fig></p>
        <p id="Par29">We also conduct experiments by comparing the outcome prediction performance using 3D CNN in our model against the case of using traditional 2D CNN. The result shows significant advantage of using 3D CNN (with the details given in Section Comparison with 2D CNN based Approaches). Another experiment shows the comparison result between supervised feature extraction method (in our proposed method) and traditional unsupervised feature extraction approaches (i.e., adopted in Radiomics studies), indicating the superiority of supervised feature extraction to unsupervised feature extraction (see details in the Section Comparison with Unsupervised Feature Extraction Approaches).</p>
      </sec>
      <sec id="Sec18">
        <title>The Role of the Features from Each Modality</title>
        <p id="Par30">To analyze the importance of the features for predicting OS, we also calculate the number of the features selected from each modality in the prediction based on multi-modal images. To do this, we use the L1-regularized SVM for classification, for internally enforcing the selection of the most discriminative features from the outputs of the fc7 layers of the CNNs. The average numbers of the discriminative features selected for T1 MRI, DTI, freq-fMRI and fTensor-fMRI are 1, 4, 4, and 4, respectively. As can be seen, the fMRI and DTI data contribute more significantly in building a better prediction model, compared to the T1 MRI. However, for the T1 MRI, we only have a single channel of data, while multiple channels for the other modalities. Therefore, we further normalize these numbers by the total number of the channels from the corresponding modality. The normalized measures are 1, 0.57, 0.8 and 0.67, respectively, for the four (sub-)modalities. In this sense, the results then show that T1 MRI encodes relatively more information for OS prediction, compared to DTI and fMRI.</p>
      </sec>
      <sec id="Sec19">
        <title>Comparison with 2D CNN based Approaches</title>
        <p id="Par31">To illustrate the superiority of using the proposed 3D CNN architectures, we also compare our proposed method with the CNNs that use the 2D filters. Specifically, we adopted a 2D version of the CNN architecture shown in page 13, in which the inputs are 2D patches along the axial plane, and the feature maps are all reduced to 2D. Moreover, we employ the same strategy shown in page 14 to train multi-channel deep networks, with 2D features. We use slices from the tumor region (64 × 64 × 64) as input for the 2D CNNs (Note, we can also use 32 × 32 as the patch size to train a 2D CNN model; however, such a small patch size could not catch much information and it resulted in unfavorable results). As the dataset is small (64 × 68 = 4352), we perform several fixed rotations of<sup>90,180,270</sup> degrees for each extracted 2D patch to augment the dataset (4352 × 4 = 17408).</p>
        <p id="Par32">The experimental results are presented in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The 2D CNN-based approach presents a decent performance such as about 81% of accuracy, 89% of sensitivity, and 74% of specificity. In contrast, the proposed 3D CNN can advance the performance by approximately 10%. These results illustrate that the proposed 3D CNN features are more effective and powerful than the 2D-CNN based features.<fig id="Fig5"><label>Figure 5</label><caption><p>The experimental results using different feature extraction methods (such as 2D CNN and the proposed 3D CNN).</p></caption><graphic xlink:href="41598_2018_37387_Fig5_HTML" id="d29e1702"/></fig></p>
      </sec>
      <sec id="Sec20">
        <title>Comparison with Unsupervised Feature Extraction Approaches</title>
        <p id="Par33">To show the advantage of our 3D-CNN-based supervised feature learning, we also perform comparisons with several unsupervised feature extraction techniques, which are popularly used in both computer vision and medical imaging fields.</p>
        <p id="Par34">Specifically, we adopt scale-invariant transform (SIFT)<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, a commonly used unsupervised image descriptor in image reconstruction, alignment and recognition tasks, as a comparison feature extraction approach. As our medical image is stored in 3D format, we employ a spatial-temporal descriptor based on 3D gradients<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> to extract the features from the tumor regions. We then cluster the vector-represented patches to “codewords” using k-means, which produces visual vocabularies for the features. Each patch in an image is represented by a certain visual vocabulary, and finally the image can be represented by a histogram of the visual vocabularies. This is the same procedure used in many recognition methods in computer vision, denoted as “bag of words” method<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p>
        <p id="Par35">We also extract the Haar-like features from tumor patches, which are originally proposed by the paper<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> for object detection and have been applied to many applications due to its efficiency. Note that we use a variant of the Haar-like features<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> calculated based on the difference between the mean values of two cubic-regions randomly located within an image patch. The size of each cubic-region is randomly chosen from an arbitrary range, i.e., 1, 3, 5 in voxels<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>.</p>
        <p id="Par36">Since we have multiple modalities of data to extract the features, we first extract features from each modality separately and then use PCA to reduce their dimensionality. Next, we concatenate the features from different modalities and the handcrafted features, and finally train an SVM model. The experimental results are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The Haar-like features present the worst performance, and the proposed deep-learning-based features result in the best performance. Specifically, our supervised feature extraction framework can improve the performance by approximately 12%.<fig id="Fig6"><label>Figure 6</label><caption><p>The experimental results using different kinds of (unsupervised vs. supervised) features.</p></caption><graphic xlink:href="41598_2018_37387_Fig6_HTML" id="d29e1749"/></fig></p>
      </sec>
      <sec id="Sec21">
        <title>Impact of Combining CNN and SVM</title>
        <p id="Par37">As reported in the Method Section, our proposed method combines CNN and SVM together, since we assume that CNN can well learned semantic features in a patch-level, and SVM can well handle a small samples-size classification problem in subject-level. To investigate the impact of combining these two models, we design comparison experiments with SVM alone method and CNN alone method.</p>
        <p id="Par38">With SVM alone, we used manual designed features, i.e., Haar and SIFT features, as reported in the Section “Comparison with Unsupervised Feature Extraction Approaches”. As for CNN-based classification, as CNN has an ability to combine feature learning and classification together, which directly generates the soft label as the final output from the neuronal network. Therefore, one can directly use CNN’s output (by treating the largest soft label result as the final classification result) as the OS prediction result. The rationale that we design our model (CNN-based feature extraction plus SVM-based classification) is that SVM generally performs better and more robustly in a study with limited sample size. Therefore, we designed CNN + SVM by extracting features from the fully connected layers as the inputs of SVM for classification.</p>
        <p id="Par39">To make the comparison results easier to understand, we concluded our results from using CNN alone, using SVM alone (with SIFT features) and using CNN + SVM in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. CNN alone performs better than SVM alone, as CNN can learn better feature representations. Since the CNN + SVM conduct the classification on the subject-level feature representations which can well aggregate the patch-level features, it can improve the performance about 2.5% compared to CNN alone.<fig id="Fig7"><label>Figure 7</label><caption><p>Comparison of experimental results with SVM, CNN and CNN + SVM.</p></caption><graphic xlink:href="41598_2018_37387_Fig7_HTML" id="d29e1770"/></fig></p>
      </sec>
      <sec id="Sec22">
        <title>Model Transparency and Robustness</title>
        <p id="Par40">Deep learning based models hierarchically process the input data (imaging data in our case), and output the highly semantic features towards the target (i.e., tumor OS prediction in our case). As in our study, we use the last two layers (i.e., fc6 and fc7) as the extracted features. These features will be highly semantic and quite effective for tumor OS classification as they are learned under supervision. However, it is currently difficult to investigate which imaging features really help improve the accuracy and what they really represent. It is also difficult for our study, although we have tried to investigate which region of the imaging data contributes most to the useful features.</p>
        <p id="Par41">However, the lack of transparency doesn’t affect the robustness of our model. As reported in Section Experiments and Results, we first validate our proposed method on the dataset with 68 patients in a 3-fold cross-validation fashion. Then we further validate it by introducing extra testing on a new dataset with 25 patients. The experimental results on these two datasets indicate that our proposed method is robust. Furthermore, a lot of similar researches based on deep learning models are recently proposed and achieve great success. For example, Setio <italic>et al</italic>. proposed a multi-stream CNN to categorize the points of interest in chest CT as a nodule or non-nodule<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Esteva <italic>et al</italic>. proposed a deep neural network to implement dermatologist-level classification of skin cancer<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. And part of these studies has even been applied to clinical trials. Thus, we believe our proposed method is useful in developing a new tumor OS prediction model.</p>
      </sec>
      <sec id="Sec23">
        <title>Limitation and Future Works</title>
        <p id="Par42">It is worth indicating the limitations of our work. For example, we only use limited clinical information in our study and thus obtain a weak clinical model. We have also thought of using other features such as genetic indicators for OS prediction<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, including IDH1, MGMT, EGFR, 1p19q. The newly revised WHO grading system even suggests using some of the genetic features to grade the gliomas. Unfortunately, we did not have such information for all the subjects because our data were collected several years ago (at that time, collecting genetic information has not become the clinical routine yet). On the other hand, in our recent paper<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, we have used our newly-enrolled subjects’ neuroimaging data to predict their genotype information (IDH1 and MGMT), indicating the existence of relationship between imaging phenotypes and genotypes. But, for these newly-enrolled subjects, since they were newly admitted to the hospital and have been only followed up for a short time, we have not had their OS information yet. In the future follow-up study, as more subjects with both genetic information and OS data, we will include genetic information for OS prediction.</p>
        <p id="Par43">As for other important predictive features related to treatment, such as the extent of resection and the type/dose/duration of the adjuvant therapy, we did check this information before carrying on this study. However, as mentioned in the Introduction section, the motivation of this study is to answer a question: “Can we predict the patients’ OS based on their presurgical neuroimages, given similar treatments”. Therefore, in the experimental design, we have deliberately enrolled subjects with total (or gross total) resection; for most of them, they have been conducted with postsurgical adjuvant radiotherapy and chemotherapy with the same protocol suggested by the guideline. With these specifically selected subjects, we can then reduce the confounding effect of treatment and focus more on the prognostic value of neuroimaging. In post hoc analysis, we found that there is no group difference in extension of resection (<italic>p</italic> = 0.22) and postoperative treatments (<italic>p</italic> = 0.82) between short and long OS groups. Of note, we acknowledge that treatment is very important to OS, and we are carrying on an ongoing study to predict OS based on both presurgical and treatment features, as well as genetic features, so that future treatment can be better tailored for each individual.</p>
        <p id="Par44">As discussed earlier, our experiments are conducted on a dataset with 68 subjects and a new dataset with 25 patients. The number of subjects is relatively small. Therefore, to obtain better generalizability of the proposed method, we need to increase the participating subjects in the future. Also, we simply concatenate features (fc6 or fc7) extracted from different modalities together and utilize them for subsequent OS prediction without considering the relationship between different modalities. We should better take it into consideration in the future work. Moreover, we have resized the tumor cuboids to make them consistent in size; however, this operation obviously affects parts of the geometric properties of the tumor. This issue can be possibly resolved by applying a multi-instance learning framework. Furthermore, our current model considers tumor patients of WHO III and IV together, and we can potentially build separate models for WHO III and WHO IV patients to make more detailed predictions. Lastly, we choose a hard threshold to classify the patients into two categories (long or short OS), which decreases the precision of our predictive results. Besides, we can further categorize the patients into more (e.g., 4 or 5) subgroups for making more precise predictions.</p>
        <p id="Par45">In our future work, we will use all currently available features, including features from presurgical imaging, treatment ways, patient statuses before and after surgery, genetic information and molecular indicators (including IDH1, MGMT, 1p19q, TERT, and ATRX), to perform OS prediction. Since these features are from different domains, more advanced feature learning and integration methods need to be developed. Moreover, since our ultimate goal is to predict the overall survival which can be better used in clinical practice, we will treat it as continuous variable with a rigorous machine-learning or deep-learning-based regression model in our future work.</p>
        <p id="Par46">Generally, in this study, we have proposed a 3D deep learning model to predict the (long or short) OS time for the patients with brain glioma. We trained 3D CNN and mCNN models for learning features from single-channel (T1 MRI) and multi-channel (DTI, freq-fMRI and fTensor-fMRI) data in a supervised manner, respectively. The extracted features were then fed into a binary SVM classifier. The performance of our supervised CNN-based learned features was compared with the performances of several other state-of-the-art methods, including those using the traditional handcrafted features. Experimental results showed that our supervised-learned features significantly improved the predictive accuracy of OS time for the glioma patients. This also indicates that our proposed 3D deep learning frameworks can provoke computational models to extract useful features for such neuro-oncological applications. Besides, the analysis on the selected features further shows that DTI data can contribute slightly more than fMRI, but both fMRI and DTI play more significant role, compared to the T1 MRI, in building successful prediction models. Overall, our proposed method shows its great promise in multi-modal MRI-based diagnosis or prognosis for a wider spectrum of neurological and psychiatric diseases.</p>
      </sec>
    </sec>
    <sec id="Sec24">
      <title>Method</title>
      <p id="Par47">In this paper, we first employ the CNN architecture to train survival prediction models with the patches from all metric maps of T1 MRI, DTI and rs-fMRI, respectively. With such trained deep learning models, we can extract features for individual patches of the respective channels/modalities in a supervised manner. Then, a binary classifier (i.e., SVM) is trained to fuse all the patches and their extracted high-level features for OS prediction (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). In the following subsections, we will introduce both our CNN-based feature extraction and the SVM-based OS prediction strategies. Note that the CNN-based feature extraction for T1 MRI is slightly different from that for DTI and fMRI. That is, there is only a single inputting channel for T1 MRI, while there are multiple inputting channels for multiple metrics computed from DTI and fMRI. Thus, we will first introduce the 3D CNN architecture for single-channel T1 MRI, and then extend it for multi-channel DTI and fMRI. Different from the conventional CNN that stacks multi-channel inputs at the beginning, we perform independent convolution streams for each inputting channel in the early layers and then fuse them in deep layers for high-level feature extraction. Of note, to augment the dataset, we flip the bounding box along three directions (x, y, z) separately for all metrics. Next, we extract numerous partially-overlapping patches with the size of 32 × 32 × 32 to train the CNNs.</p>
      <sec id="Sec25">
        <title>Single-Channel Feature Extraction</title>
        <p id="Par48">For 3D T1 MRI, we propose a 3D CNN model with a set of 3D trainable filters. CNN derives the high-level features from the low-level input, while the estimated high-level features directly contribute to the classification of the input data. The network architecture usually consists of a number of layers. As we go deeper in the network, the layer will generate higher-level features. For example, the last layer can represent more intrinsic features compared to the earlier layer(s)<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>.</p>
        <p id="Par49">Inspired by the very deep convolutional networks (VGGNet)<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, we design our CNN architecture with four convolutional layer groups and three fully-connected layers. The detailed configurations of the four convolutional layer groups (conv1 to conv4) are shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The input to the CNN is a 3D patch with the size of 32 × 32 × 32, which is extracted in the bounded neighborhood of the tumor. The convolutional layers compute their outputs from the input 3D patch, by applying the convolutional operations with 3D filters of the size 3 × 3 × 3. The convolutional operation results in the 3D output patch of the same size as the input, followed by max-pooling to down-sample the patch. The last three layers in the CNN are fully connected (fc5 to fc7). These fully-connected layers include the neurons that are connected to all outputs of their precedent layers, as in the conventional neural networks. The last layer (fc7) has 2 neurons, whose correspond to the probabilities of classifying the patient into the long or the short OS group.<fig id="Fig8"><label>Figure 8</label><caption><p>An illustration of the CNN architecture for the single-channel feature extraction from 3D patches. There are four convolutional layer groups and three fully-connected layers. The network’s input is a 32 × 32 × 32 patch from a tumor region of a patient, and the output is the decision that this patient belongs to the long or short survival group.</p></caption><graphic xlink:href="41598_2018_37387_Fig8_HTML" id="d29e1853"/></fig></p>
        <p id="Par50">The supervision on the classification of the training data leads to a back-propagation procedure for learning the most relevant features in the CNN. Specifically, we regard the outputs from the last two layers of the CNN (fc6 and fc7) as the learned high-level appearance features of individual input patch. The efficiency and effectiveness of the extracted features will be verified in the experiments.</p>
        <p id="Par51">There are four convolutional layer groups and three fully-connected layers. The network’s input is a 32 × 32 × 32 patch from a tumor region of a patient, and the output is the decision that this patient belongs to the long or short survival group.</p>
      </sec>
      <sec id="Sec26">
        <title>Multi-Channel Feature Extraction</title>
        <p id="Par52">We compute multiple metric maps for DTI and rs-fMRI. Each metric map corresponds to an input channel when learning the high-level appearance features. To effectively employ all multi-channel data for providing complementary information for the brain tumor, we propose a new multi-channel-CNN (mCNN) architecture to train one mCNN for each modality. Inspired by the multi-modal deep Boltzmann machine<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>, we extend our single-channel 3D CNN architecture to deal with multi-channel data. Specifically, in the proposed mCNN, the same convolutional layer groups are applied to each channel separately. Then, a fusion layer is added to integrate the outputs of the last convolutional layer group (conv4) from all channels by concatenating them. Then, three fully-connected layers are further incorporated to finally extract the features. The mCNN architecture is illustrated in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. Note that the major difference between mCNN and single-channel CNN is the fusion layer. Other layers, including the convolutional layers and the fully-connected layers, follow the same configuration.<fig id="Fig9"><label>Figure 9</label><caption><p>Architecture of mCNN for feature extraction from multi-channel data.</p></caption><graphic xlink:href="41598_2018_37387_Fig9_HTML" id="d29e1878"/></fig></p>
        <p id="Par53">It is important to note that the statistical properties of different channels of the input data can vary largely, which makes it difficult for a single-channel model to directly encode multi-channel data (i.e., by simply concatenating multi-channel data and then applying the single-channel model). In contrast, our proposed mCNN model sustains much better capability of modeling multi-channel input data and fusing them together to generate high-level features.</p>
      </sec>
      <sec id="Sec27">
        <title>SVM-Based Survival Prediction</title>
        <p id="Par54">Once we complete training a CNN (Fig. <xref rid="Fig8" ref-type="fig">8</xref>) for T1 MRI and two mCNN (Fig. <xref rid="Fig9" ref-type="fig">9</xref> for DTI and fMRI, respectively, we can predict the short/long survival given 3D patches extracted in each of the four “modalities” (note that we derive two sub-modalities from rs-fMRI, i.e., freq-fMRI and fTensor-fMRI). That is, the patch(es), from single or multiple channels of the metrics, will go through the convolutional networks. Then the CNNs convert the input patch(es) to the high-level features and obtain the survival estimation in the final layer. In particular, the high-level features extracted at the last two layers (fc6 and fc7) of our CNN architectures are perceived to be suitable image-level descriptors<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. In this way, each patch can associate its high-level features with the survival time of the patient under consideration. Note that the fc6 layer has 256 neurons, while the last (fc7) layer comprises of two neurons.</p>
        <p id="Par55">In addition to these high-level appearance features, the limited demographic and tumor-related features (dtf) are also included in our experiments. These limited demographic and tumor-related features consist of generic brain tumor features, including gender, age at diagnosis, tumor location, size of tumor, and the WHO grade. Tumor location is defined by two metrics, i.e., (1) major location (such as the brain lobe where the tumor is mainly located), and (2) tumor distribution (i.e., a three-grade scale, ranging from 1 to 3, denoting the number of different brain lobes (such as 5 different lobes used in this study, including occipital, temporal, parietal, frontal and insula lobe) with tumor). For example, the tumor distribution of 1 denotes that the tumor appears only in one brain lobe. The assessment was conducted by three authors (HZ, JW and JL) with consensus. The size of the tumor was calculated based on T1 contrast-enhanced MRI by manually delineating the volume with abnormal intensity (i.e., the volume of a tumor as shown in the presurgical imaging). This was conducted by one neurosurgeon with 8-year experience (JL) to ensure the consistent tumor delineation criteria. There is no significant difference in the tumor volume (<italic>p</italic> = 0.55) between the two OS groups. Patient performance status was evaluated by using Karnofsky Performance Status (KPS) scores; however, the two OS groups have no group difference in KPS score (<italic>p</italic> &gt; 0.99). For the treatment variables, between the two OS groups, there is no significant difference in resection extension (<italic>p</italic> = 0.22), and the ways of post-surgical treatments received (<italic>p</italic> = 0.82). Although not included as the limited demographic and tumor-related features, we think that these factors will not likely to make significant contribution to the individualized OS prediction.</p>
        <p id="Par56">Since the numbers of the features in the fc6 and fc7 layers are huge, we further conduct feature reduction or selection for the features from each modality separately. Specifically, we use Principal Component Analysis (PCA) and Sparse Representation (SR)<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> (see Experimental Settings). Finally, we adopt SVM<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> to predict the patient’s survival time at an individual level based on the selected features.</p>
      </sec>
    </sec>
    <sec sec-type="supplementary-material">
      <title>Supplementary information</title>
      <sec id="Sec28">
        <p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2018_37387_MOESM1_ESM.zip"><caption><p>LaTeX Supplementary File</p></caption></media></supplementary-material>
</p>
      </sec>
    </sec>
  </body>
  <back>
    <fn-group>
      <fn>
        <p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
      </fn>
      <fn>
        <p>Dong Nie, Junfeng Lu and Han Zhang contributed equally.</p>
      </fn>
    </fn-group>
    <sec>
      <title>Electronic supplementary material</title>
      <p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-018-37387-9.</p>
    </sec>
    <ack>
      <title>Acknowledgements</title>
      <p>This work was supported in part by National Institutes of Health (NIH) grants (EB006733, EB008374, MH100217, MH108914, AG041721, AG049371, AG042599, AG053867, EB022880), in part by Science and Technology Commission of Shanghai Municipality (16410722400), the National Key Technology R\&amp;D Program of China (2014BAI04B05), Shanghai Municipal Commission of Health and Family Planning (2017YQ014), and the National Natural Science Foundation of China (81401395).</p>
    </ack>
    <notes notes-type="author-contribution">
      <title>Author Contributions</title>
      <p>D.N. and H.Z. conducted the experiments. J.L., Z.Y., L.L. and J.W. processed the data. D.N., H.Z., J.W. and D.S. wrote and revised the main manuscript text. All authors reviewed the manuscript.</p>
    </notes>
    <notes notes-type="COI-statement">
      <sec id="FPar1">
        <title>Competing Interests</title>
        <p>The authors declare no competing interests.</p>
      </sec>
    </notes>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Curran</surname>
              <given-names>WJ</given-names>
              <suffix>Jr.</suffix>
            </name>
            <etal/>
          </person-group>
          <article-title>Recursive partitioning analysis of prognostic factors in three radiation therapy oncology group malignant glioma trials</article-title>
          <source>JNCI: Journal of the National Cancer Institute</source>
          <year>1993</year>
          <volume>85</volume>
          <fpage>704</fpage>
          <lpage>710</lpage>
          <pub-id pub-id-type="doi">10.1093/jnci/85.9.704</pub-id>
          <pub-id pub-id-type="pmid">8478956</pub-id>
        </element-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gittleman</surname>
              <given-names>H</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>An independently validated nomogram for individualized estimation of survival among patients with newly diagnosed glioblastoma: Nrg oncology rtog 0525 and 0825</article-title>
          <source>Neuro-oncology</source>
          <year>2017</year>
          <volume>19</volume>
          <fpage>669</fpage>
          <lpage>677</lpage>
          <pub-id pub-id-type="doi">10.1093/neuonc/nox168.286</pub-id>
          <pub-id pub-id-type="pmid">28453749</pub-id>
        </element-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lacroix</surname>
              <given-names>M</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A multivariate analysis of 416 patients with glioblastoma multiforme: prognosis, extent of resection, and survival</article-title>
          <source>Journal of neurosurgery</source>
          <year>2001</year>
          <volume>95</volume>
          <fpage>190</fpage>
          <lpage>198</lpage>
          <pub-id pub-id-type="doi">10.3171/jns.2001.95.2.0190</pub-id>
        </element-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>DeAngelis</surname>
              <given-names>LM</given-names>
            </name>
          </person-group>
          <article-title>Brain tumors</article-title>
          <source>New England Journal of Medicine</source>
          <year>2001</year>
          <volume>344</volume>
          <fpage>114</fpage>
          <lpage>123</lpage>
          <pub-id pub-id-type="doi">10.1056/NEJM200101113440207</pub-id>
          <pub-id pub-id-type="pmid">11150363</pub-id>
        </element-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guillamo</surname>
              <given-names>J-S</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Brainstem gliomas in adults: prognostic factors and classification</article-title>
          <source>Brain</source>
          <year>2001</year>
          <volume>124</volume>
          <fpage>2528</fpage>
          <lpage>2539</lpage>
          <pub-id pub-id-type="doi">10.1093/brain/124.12.2528</pub-id>
          <pub-id pub-id-type="pmid">11701605</pub-id>
        </element-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Coons</surname>
              <given-names>SW</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>PC</given-names>
            </name>
            <name>
              <surname>Scheithauer</surname>
              <given-names>BW</given-names>
            </name>
            <name>
              <surname>Yates</surname>
              <given-names>AJ</given-names>
            </name>
            <name>
              <surname>Pearl</surname>
              <given-names>DK</given-names>
            </name>
          </person-group>
          <article-title>Improving diagnostic accuracy and interobserver concordance in the classification and grading of primary gliomas</article-title>
          <source>Cancer</source>
          <year>1997</year>
          <volume>79</volume>
          <fpage>1381</fpage>
          <lpage>1393</lpage>
          <pub-id pub-id-type="doi">10.1002/(SICI)1097-0142(19970401)79:7&lt;1381::AID-CNCR16&gt;3.0.CO;2-W</pub-id>
          <pub-id pub-id-type="pmid">9083161</pub-id>
        </element-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eckel-Passow</surname>
              <given-names>JE</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Glioma groups based on 1p/19q, idh, and tert promoter mutations in tumors</article-title>
          <source>New England Journal of Medicine</source>
          <year>2015</year>
          <volume>372</volume>
          <fpage>2499</fpage>
          <lpage>2508</lpage>
          <pub-id pub-id-type="doi">10.1056/NEJMoa1407279</pub-id>
          <pub-id pub-id-type="pmid">26061753</pub-id>
        </element-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <mixed-citation publication-type="other">Cancer Genome Atlas Research Networ Comprehensive, integrative genomic analysis of diffuse lower-grade gliomas. <italic>New England Journal of Medicine</italic><bold>372</bold>, 2481–2498 (2015).</mixed-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van den Bent</surname>
              <given-names>MJ</given-names>
            </name>
          </person-group>
          <article-title>Interobserver variation of the histopathological diagnosis in clinical trials on glioma: a clinician’s perspective</article-title>
          <source>Acta neuropathologica</source>
          <year>2010</year>
          <volume>120</volume>
          <fpage>297</fpage>
          <lpage>304</lpage>
          <pub-id pub-id-type="doi">10.1007/s00401-010-0725-7</pub-id>
          <pub-id pub-id-type="pmid">20644945</pub-id>
        </element-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Itakura</surname>
              <given-names>H</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Magnetic resonance image features identify glioblastoma phenotypic subtypes with distinct molecular pathway activities</article-title>
          <source>Science translational medicine</source>
          <year>2015</year>
          <volume>7</volume>
          <fpage>303ra138</fpage>
          <lpage>303ra138</lpage>
          <pub-id pub-id-type="doi">10.1126/scitranslmed.aaa7582</pub-id>
        </element-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>L</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Learning-based structurally-guided construction of resting-state functional correlation tensors</article-title>
          <source>Magnetic resonance imaging</source>
          <year>2017</year>
          <volume>43</volume>
          <fpage>110</fpage>
          <lpage>121</lpage>
          <pub-id pub-id-type="doi">10.1016/j.mri.2017.07.008</pub-id>
          <pub-id pub-id-type="pmid">28729016</pub-id>
        </element-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <mixed-citation publication-type="other">Zhu, X., Suk, H.-.I., Lee, S.-W. &amp; Shen, D. Subspace regularized sparse multi-task learning for multi-class neurodegenerative disease identification. <italic>IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING</italic><bold>63</bold>, 607–618 (2016).</mixed-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Suk</surname>
              <given-names>H.-I.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S.-W.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>A novel relational regularization feature selection method for joint regression and classification in AD diagnosis</article-title>
          <source>Medical Image Analysis</source>
          <year>2017</year>
          <volume>38</volume>
          <fpage>205</fpage>
          <lpage>214</lpage>
          <pub-id pub-id-type="doi">10.1016/j.media.2015.10.008</pub-id>
          <pub-id pub-id-type="pmid">26674971</pub-id>
        </element-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gutman</surname>
              <given-names>DA</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Mr imaging predictors of molecular profile and survival: multi-institutional study of the tcga glioblastoma data set</article-title>
          <source>Radiology</source>
          <year>2013</year>
          <volume>267</volume>
          <fpage>560</fpage>
          <lpage>569</lpage>
          <pub-id pub-id-type="doi">10.1148/radiol.13120118</pub-id>
          <pub-id pub-id-type="pmid">23392431</pub-id>
        </element-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kickingereder</surname>
              <given-names>P</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Radiomic profiling of glioblastoma: identifying an imaging predictor of patient survival with improved performance over established clinical and radiologic risk models</article-title>
          <source>Radiology</source>
          <year>2016</year>
          <volume>280</volume>
          <fpage>880</fpage>
          <lpage>889</lpage>
          <pub-id pub-id-type="doi">10.1148/radiol.2016160845</pub-id>
          <pub-id pub-id-type="pmid">27326665</pub-id>
        </element-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Patterns of tumor contrast enhancement predict the prognosis of anaplastic gliomas with idh1 mutation</article-title>
          <source>American Journal of Neuroradiology</source>
          <year>2015</year>
          <volume>36</volume>
          <fpage>2023</fpage>
          <lpage>2029</lpage>
          <pub-id pub-id-type="doi">10.3174/ajnr.A4407</pub-id>
          <pub-id pub-id-type="pmid">26316565</pub-id>
        </element-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pope</surname>
              <given-names>WB</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Mr imaging correlates of survival in patients with high-grade gliomas</article-title>
          <source>American Journal of Neuroradiology</source>
          <year>2005</year>
          <volume>26</volume>
          <fpage>2466</fpage>
          <lpage>2474</lpage>
          <pub-id pub-id-type="pmid">16286386</pub-id>
        </element-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>R</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Genomic mapping and survival prediction in glioblastoma: molecular subclassification strengthened by hemodynamic imaging biomarkers</article-title>
          <source>Radiology</source>
          <year>2013</year>
          <volume>267</volume>
          <fpage>212</fpage>
          <lpage>220</lpage>
          <pub-id pub-id-type="doi">10.1148/radiol.12120846</pub-id>
          <pub-id pub-id-type="pmid">23238158</pub-id>
        </element-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>R</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Outcome prediction in patients with glioblastoma by using imaging, clinical, and genomic biomarkers: focus on the nonenhancing component of the tumor</article-title>
          <source>Radiology</source>
          <year>2014</year>
          <volume>272</volume>
          <fpage>484</fpage>
          <lpage>493</lpage>
          <pub-id pub-id-type="doi">10.1148/radiol.14131691</pub-id>
          <pub-id pub-id-type="pmid">24646147</pub-id>
        </element-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reyes-Botero</surname>
              <given-names>G</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Contrast enhancement in 1p/19q-codeleted anaplastic oligodendrogliomas is associated with 9p loss, genomic instability, and angiogenic gene expression</article-title>
          <source>Neuro-oncology</source>
          <year>2013</year>
          <volume>16</volume>
          <fpage>662</fpage>
          <lpage>670</lpage>
          <pub-id pub-id-type="doi">10.1093/neuonc/not235</pub-id>
          <pub-id pub-id-type="pmid">24353325</pub-id>
        </element-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Saksena</surname>
              <given-names>S</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Predicting survival in glioblastomas using diffusion tensor imaging metrics</article-title>
          <source>Journal of Magnetic Resonance Imaging</source>
          <year>2010</year>
          <volume>32</volume>
          <fpage>788</fpage>
          <lpage>795</lpage>
          <pub-id pub-id-type="doi">10.1002/jmri.22304</pub-id>
          <pub-id pub-id-type="pmid">20882608</pub-id>
        </element-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Oh</surname>
              <given-names>J</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Survival analysis in patients with glioblastoma multiforme: Predictive value of choline-to-n-acetylaspartate index, apparent diffusion coefficient, and relative cerebral blood volume</article-title>
          <source>Journal of Magnetic Resonance Imaging</source>
          <year>2004</year>
          <volume>19</volume>
          <fpage>546</fpage>
          <lpage>554</lpage>
          <pub-id pub-id-type="doi">10.1002/jmri.20039</pub-id>
          <pub-id pub-id-type="pmid">15112303</pub-id>
        </element-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zacharaki</surname>
              <given-names>EI</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Survival analysis of patients with high-grade gliomas based on data mining of imaging variables</article-title>
          <source>American Journal of Neuroradiology</source>
          <year>2012</year>
          <volume>33</volume>
          <fpage>1065</fpage>
          <lpage>1071</lpage>
          <pub-id pub-id-type="doi">10.3174/ajnr.A2939</pub-id>
          <pub-id pub-id-type="pmid">22322603</pub-id>
        </element-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>D</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Preoperative sensorimotor mapping in brain tumor patients using spontaneous fluctuations in neuronal activity imaged with functional magnetic resonance imaging: initial experience</article-title>
          <source>Operative Neurosurgery</source>
          <year>2009</year>
          <volume>65</volume>
          <fpage>ons226</fpage>
          <lpage>ons236</lpage>
          <pub-id pub-id-type="doi">10.1227/01.NEU.0000350868.95634.CA</pub-id>
        </element-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>S-P</given-names>
            </name>
            <name>
              <surname>Duong</surname>
              <given-names>TQ</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Iadecola</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S-G</given-names>
            </name>
          </person-group>
          <article-title>Relative changes of cerebral arterial and venous blood volumes during increased cerebral blood flow: implications for bold fmri</article-title>
          <source>Magnetic resonance in medicine</source>
          <year>2001</year>
          <volume>45</volume>
          <fpage>791</fpage>
          <lpage>800</lpage>
          <pub-id pub-id-type="doi">10.1002/mrm.1107</pub-id>
          <pub-id pub-id-type="pmid">11323805</pub-id>
        </element-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pillai</surname>
              <given-names>JJ</given-names>
            </name>
            <name>
              <surname>Mikulis</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Cerebrovascular reactivity mapping: an evolving standard for clinical functional imaging</article-title>
          <source>American Journal of Neuroradiology</source>
          <year>2015</year>
          <volume>36</volume>
          <fpage>7</fpage>
          <lpage>13</lpage>
          <pub-id pub-id-type="doi">10.3174/ajnr.A3941</pub-id>
          <pub-id pub-id-type="pmid">24788129</pub-id>
        </element-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hardee</surname>
              <given-names>ME</given-names>
            </name>
            <name>
              <surname>Zagzag</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Mechanisms of glioma-associated neovascularization</article-title>
          <source>The American journal of pathology</source>
          <year>2012</year>
          <volume>181</volume>
          <fpage>1126</fpage>
          <lpage>1141</lpage>
          <pub-id pub-id-type="doi">10.1016/j.ajpath.2012.06.030</pub-id>
          <pub-id pub-id-type="pmid">22858156</pub-id>
        </element-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Agnihotri</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Zadeh</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Metabolic reprogramming in glioblastoma: the influence of cancer metabolism on epigenetics and unanswered questions</article-title>
          <source>Neuro-oncology</source>
          <year>2015</year>
          <volume>18</volume>
          <fpage>160</fpage>
          <lpage>172</lpage>
          <pub-id pub-id-type="doi">10.1093/neuonc/nov125</pub-id>
          <pub-id pub-id-type="pmid">26180081</pub-id>
        </element-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mineo</surname>
              <given-names>J-F</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Prognosis factors of survival time in patients with glioblastoma multiforme: a multivariate analysis of 340 patients</article-title>
          <source>Acta neurochirurgica</source>
          <year>2007</year>
          <volume>149</volume>
          <fpage>245</fpage>
          <lpage>253</lpage>
          <pub-id pub-id-type="doi">10.1007/s00701-006-1092-y</pub-id>
          <pub-id pub-id-type="pmid">17273889</pub-id>
        </element-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>Y</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Prognostic imaging biomarkers in glioblastoma: development and independent validation on the basis of multiregion and quantitative analysis of mr images</article-title>
          <source>Radiology</source>
          <year>2015</year>
          <volume>278</volume>
          <fpage>546</fpage>
          <lpage>553</lpage>
          <pub-id pub-id-type="doi">10.1148/radiol.2015150358</pub-id>
          <pub-id pub-id-type="pmid">26348233</pub-id>
        </element-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <mixed-citation publication-type="other">Glorot, X. &amp; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In <italic>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</italic>, 249–256 (2010).</mixed-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Deep learning</article-title>
          <source>nature</source>
          <year>2015</year>
          <volume>521</volume>
          <fpage>436</fpage>
          <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
          <pub-id pub-id-type="pmid">26017442</pub-id>
        </element-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>Proceedings of the IEEE</source>
          <year>1998</year>
          <volume>86</volume>
          <fpage>2278</fpage>
          <lpage>2324</lpage>
          <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
        </element-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shen</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Suk</surname>
              <given-names>H-I</given-names>
            </name>
          </person-group>
          <article-title>Deep learning in medical image analysis</article-title>
          <source>Annual review of biomedical engineering</source>
          <year>2017</year>
          <volume>19</volume>
          <fpage>221</fpage>
          <lpage>248</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id>
        </element-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <mixed-citation publication-type="other">Spanhol, F. A., Oliveira, L. S., Petitjean, C. &amp; Heutte, L. Breast cancer histopathological image classification using convolutional neural networks. In <italic>Neural Networks (IJCNN), 2016 International Joint Conference on</italic>, 2560–2567 (IEEE, 2016).</mixed-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chang</surname>
              <given-names>C-C</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C-J</given-names>
            </name>
          </person-group>
          <article-title>Libsvm: a library for support vector machines</article-title>
          <source>ACM transactions on intelligent systems and technology (TIST)</source>
          <year>2011</year>
          <volume>2</volume>
          <fpage>27</fpage>
        </element-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fan</surname>
              <given-names>R-E</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>K-W</given-names>
            </name>
            <name>
              <surname>Hsieh</surname>
              <given-names>C-J</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X-R</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C-J</given-names>
            </name>
          </person-group>
          <article-title>Liblinear: A library for large linear classification</article-title>
          <source>Journal of machine learning research</source>
          <year>2008</year>
          <volume>9</volume>
          <fpage>1871</fpage>
          <lpage>1874</lpage>
        </element-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <mixed-citation publication-type="other">Nie, D., Zhang, H., Adeli, E., Liu, L. &amp; Shen, D. 3d deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 212–220 (Springer, 2016).</mixed-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>J-S</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Clinical evaluation and follow-up outcome of diffusion tensor imaging-based functional neuronavigation: a prospective, controlled study in patients with gliomas involving pyramidal tracts</article-title>
          <source>Neurosurgery</source>
          <year>2007</year>
          <volume>61</volume>
          <fpage>935</fpage>
          <lpage>949</lpage>
          <pub-id pub-id-type="doi">10.1227/01.neu.0000303189.80049.ab</pub-id>
          <pub-id pub-id-type="pmid">18091270</pub-id>
        </element-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stupp</surname>
              <given-names>R</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Promising survival for patients with newly diagnosed glioblastoma multiforme treated with concomitant radiation plus temozolomide followed by adjuvant temozolomide</article-title>
          <source>Journal of Clinical Oncology</source>
          <year>2002</year>
          <volume>20</volume>
          <fpage>1375</fpage>
          <lpage>1382</lpage>
          <pub-id pub-id-type="doi">10.1200/JCO.2002.20.5.1375</pub-id>
          <pub-id pub-id-type="pmid">11870182</pub-id>
        </element-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <mixed-citation publication-type="other">Zacharaki, E. I., Hogea, C. S., Shen, D., Biros, G. &amp; Davatzikos, C. Non-diffeomorphic registration of brain tumor images by simulating tissue loss and tumor growth. <italic>Neuroimage</italic><bold>46</bold>, 762–774 (2009).</mixed-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>Z</given-names>
            </name>
            <name>
              <surname>Zhong</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>Panda: a pipeline toolbox for analyzing brain diffusion images</article-title>
          <source>Frontiers in human neuroscience</source>
          <year>2013</year>
          <volume>7</volume>
          <fpage>42</fpage>
          <pub-id pub-id-type="pmid">23439846</pub-id>
        </element-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Alexander</surname>
              <given-names>AL</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Characterization of cerebral white matter properties using quantitative magnetic resonance imaging stains</article-title>
          <source>Brain connectivity</source>
          <year>2011</year>
          <volume>1</volume>
          <fpage>423</fpage>
          <lpage>446</lpage>
          <pub-id pub-id-type="doi">10.1089/brain.2011.0071</pub-id>
          <pub-id pub-id-type="pmid">22432902</pub-id>
        </element-citation>
      </ref>
      <ref id="CR44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Zang</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>Dparsf: a matlab toolbox for” pipeline” data analysis of resting-state fmri</article-title>
          <source>Frontiers in systems neuroscience</source>
          <year>2010</year>
          <volume>4</volume>
          <fpage>13</fpage>
          <pub-id pub-id-type="pmid">20577591</pub-id>
        </element-citation>
      </ref>
      <ref id="CR45">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buzsáki</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Draguhn</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Neuronal oscillations in cortical networks</article-title>
          <source>science</source>
          <year>2004</year>
          <volume>304</volume>
          <fpage>1926</fpage>
          <lpage>1929</lpage>
          <pub-id pub-id-type="doi">10.1126/science.1099745</pub-id>
          <pub-id pub-id-type="pmid">15218136</pub-id>
        </element-citation>
      </ref>
      <ref id="CR46">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penttonen</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Buzsáki</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Natural logarithmic relationship between brain oscillators</article-title>
          <source>Thalamus &amp; Related Systems</source>
          <year>2003</year>
          <volume>2</volume>
          <fpage>145</fpage>
          <lpage>152</lpage>
        </element-citation>
      </ref>
      <ref id="CR47">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ding</surname>
              <given-names>Z</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Visualizing functional pathways in the human brain using correlation tensors and magnetic resonance imaging</article-title>
          <source>Magnetic resonance imaging</source>
          <year>2016</year>
          <volume>34</volume>
          <fpage>8</fpage>
          <lpage>17</lpage>
          <pub-id pub-id-type="doi">10.1016/j.mri.2015.10.003</pub-id>
          <pub-id pub-id-type="pmid">26477562</pub-id>
        </element-citation>
      </ref>
      <ref id="CR48">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>T-L</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Effects of anesthesia on resting state bold signals in white matter of non-human primates</article-title>
          <source>Magnetic resonance imaging</source>
          <year>2016</year>
          <volume>34</volume>
          <fpage>1235</fpage>
          <lpage>1241</lpage>
          <pub-id pub-id-type="doi">10.1016/j.mri.2016.07.001</pub-id>
          <pub-id pub-id-type="pmid">27451405</pub-id>
        </element-citation>
      </ref>
      <ref id="CR49">
        <label>49.</label>
        <mixed-citation publication-type="other">Jia, Y. <italic>et al</italic>. Caffe: Convolutional architecture for fast feature embedding. In <italic>Proceedings of the 22nd ACM international conference on Multimedia</italic>, 675–678 (ACM, 2014).</mixed-citation>
      </ref>
      <ref id="CR50">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guyon</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Elisseeff</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>An introduction to variable and feature selection</article-title>
          <source>Journal of machine learning research</source>
          <year>2003</year>
          <volume>3</volume>
          <fpage>1157</fpage>
          <lpage>1182</lpage>
        </element-citation>
      </ref>
      <ref id="CR51">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>J</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Slep: Sparse learning with efficient projections</article-title>
          <source>Arizona State University</source>
          <year>2009</year>
          <volume>6</volume>
          <fpage>7</fpage>
        </element-citation>
      </ref>
      <ref id="CR52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lowe</surname>
              <given-names>DG</given-names>
            </name>
          </person-group>
          <article-title>Distinctive image features from scale-invariant keypoints</article-title>
          <source>International journal of computer vision</source>
          <year>2004</year>
          <volume>60</volume>
          <fpage>91</fpage>
          <lpage>110</lpage>
          <pub-id pub-id-type="doi">10.1023/B:VISI.0000029664.99615.94</pub-id>
        </element-citation>
      </ref>
      <ref id="CR53">
        <label>53.</label>
        <mixed-citation publication-type="other">Scovanner, P., Ali, S. &amp; Shah, M. A 3-dimensional sift descriptor and its application to action recognition. In <italic>Proceedings of the 15th ACM international conference on Multimedia</italic>, 357–360 (ACM, 2007).</mixed-citation>
      </ref>
      <ref id="CR54">
        <label>54.</label>
        <mixed-citation publication-type="other">Yang, J., Jiang, Y.-G., Hauptmann, A. G. &amp; Ngo, C.-W. Evaluating bag-of-visual-words representations in scene classification. In <italic>Proceedings of the international workshop on Workshop on multimedia information retrieval</italic>, 197–206 (ACM, 2007).</mixed-citation>
      </ref>
      <ref id="CR55">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Viola</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>MJ</given-names>
            </name>
          </person-group>
          <article-title>Robust real-time face detection</article-title>
          <source>International journal of computer vision</source>
          <year>2004</year>
          <volume>57</volume>
          <fpage>137</fpage>
          <lpage>154</lpage>
          <pub-id pub-id-type="doi">10.1023/B:VISI.0000013087.49260.fb</pub-id>
        </element-citation>
      </ref>
      <ref id="CR56">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Q</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Automatic labeling of mr brain images by hierarchical learning of atlas forests</article-title>
          <source>Medical physics</source>
          <year>2016</year>
          <volume>43</volume>
          <fpage>1175</fpage>
          <lpage>1186</lpage>
          <pub-id pub-id-type="doi">10.1118/1.4941011</pub-id>
          <pub-id pub-id-type="pmid">26936703</pub-id>
        </element-citation>
      </ref>
      <ref id="CR57">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>L</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Concatenated spatially-localized random forests for hippocampus labeling in adult and infant mr brain images</article-title>
          <source>Neurocomputing</source>
          <year>2017</year>
          <volume>229</volume>
          <fpage>3</fpage>
          <lpage>12</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2016.05.082</pub-id>
          <pub-id pub-id-type="pmid">28133417</pub-id>
        </element-citation>
      </ref>
      <ref id="CR58">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Setio</surname>
              <given-names>AAA</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Pulmonary nodule detection in ct images: false positive reduction using multi-view convolutional networks</article-title>
          <source>IEEE transactions on medical imaging</source>
          <year>2016</year>
          <volume>35</volume>
          <fpage>1160</fpage>
          <lpage>1169</lpage>
          <pub-id pub-id-type="doi">10.1109/TMI.2016.2536809</pub-id>
          <pub-id pub-id-type="pmid">26955024</pub-id>
        </element-citation>
      </ref>
      <ref id="CR59">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Esteva</surname>
              <given-names>A</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>
          <source>Nature</source>
          <year>2017</year>
          <volume>542</volume>
          <fpage>115</fpage>
          <pub-id pub-id-type="doi">10.1038/nature21056</pub-id>
          <pub-id pub-id-type="pmid">28117445</pub-id>
        </element-citation>
      </ref>
      <ref id="CR60">
        <label>60.</label>
        <mixed-citation publication-type="other">Chen, L. <italic>et al</italic>. Multi-label inductive matrix completion for joint mgmt and idh1 status prediction for glioma patients. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic>, 450–458 (Springer, 2017).</mixed-citation>
      </ref>
      <ref id="CR61">
        <label>61.</label>
        <mixed-citation publication-type="other">Xu, Y. <italic>et al</italic>. Deep learning of feature representation with multiple instance learning for medical image analysis. In <italic>Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on</italic>, 1626–1630 (IEEE, 2014).</mixed-citation>
      </ref>
      <ref id="CR62">
        <label>62.</label>
        <mixed-citation publication-type="other">Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. <italic>arXiv preprint arXiv:1409.1556</italic> (2014).</mixed-citation>
      </ref>
      <ref id="CR63">
        <label>63.</label>
        <mixed-citation publication-type="other">Srivastava, N. &amp; Salakhutdinov, R. R. Multimodal learning with deep boltzmann machines. In <italic>Advances in neural information processing systems</italic>, 2222–2230 (2012).</mixed-citation>
      </ref>
      <ref id="CR64">
        <label>64.</label>
        <mixed-citation publication-type="other">Long, J. L., Zhang, N. &amp; Darrell, T. Do convnets learn correspondence? In <italic>Advances in Neural Information Processing Systems</italic>, 1601–1609 (2014).</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
