<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Proc Natl Acad Sci U S A</journal-id>
      <journal-id journal-id-type="iso-abbrev">Proc Natl Acad Sci U S A</journal-id>
      <journal-id journal-id-type="publisher-id">PNAS</journal-id>
      <journal-title-group>
        <journal-title>Proceedings of the National Academy of Sciences of the United States of America</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0027-8424</issn>
      <issn pub-type="epub">1091-6490</issn>
      <publisher>
        <publisher-name>National Academy of Sciences</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">37467265</article-id>
      <article-id pub-id-type="pmc">10372615</article-id>
      <article-id pub-id-type="publisher-id">202300888</article-id>
      <article-id pub-id-type="doi">10.1073/pnas.2300888120</article-id>
      <article-categories>
        <subj-group subj-group-type="badge">
          <compound-subject>
            <compound-subject-part content-type="code">dataset</compound-subject-part>
            <compound-subject-part content-type="label">Dataset</compound-subject-part>
          </compound-subject>
        </subj-group>
        <subj-group subj-group-type="type">
          <compound-subject>
            <compound-subject-part content-type="code">research-article</compound-subject-part>
            <compound-subject-part content-type="label">Research Article</compound-subject-part>
          </compound-subject>
        </subj-group>
        <subj-group subj-group-type="topic">
          <compound-subject>
            <compound-subject-part content-type="code">neuro</compound-subject-part>
            <compound-subject-part content-type="label">Neuroscience</compound-subject-part>
          </compound-subject>
        </subj-group>
        <subj-group subj-group-type="hwp-journal-coll">
          <subject>424</subject>
        </subj-group>
        <subj-group subj-group-type="heading">
          <subject>Biological Sciences</subject>
          <subj-group>
            <subject>Neuroscience</subject>
          </subj-group>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Partial entropy decomposition reveals higher-order information structures in human brain activity</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Varley</surname>
            <given-names>Thomas F.</given-names>
          </name>
          <email>tvarley@iu.edu</email>
          <xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref>
          <xref rid="aff2" ref-type="aff">
<sup>b</sup>
</xref>
          <xref rid="cor1" ref-type="corresp">
<sup>1</sup>
</xref>
          <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3317-9882</contrib-id>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Pope</surname>
            <given-names>Maria</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref>
          <xref rid="aff3" ref-type="aff">
<sup>c</sup>
</xref>
          <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6717-7132</contrib-id>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Puxeddu</surname>
            <given-names>Maria Grazia</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">
<sup>b</sup>
</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Faskowitz</surname>
            <given-names>Joshua</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref>
          <xref rid="aff3" ref-type="aff">
<sup>c</sup>
</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>a</sup>
</xref>
          <xref rid="aff2" ref-type="aff">
<sup>b</sup>
</xref>
          <xref rid="aff3" ref-type="aff">
<sup>c</sup>
</xref>
        </contrib>
        <aff id="aff1"><sup>a</sup><institution>School of Informatics, Computing and Engineering</institution>, <institution>Indiana University</institution>, <city>Bloomington</city>, <state>IN</state>
<postal-code>47405</postal-code></aff>
        <aff id="aff2"><sup>b</sup><institution>Department of Psychological and Brain Sciences</institution>, <institution>Indiana University</institution>, <city>Bloomington</city>, <state>IN</state>
<postal-code>47405</postal-code></aff>
        <aff id="aff3"><sup>c</sup><institution>Program in Neuroscience</institution>, <institution>Indiana University</institution>, <city>Bloomington</city>, <state>IN</state>
<postal-code>47405</postal-code></aff>
      </contrib-group>
      <author-notes>
        <corresp id="cor1"><sup>1</sup>To whom correspondence may be addressed. Email: <email>tvarley@iu.edu</email>.</corresp>
        <fn fn-type="edited-by" id="fn1">
          <p>Edited by Marcus Raichle, Washington University in St Louis School of Medicine, St. Louis, MO; received January 16, 2023; accepted June 6, 2023</p>
        </fn>
      </author-notes>
      <pub-date publication-format="electronic" date-type="pub">
        <day>19</day>
        <month>7</month>
        <year>2023</year>
      </pub-date>
      <pub-date publication-format="print" date-type="pub">
        <day>25</day>
        <month>7</month>
        <year>2023</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>19</day>
        <month>7</month>
        <year>2023</year>
      </pub-date>
      <!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.-->
      <volume>120</volume>
      <issue>30</issue>
      <elocation-id>e2300888120</elocation-id>
      <history>
<date date-type="received"><day>16</day><month>1</month><year>2023</year></date>
<date date-type="accepted"><day>06</day><month>6</month><year>2023</year></date>
</history>
      <permissions>
        <copyright-statement>Copyright © 2023 the Author(s). Published by PNAS.</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
          <license-p>This open access article is distributed under <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND)</ext-link>.</license-p>
        </license>
      </permissions>
      <self-uri content-type="pdf" xlink:href="pnas.202300888.pdf"/>
      <abstract abstract-type="executive-summary">
        <title>Significance</title>
        <p>One of the most common ways scientists understand the brain is as a network. This approach is limited, though; since everything is built out of pairs, there is no way to directly assess the interaction of three or more elements at once. In this paper, we use information theory to explore the information that is synergistic (i.e., cannot be reduced to pairs of nodes). We find that synergistic information is very widespread and is invisible to standard network-based approaches. This structure is complex and changes over time, opening a vast space to explore for brain/behavior relationships.</p>
      </abstract>
      <abstract>
        <p>The standard approach to modeling the human brain as a complex system is with a network, where the basic unit of interaction is a pairwise link between two brain regions. While powerful, this approach is limited by the inability to assess higher-order interactions involving three or more elements directly. In this work, we explore a method for capturing higher-order dependencies in multivariate data: the partial entropy decomposition (PED). Our approach decomposes the joint entropy of the whole system into a set of nonnegative atoms that describe the redundant, unique, and synergistic interactions that compose the system’s structure. PED gives insight into the mathematics of functional connectivity and its limitation. When applied to resting-state fMRI data, we find robust evidence of higher-order synergies that are largely invisible to standard functional connectivity analyses. Our approach can also be localized in time, allowing a frame-by-frame analysis of how the distributions of redundancies and synergies change over the course of a recording. We find that different ensembles of regions can transiently change from being redundancy-dominated to synergy-dominated and that the temporal pattern is structured in time. These results provide strong evidence that there exists a large space of unexplored structures in human brain data that have been largely missed by a focus on bivariate network connectivity models. This synergistic structure is dynamic in time and likely will illuminate interesting links between brain and behavior. Beyond brain-specific application, the PED provides a very general approach for understanding higher-order structures in a variety of complex systems.</p>
      </abstract>
      <kwd-group>
        <kwd>information theory</kwd>
        <kwd>synergy</kwd>
        <kwd>higher-order network</kwd>
        <kwd>fMRI</kwd>
        <kwd>neuroscience</kwd>
      </kwd-group>
      <funding-group specific-use="FundRef">
        <award-group award-type="grant">
          <funding-source id="sp1">
<institution-wrap><institution>National Science Foundation (NSF)</institution><institution-id institution-id-type="FundRef">100000001</institution-id></institution-wrap>
</funding-source>
          <award-id rid="sp1">1735095</award-id>
          <principal-award-recipient>Thomas Varley</principal-award-recipient>
          <principal-award-recipient>Maria Pope</principal-award-recipient>
        </award-group>
      </funding-group>
      <counts>
        <page-count count="12"/>
        <word-count count="8960"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <p content-type="flushleft">Since the notion of the connectome was first formalized in neuroscience (<xref rid="r1" ref-type="bibr">1</xref>), network models of the nervous system have become ubiquitous in the field (<xref rid="r2" ref-type="bibr">2</xref>, <xref rid="r3" ref-type="bibr">3</xref>). In a network model, elements of a complex system (typically neurons or brain regions) are modeled as a graph composed of vertices (or nodes) connected by edges, which denote some kind of connectivity or statistical dependency between them. Arguably, the most ubiquitous application of network models to the brain is the functional connectivity (FC) framework (<xref rid="r3" ref-type="bibr">3</xref><xref rid="r4" ref-type="bibr"/>–<xref rid="r5" ref-type="bibr">5</xref>). In whole-brain neuroimaging, FC networks generally define connections as correlations between the associated regional time series (e.g., fMRI BOLD signals, EEG waves, etc). The correlation matrix is then cast as the adjacency matrix of a weighted network, on which a wide number of network measures can be computed (<xref rid="r6" ref-type="bibr">6</xref>).</p>
    <p>Despite the widespread adoption of functional connectivity analyses, there remains a little-discussed, but profound, limitation inherent to the entire methodology: The only statistical dependencies directly visible to pairwise correlation are bivariate, and in the most commonly performed network analyses, every edge between pairs <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> is treated as independent of any other edge. There are no direct ways to infer statistical dependencies between three or more variables. Higher-order interactions are constructed by aggregating bivariate couplings in analyses such as motifs (<xref rid="r7" ref-type="bibr">7</xref>) or community detection (<xref rid="r8" ref-type="bibr">8</xref>). One of the largest issues holding back the direct study of higher-order interactions has been the lack of effective, accessible mathematical tools with which such interactions can be recognized (<xref rid="r9" ref-type="bibr">9</xref>). Recently, however, work in the field of multivariate information theory has enabled the development of a plethora of different measures and frameworks for capturing statistical dependencies beyond the pairwise correlation (<xref rid="r10" ref-type="bibr">10</xref>).</p>
    <p>The few applications of these techniques to brain data have suggested that higher-order dependencies can encode meaningful biomarkers such as discriminating between health and pathological states induced by anesthesia or brain injury (<xref rid="r11" ref-type="bibr">11</xref>) and reflect changes associated with age (<xref rid="r12" ref-type="bibr">12</xref>). Since the space of possible higher-order structures is so much vaster than the space of pairwise dependencies, the development of tools that make these structures accessible opens the doors to a large number of possible studies linking brain activity to cognition and behavior.</p>
    <p>One of the most well-developed tools is the partial information decomposition (<xref rid="r13" ref-type="bibr">13</xref>, <xref rid="r14" ref-type="bibr">14</xref>) (PID), which reveals that multiple interacting variables can participate in a variety of distinct information-sharing relationships, including redundant, unique, and synergistic modes. Redundant and synergistic information sharing represent two distinct but related types of higher-order interaction: Redundancy refers to information that is duplicated over many elements, so that the same information could be learned by observing <italic toggle="yes">X</italic><sub>1</sub> ∨ <italic toggle="yes">X</italic><sub>2</sub>, ∨, …, ∨<italic toggle="yes">X</italic><sub><italic toggle="yes">N</italic></sub>. In contrast, synergy refers to information that is only accessible when considering the joint states of multiple elements and no simpler combinations of sources. Synergistic information can only be learned by observing <italic toggle="yes">X</italic><sub>1</sub> ∧ … ∧ <italic toggle="yes">X</italic><sub><italic toggle="yes">N</italic></sub>.</p>
    <p>Redundant and synergistic information-sharing modes can be combined to create more complex relationships. For example, given three variables <italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>2</sub>, and <italic toggle="yes">X</italic><sub>3</sub>, information can be redundantly common to all three, which could be learned by observing <italic toggle="yes">X</italic><sub>1</sub> ∨ <italic toggle="yes">X</italic><sub>2</sub> ∨ <italic toggle="yes">X</italic><sub>3</sub>. We can also consider the information redundantly shared by joint states: For example, the information that could be learned by observing <italic toggle="yes">X</italic><sub>1</sub> ∨ (<italic toggle="yes">X</italic><sub>2</sub> ∧ <italic toggle="yes">X</italic><sub>3</sub>) (i.e., observing <italic toggle="yes">X</italic><sub>1</sub> or the joint state of <italic toggle="yes">X</italic><sub>2</sub> and <italic toggle="yes">X</italic><sub>3</sub>). For a finite set of interacting variables, it is possible to enumerate all possible information-sharing modes, and given a formal definition of redundancy, they can be calculated (for details see below).</p>
    <p>The identification of redundancy and synergy as possible families of statistical dependence raises questions about how such relationships might be reflected (or missed) by the standard, pairwise correlation-based approach for inferring networks. We propose two criteria by which we might assess the performance of bivariate functional connectivity. The first we call specificity: the degree to which a pairwise correlation between some <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> reports dependencies that are unique to <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> alone and not shared with any other edges. In a sense, it reflects how appropriate the ubiquitous assumption that edges are independent is. The second criterion we call completeness: whether all of the statistical dependencies present in a dataset are accounted for and incorporated into the model.</p>
    <p>We hypothesized that classical functional connectivity would prove to be both nonspecific (due to the presence of multivariate redundancies that get repeatedly seen by many pairwise correlations) and incomplete (due to the presence of synergies). To test this hypothesis, we used a framework derived from the PID: the partial entropy decomposition (PED) (<xref rid="r15" ref-type="bibr">15</xref>) (PED, explained in detail below) to fully retrieve all components of statistical dependencies in sets of three and four brain regions.</p>
    <p>By computing the full PED for all triads of 200 brain regions, and a subset of approximately two million tetrads, we can provide a rich and detailed picture of beyond-pairwise dependencies in the brain. Furthermore, by separately considering redundancy and synergy instead of assessing just which one is dominant as is commonly done (<xref rid="r12" ref-type="bibr">12</xref>, <xref rid="r16" ref-type="bibr">16</xref>), we can reveal previously unseen structures in resting-state brain activity.</p>
    <sec id="s1">
      <label>1.</label>
      <title>Theory</title>
      <sec id="s2">
        <title>A Note on Notation.</title>
        <p>In this paper, we will be making reference to multiple different kinds of random variables. In general, we will use uppercase italics to refer to single variables (e.g., <italic toggle="yes">X</italic>). Sets of multiple variables will be denoted in boldface (e.g., <bold>X</bold> = {<italic toggle="yes">X</italic><sub>1</sub>, …, <italic toggle="yes">X</italic><sub><italic toggle="yes">N</italic></sub>}, with subscript indexing). Specific instances of a variable will be denoted with lowercase font: <italic toggle="yes">X</italic> = <italic toggle="yes">x</italic>. Functions (such as the probability, entropy, and mutual information) will be denoted using calligraphic font. Finally, when referring to the partial entropy function ℋ<sub>∂</sub> (described below), we will use superscript index notation to indicate the full set of variables that contextualizes the individual atom (this notation was first introduced by Ince in ref. <xref rid="r15" ref-type="bibr">15</xref>). For example, ℋ<sub>∂</sub><sup>123</sup>({1}{2}) refers to the information redundantly shared by <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>, when both are considered as part of the triad <bold>X</bold> = {<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>2</sub>, <italic toggle="yes">X</italic><sub>3</sub>}, while ℋ<sub>∂</sub><sup>12</sup>({1}{2}) refers to the information redundantly shared by <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub> qua themselves.</p>
      </sec>
      <sec id="s3">
        <label>A.</label>
        <title>Partial Entropy Decomposition.</title>
        <p>The PED provides a framework with which we can extract all of the meaningful structure in a system of interacting random variables (<xref rid="r15" ref-type="bibr">15</xref>). By structure, we are referring to the (possibly higher-order) patterns of information-sharing between elements. For a detailed, mathematical derivation of the PED, see <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic></ext-link>, but we will briefly review the main concept here. We begin with the Shannon entropy of a multidimensional random variable:<disp-formula id="eqn1"><label>[1]</label><mml:math id="me1" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold-fraktur">X</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mstyle><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mo>log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>where <bold>x</bold> indicates a particular configuration of <bold>X</bold>, and 𝔛 is the support set of <bold>X</bold>. This joint entropy quantifies, on average, how much it is possible to know about <bold>X</bold> (i.e., how many bits of information would be required, on average, to reduce our uncertainty about the state of <bold>X</bold> to zero). The measure ℋ(<bold>X</bold>) is a very crude one: It gives us a single summary statistic that describes the behavior of the whole without making reference to the structure of the relationships between <bold>X</bold>’s constituent elements. If <bold>X</bold> has some nontrivial structure that integrates multiple elements (or ensembles of elements), then we propose that those elements must share entropy. This notion of shared entropy forms the cornerstone of the PED: By “share entropy,” we mean how much uncertainty about the state of the whole could be resolved by learning information about the states of the constituent parts.</p>
        <p>For example, consider the bivariate system <bold>X</bold> = {<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>2</sub>}. We can decompose the joint entropy:<disp-formula id="eqn2"><label>[2]</label><mml:math id="me2" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>We can understand these four values in terms of redundant, unique, and synergistic information-sharing modes. The first term ℋ<sub>∂</sub><sup>12</sup>({1}{2}) is the uncertainty about the state of <bold>X</bold> that would be resolved if we learned either <italic toggle="yes">X</italic><sub>1</sub> alone or <italic toggle="yes">X</italic><sub>2</sub> alone (redundancy). The term ℋ<sub>∂</sub><sup>12</sup>({1}) is the information about <bold>X</bold> that can only be learned by observing <italic toggle="yes">X</italic><sub>1</sub>, and likewise for ℋ<sub>∂</sub><sup>12</sup>({2}). The final term, ℋ<sub>∂</sub><sup>12</sup>({1, 2}) is the information about <bold>X</bold> that can only be learned when <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub> are learned together. Said otherwise, it is the irreducible information that the whole <bold>X</bold> that can only be learned by observing the whole <bold>X</bold> itself. Furthermore, we can decompose the associated marginal entropies in a manner consistent with the partial information decomposition (<xref rid="r13" ref-type="bibr">13</xref>):<disp-formula id="eqn3"><label>[3]</label><mml:math id="me3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="eqn4"><label>[4]</label><mml:math id="me4" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>The result is a set of three known variables (the joint and marginal entropies) and four unknown variables), resulting in an underdetermined system of equations. By defining a redundant entropy function that solves ℋ<sub>∂</sub><sup>12</sup>({1}{2}), it is possible to solve the remaining three terms with simple algebra. Here, we opt to use the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> measure first proposed by Makkeh et al. (<xref rid="r17" ref-type="bibr">17</xref>), discussed in more detail below.</p>
        <p>These decompositions can be done for larger ensembles or more statistical dependencies (see below) and can reveal how higher-order interactions can complicate (and in some cases, compromise) the standard bivariate approaches to functional connectivity.</p>
        <sec id="s4">
          <label>A.1.</label>
          <title>Analytic vs. empirical PED analysis.</title>
          <p>The PED reveals a rich and complex structure of statistical dependencies even in small systems. Like the PID, it is unusual in that it reveals the structure of multivariate information, but computing the value of any atom requires the additional step of proposing an operational measure of redundant entropy. Consequently, there are two lenses with which we can use PED to assess the relationship between higher-order information and functional connectivity. The first approach is analytic: considering the structure of multivariate entropy qua itself without defining a redundancy function. The second approach is empirical: After choosing a function, we can compute values of redundancy and synergy from real data and compare them numerically to other measures, such as the Pearson correlation coefficient. Both approaches have strengths and weaknesses: The strength of the analytic approach is in its universality. The results do not hinge on a particular definition of redundancy and reflect the fundamental mathematical structure of multivariate information. The primary weakness, however, is that the high level of abstraction makes analysis of real-world data impossible. In contrast, the empirical approach does require making an ad hoc choice of redundancy function. In the context of PID, different redundancy functions can lead to strikingly different results (<xref rid="r18" ref-type="bibr">18</xref>), and it is conceivable that similar effects may be inherited by the PED. Consequently, any particular set of results must be understood as reflecting the particular definition of redundancy chosen. The benefit of this approach is that, given a suitable redundancy function, it is possible to use PED to explore the information structure of real systems, rather than just the abstract structure of information itself.</p>
          <p>In this paper, we apply both lenses. In Section <xref rid="s5" ref-type="sec">A.2</xref>, we explore the analytic properties of the PED and discuss their implications for bivariate, FC network analysis, as well as existing information-theoretic measures of higher-order dependency, such as the O-information (<xref rid="r19" ref-type="bibr">19</xref>). In Section <xref rid="s8" ref-type="sec">B</xref>, we empirically analyze resting-state fMRI data using the PED coupled with the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> redundancy function (<xref rid="r17" ref-type="bibr">17</xref>), to compare the empirical distribution of higher-order redundancies and synergies with the structure of bivariate FC networks.</p>
        </sec>
        <sec id="s5">
          <label>A.2.</label>
          <title>Mathematical analysis of the PED.</title>
          <p>Before considering the empirical results (which requires operationalizing a method of redundancy), it is worth discussing how the PED analytically relates to classic measures from information theory and what it reveals about the limitations of bivariate FC measures. These results are agnostic to the specific definition of redundancy chosen are expected to hold for any viable redundant entropy function.</p>
          <p>The first key finding is that the PED provides interesting insights into the nature of bivariate mutual information. Typically, mutual information is conflated with redundancy at the outset (for example, in Venn diagrams); however, when considering the PED of two variables <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>, it becomes clear that:<disp-formula id="eqn5"><label>[5]</label><mml:math id="me5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">I</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>12</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>This relationship was originally noted by Ince (<xref rid="r15" ref-type="bibr">15</xref>) and later rederived by Finn and Lizier (<xref rid="r20" ref-type="bibr">20</xref>). In a sense, the higher-order information present in the joint state of (<italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>) obscures the lower-order structure. We conjecture that this issue is also inherited by parametric correlation measures based on the Pearson correlation coefficient, since the mutual information is a deterministic function of Pearson’s <italic toggle="yes">r</italic> for Gaussian variables (<xref rid="r21" ref-type="bibr">21</xref>). A deeper mathematical exploration of the relationship between partial entropy and other correlation measures beyond mutual information remains an area for future work.</p>
          <p>We can do a similar analysis extracting the bivariate mutual information from the trivariate PED (also first derived in ref. <xref rid="r15" ref-type="bibr">15</xref>), which reveals that the bivariate correlation is not specific:<disp-formula id="eqn6"><label>[6]</label><mml:math id="me6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>It is clear from Eq. <xref rid="eqn6" ref-type="disp-formula"><bold>6</bold></xref> that the bivariate mutual information incorporates information that is triple-redundant across three variables (ℋ<sub>∂</sub><sup>123</sup>({1}{2}{3})), and if one were to take the standard FC approach to a triad (pairwise correlation between all three pairs of elements), that the triple redundancy would be triple counted: ℋ<sub>∂</sub><sup>123</sup>({1}{2}{3}) will contribute positively to <italic toggle="yes">I</italic>(<italic toggle="yes">X</italic><sub>1</sub>; <italic toggle="yes">X</italic><sub>2</sub>), <italic toggle="yes">I</italic>(<italic toggle="yes">X</italic><sub>1</sub>; <italic toggle="yes">X</italic><sub>3</sub>), and <italic toggle="yes">I</italic>(<italic toggle="yes">X</italic><sub>2</sub>; <italic toggle="yes">X</italic><sub>3</sub>). Furthermore, not only does bivariate mutual information double-count redundancy, but it also penalizes higher-order synergies. Any higher-order atom that includes the joint state of <italic toggle="yes">X</italic><sub>1</sub> ∧ <italic toggle="yes">X</italic><sub>2</sub> counts against ℐ(<italic toggle="yes">X</italic><sub>1</sub>; <italic toggle="yes">X</italic><sub>2</sub>). We should stress that the above results do not mean that pairwise mutual information is “wrong” in any sense (the triple redundancy is part of the pairwise dependency), but it does complicate the interpretation of functional connectivity, particularly when the change in the value of a particular edge is of scientific interest. For example, many neuroimaging studies report results of the form “FC between region A and region B was greater in condition 1 than in condition 2”. These results are typically interpreted as revealing something specific about the computations regions A and B perform; however, the above results show that we cannot be confident that a change in pairwise connectivity is specific to those two nodes, but may be driven by higher-order, nonlocal redundancies (or alternately, suppressed by synergies). Consequently, while the FC network itself is mathematically well described, the interpretation and ability to appropriately assign dependencies to particular sets of regions is surprisingly complex and subtle.</p>
          <p>Having established that the presence of higher-order redundancies precludes bivariate correlation from being specific, we now ask the following: Can we improve the specificity using common statistical methods? One approach aimed at controlling for the context of additional variables in a bivariate correlation analysis is using conditioning or partial correlation. Typically, these analyses are assumed to improve the specificity of a pairwise dependency by removing the influence of confounders; however, by decomposing the conditional mutual information between three variables, we can see that conditioning does not ensure specificity:<disp-formula id="eqn7"><label>[7]</label><mml:math id="me7" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>The decomposition of ℐ(<italic toggle="yes">X</italic><sub>1</sub>; <italic toggle="yes">X</italic><sub>2</sub>|<italic toggle="yes">X</italic><sub>3</sub>) conflates the true pairwise redundancy (ℋ<sub>∂</sub><sup>123</sup>({1}{2})) with the a higher-order redundancy involving the joint state of <italic toggle="yes">X</italic><sub>1</sub> ∧ <italic toggle="yes">X</italic><sub>3</sub> and <italic toggle="yes">X</italic><sub>2</sub> ∧ <italic toggle="yes">X</italic><sub>3</sub>: ℋ<sub>∂</sub><sup>123</sup>({1, 3}{2, 3}) (<xref rid="r15" ref-type="bibr">15</xref>). Furthermore, the conditional mutual information penalizes synergistic entropy shared in the joint state of all three variables (ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3})). Consequently, we can conclude that the specificity of bivariate functional connectivity cannot be salvaged using conditioning or partial correlation. Not only does controlling fail to provide specificity, it also actively compromises completeness, since it brings in higher-order interactions. Given that conditional mutual information and partial correlation are equivalent for Gaussian variables (<xref rid="r22" ref-type="bibr">22</xref>), we conjecture that this issue also affects standard, parametric approaches to conditional connectivity, just as with bivariate mutual information/Pearson correlation.</p>
          <p>It is important to understand that these analytic results are not a consequence of the particular form of <italic toggle="yes">h</italic><sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub>: Any shared entropy function that allows for the formation of a partial entropy lattice will produce these same results many of these analytic relationships were first derived by Ince (<xref rid="r15" ref-type="bibr">15</xref>).</p>
        </sec>
        <sec id="s6">
          <label>A.3.</label>
          <title>Higher-order dependency measures.</title>
          <p>In addition, revealing the structure of commonly used correlations (bivariate and partial correlations), the PED can also be used to develop intuitions about multivariate generalizations of the mutual information. Many of these generalizations exist, and here, we will focus on four: the total correlation (<xref rid="r23" ref-type="bibr">23</xref>), the dual total correlation (<xref rid="r24" ref-type="bibr">24</xref>), the O-information, and the S-information (<xref rid="r19" ref-type="bibr">19</xref>). While useful, these measures are often difficult to intuitively understand and can display surprising behavior. Since they can all be written in terms of sums and differences of joint and marginal entropies, we can use the PED framework to more completely understand them.</p>
          <p>The oldest measure is the total correlation, defined as<disp-formula id="eqn8"><label>[8]</label><mml:math id="me8" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover></mml:mstyle><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>which is equivalent to the Kullback–Leibler divergence between the true joint distribution 𝒫(<bold>X</bold>) and the product of the marginals:<disp-formula id="eqn9"><label>[9]</label><mml:math id="me9" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi mathvariant="italic">KL</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover></mml:mstyle><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>Based on Eq. <xref rid="eqn9" ref-type="disp-formula"><bold>9</bold></xref>, we can understand the total correlation as the divergence from the maximum entropy distribution to the true distribution, implying that it might be something like a measure of the total structure of the system (as its name would suggest). We can decompose the 3-variable case to get a full picture of the structure of the TC:<disp-formula id="eqn10"><label>[10]</label><mml:math id="me10" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>For a step-by-step walkthrough of this derivation, see <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic>, SI 5</ext-link>: Derivations. We can see that the total correlation is largely a measure of redundancy, sensitive to information shared between single elements, but penalizing higher-order information present in joint states. This can be understood by considering the lattice in <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic> Fig. S2</ext-link>: Each of the ℋ(<italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub>) terms will only incorporate atoms preceding (or equal to) the unique entropy term ℋ<sub>∂</sub><sup>123</sup>(<italic toggle="yes">i</italic>)—anything that can only be seen by considering the joint state of <bold>X</bold> will be negative.</p>
          <p>The second generalization of mutual information is the dual total correlation (<xref rid="r24" ref-type="bibr">24</xref>). Defined in terms of entropies by<disp-formula id="eqn11"><label>[11]</label><mml:math id="me11" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:munderover></mml:mstyle><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>where <bold>X</bold><sup>−<italic toggle="yes">i</italic></sup> refers to the set of every element of <bold>X</bold> excluding the <italic toggle="yes">i</italic><sup><italic toggle="yes">t</italic><italic toggle="yes">h</italic></sup>. The dual total correlation can be understood as the difference between the total entropy of <bold>X</bold> and all of the entropy in each element of <italic toggle="yes">X</italic> that is intrinsic to it and not shared with any other part. When we decompose the three-variable case, we find<disp-formula id="eqn12"><label>[12]</label><mml:math id="me12" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>23</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>This shows that dual total correlation is a much more complete picture of the structure of a system than total correlation. It is sensitive to both shared redundancies and synergies, penalizing only the unshared, higher-order synergy terms such as ℋ<sub>∂</sub><sup>123</sup>({1, 2}).</p>
          <p>The sum of the total correlation and the dual total correlation is the exogenous information (<xref rid="r25" ref-type="bibr">25</xref>), also called by the S-information.<disp-formula id="eqn13"><label>[13]</label><mml:math id="me13" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>Prior work has shown the exogenous entropy to be very tightly correlated with the Tononi–Sporns–Edelman complexity (<xref rid="r16" ref-type="bibr">16</xref>, <xref rid="r19" ref-type="bibr">19</xref>, <xref rid="r26" ref-type="bibr">26</xref>), a measure of global integration/segregation balance. James also showed that the S-information quantified the total information that every element shares with every other element (<xref rid="r25" ref-type="bibr">25</xref>). We can see that<disp-formula id="ueqn1"><mml:math id="me21" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>This reveals that S-information to be an unusual measure, in that it counts each redundancy term multiple times (i.e., in the case of three variables, the triple redundancy term appears three times, each double-redundancy term appears twice, etc.) and penalizes them likewise when considering unshared synergies.</p>
          <p>The final, and arguably most interesting, measure is the difference between the total correlation, and the dual total correlation is often referred to as the O-information (<xref rid="r19" ref-type="bibr">19</xref>) and has been hypothesized to give a heuristic measure of the extent to which a given system is dominated by redundant or synergistic interactions:<disp-formula id="eqn14"><label>[14]</label><mml:math id="me14" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>where 𝒪(<bold>X</bold>)&gt; 0 implies a redundancy-dominated structure and 𝒪(<bold>X</bold>)&lt; 0 implies a synergy dominated one. PED analysis reveals<disp-formula id="eqn15"><label>[15]</label><mml:math id="me15" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="2em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>This shows that the O-information generally satisfies the intuitions proposed by Rosas et al., as it is positively sensitive to the nonpairwise redundancy (in this case just ℋ<sub>∂</sub><sup>123</sup>({1}{2}{3})) and negatively sensitive to any higher-order shared information. Curiously, 𝒪(<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>2</sub>, <italic toggle="yes">X</italic><sub>3</sub>) positively counts the highest, unshared synergy atom (ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3}). Conceivably, it may be possible for a set of three variables with no redundancy to return a positive O-information, although whether this can actually occur is an area of future research.</p>
          <p>For three-element systems, the O-information is also equivalent to the coinformation (<xref rid="r19" ref-type="bibr">19</xref>), which forms the base of the original redundant entropy function ℋ<sub><italic toggle="yes">c</italic><italic toggle="yes">s</italic></sub> proposed by Ince (<xref rid="r15" ref-type="bibr">15</xref>). From this, we can see that, at least for three variables, coinformation is not a pure measure of redundancy, conflating the true redundancy and the highest synergy term, as well as penalizing other higher-order modes of information-sharing. A similar argument was made by Williams and Beer using the mutual information-based interpretation of coinformation (<xref rid="r13" ref-type="bibr">13</xref>). While the O-information and coinformation diverge for <italic toggle="yes">N</italic> &gt; 3, we anticipate that the behavior of the coinformation will remain similarly complex at higher <italic toggle="yes">N</italic>. These results reveal how the PED framework can provide clarity to the often-murky world of multivariate information theory.</p>
        </sec>
        <sec id="s7">
          <label>A.4.</label>
          <title>Novel Higher-order measures.</title>
          <p>From these PED atoms, we can construct a measure of higher-order dependence that extends beyond TC, DTC, O-Information, and S-Information.</p>
          <p>When considering higher-order redundancy, we are interested in all of those atoms that duplicate information over three or more individual elements. We define this as the redundant structure. For a three-element system,<disp-formula id="eqn16"><label>[16]</label><mml:math id="me16" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>For a four-element system,<disp-formula id="eqn17"><label>[17]</label><mml:math id="me17" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>And so on for larger systems.</p>
          <p>We can also define an analogous measure of synergistic structure: All those atoms representing information duplicated over the joint state of two or more elements. The requirement that higher-order synergies must also be shared reflects the idea that the structure of a system refers to dependencies between elements or groups of elements. For example, for a three-element system,<disp-formula id="eqn18"><label>[18]</label><mml:math id="me18" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="script">S</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>∂</mml:mi></mml:mrow><mml:mn>123</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>Note that the atom ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3}) is not included in the synergistic structure, as it does not refer to information duplicated over (shared between) groups of elements. Instead, it refers to the intrinsic uncertainty the whole <bold>X</bold> that can only resolved by observing the whole.</p>
          <p>For three-element systems, the difference 𝒮<sub><italic toggle="yes">R</italic></sub> − 𝒮<sub><italic toggle="yes">S</italic></sub> is analogous to a “corrected” O-information: The atom ℋ<sub>∂</sub><sup>123</sup>({1, 2}{1, 3}{2, 3}) is counted only once, and the confounding triple synergy ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3}) is not included. Finally, we can define a measure of total (integrated) structure (i.e., all shared information) as the sum of all atoms composed of multiple sources:<disp-formula id="eqn19"><label>[19]</label><mml:math id="me19" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="bold-fraktur">A</mml:mi></mml:mrow></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo>⟺</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        </sec>
      </sec>
      <sec id="s8">
        <label>B.</label>
        <title>Applications to the Brain.</title>
        <p>The mathematical structure of the PED is domain agnostic: Any complex system composed of discrete random variables is amenable to this kind of information-theoretic analysis. In this paper, we focus on data collected from the human brain with functional magnetic resonance imaging (fMRI). For detailed methods, see the <italic toggle="yes">Materials &amp; Methods</italic>, but in brief, data from ninety-five human subjects resting quietly were recorded as part of the Human Connectome Project (<xref rid="r27" ref-type="bibr">27</xref>). All of the scans were concatenated and each channel binarized about the mean (<xref rid="r28" ref-type="bibr">28</xref>) to create multidimensional, binary time series. We then computed the full PED for all triads, and approximately two million tetrads, to compare to the standard, bivariate functional connectivity network (computed with mutual information).</p>
        <p>By looking at the redundant and synergistic structures, and relating them to the standard FC, we can explore how higher-order dependencies are represented in bivariate networks as well as what brain regions participate in more redundancy- or synergy-dominated ensembles.</p>
        <sec id="s9">
          <label>B.1.</label>
          <title>The 𝓗<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> redundancy function.</title>
          <p>As previously mentioned, the application of the PED to empirical data requires making a choice about the best way to operationalize the notion of redundant entropy. Here, we used the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> measure first proposed by Makkeh et al. (<xref rid="r17" ref-type="bibr">17</xref>), due to its intuitive interpretations in terms of logical conjunctions and disjunctions. For a set, <bold><italic toggle="yes">α</italic></bold>, of <italic toggle="yes">k</italic>, potentially overlapping, subsets of a variable <bold>X</bold> (referred to as sources), the redundant entropy shared by all sources is given by<disp-formula id="eqn20"><label>[20]</label><mml:math id="me20" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi mathvariant="italic">sx</mml:mi></mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>log</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∪</mml:mo><mml:mo>…</mml:mo><mml:mo>∪</mml:mo><mml:msub><mml:mi mathvariant="bold">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
          <p>For example, if <bold><italic toggle="yes">α</italic></bold> = {{<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>2</sub>},{<italic toggle="yes">X</italic><sub>1</sub>, <italic toggle="yes">X</italic><sub>3</sub>},{<italic toggle="yes">X</italic><sub>2</sub>, <italic toggle="yes">X</italic><sub>3</sub>}}, then <inline-formula><mml:math id="i1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi mathvariant="italic">sx</mml:mi></mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is interpreted as the information about the state of the whole <bold>X</bold> that could be learned by observing (<italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>) or (<italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>3</sub>) or (<italic toggle="yes">X</italic><sub>2</sub> and <italic toggle="yes">X</italic><sub>3</sub>). For a more detailed discussion of ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub>, see <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic></ext-link>.</p>
        </sec>
      </sec>
    </sec>
    <sec sec-type="results" id="s10">
      <label>2.</label>
      <title>Results</title>
      <sec id="s11">
        <label>A.</label>
        <title>PED Reveals the Limitations of Bivariate Networks.</title>
        <p>We now discuss how the PED relates to multivariate measures of bivariate network structure commonly used in the functional connectivity literature. These measures describe statistical dependencies between ensembles of regions but mediated by the topology of bivariate connections. We hypothesized that this emergence from bivariate dependencies would render them largely insensitive to synergies, which in turn would mean that such measures do not solve the issue of incompleteness in functional connectivity.</p>
        <p>Following (<xref rid="r29" ref-type="bibr">29</xref>), we compared the redundant and synergistic structure of triads and tetrads to a measure of subgraph strength: the arithmetic mean of all edges in the subgraph. We found that the arithmetic mean FC density was positively correlated with redundancy for triads (Pearson’s <italic toggle="yes">r</italic> = 0.999, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>) and tetrads (Pearson’s <italic toggle="yes">r</italic> = 0.995, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>), indicating that information duplicated over many brain regions contributes to multiple edges, leading to double-counting. In contrast, for triads, arithmetic mean FC density was largely independent of synergistic structure (Pearson’s <italic toggle="yes">r</italic> = −0.05, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>), but for tetrads, they were strongly anticorrelated (Pearson’s <italic toggle="yes">r</italic> = −0.988, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). For visualization, see <xref rid="fig01" ref-type="fig">Fig. 1<italic toggle="yes">A</italic>–<italic toggle="yes">D</italic></xref>.</p>
        <fig position="float" id="fig01" fig-type="featured">
          <label>Fig. 1.</label>
          <caption>
            <p>The limits of bivariate functional connectivity. (<italic toggle="yes">A</italic>) In triads, bivariate functional connectivity is largely independent of synergistic structure (all correlations computed with the Pearson correlation coefficient) and (<italic toggle="yes">B</italic>) is very positively correlated with redundant structure. (<italic toggle="yes">C</italic>) In tetrads, bivariate functional connectivity is strongly negatively correlated with synergistic structure and (<italic toggle="yes">D</italic>) is strongly correlated with redundant structure. (<italic toggle="yes">E</italic> and <italic toggle="yes">F</italic>) Triads that have all elements within one FC community have significantly less synergistic structure than those that have elements with two communities, while for redundant structure, there was a clear pattern that the more FC communities a triad straddled, the lower its overall redundant structure. (<italic toggle="yes">G</italic> and <italic toggle="yes">H</italic>) The same pattern was even more pronounced in tetrads: As the number of FC communities a tetrad straddled increased, the expected synergistic structure climbed, while expected redundant structure fell.</p>
          </caption>
          <graphic xlink:href="pnas.2300888120fig01" position="float"/>
        </fig>
        <p>In addition to subgraph structure, another common method of assessing polyadic interactions in networks is via community detection (<xref rid="r8" ref-type="bibr">8</xref>). Using the multiresolution consensus clustering algorithm (<xref rid="r30" ref-type="bibr">30</xref>), we clustered the bivariate functional connectivity matrix into nonoverlapping communities. We then looked at the distributions of higher-order redundant and synergistic structure for triads and tetrads that spanned different numbers of consensus communities. We found that triads where all nodes were members of one community had significantly less synergy than triads that spanned two or three communities (Kolmogorov–Smirnov two sample test, <italic toggle="yes">D</italic> = 0.44, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). The pattern was more pronounced when considering tetrads: tetrads that all belonged to one community had lower synergy than those that spanned two communities (<italic toggle="yes">D</italic> = 0.45, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>), who in turn had lower synergy than those that spanned three communities (<italic toggle="yes">D</italic> = 0.37, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). In <xref rid="fig01" ref-type="fig">Fig. 1</xref> (<italic toggle="yes">Top</italic> row), we show cumulative probability density plots for the distribution of synergies for triads and tetrads that spanned one, two, three, and four FC communities, where it is clear that participation in increasingly diverse communities is associated with a greater synergistic structure. In contrast, a redundant structure was higher in triads that were all members of a small number of communities. Triads that spanned three communities had lower redundancy than triads that spanned two communities (<italic toggle="yes">D</italic> = 0.48, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>), which in turn had lower redundancy than those that were all members of one community (<italic toggle="yes">D</italic> = 0.47, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>) (<xref rid="fig01" ref-type="fig">Fig. 1 <italic toggle="yes">G</italic> and <italic toggle="yes">H</italic></xref>). These results, coupled with the mathematical analysis of the PED discussed in Section <xref rid="s1" ref-type="sec">1</xref>, provide strong theoretical and empirical evidence that bivariate, correlation-based FC measures are largely sensitive to redundant information duplicated over many individual brain regions but largely insensitive to (or even anticorrelated with) higher-order synergies involving the joint state of multiple regions. These results imply the possibility that there is a vast space of neural dynamics and structures that have not previously been captured in FC analyses.</p>
        <sec id="s12">
          <label>A.1.</label>
          <title>PED with 𝓗<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> is consistent with O-information.</title>
          <p>To test whether the PED using the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> redundancy function was consistent with other, information-theoretic measures of redundancy and synergy, we compared the average redundant and synergistic structures (as revealed by PED) to the O-information. We hypothesized that redundant structure would be positively correlated with O-information (as 𝒪 &gt; 0 implies redundancy dominance) and that synergistic structure would be negatively correlated, for the same reason.</p>
          <p>For both triads and tetrads, our hypothesis was bourne out. The Pearson correlation between O-information and redundant structure was significantly positive for both triads (Pearson’s <italic toggle="yes">r</italic> = 0.72, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>) and tetrads (Pearson’s <italic toggle="yes">r</italic> = 0.82, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). Conversely, the Pearson correlation between the O-information and the synergistic structure was significantly negative (triads: Pearson’s <italic toggle="yes">r</italic> = −0.7, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>, tetrads: Pearson’s <italic toggle="yes">r</italic> = −0.72, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). These results show that the structures revealed by the PED are consistent with other, nondecomposition-based inference methods and serve to validate the overall framework.</p>
          <p>Interestingly, when comparing the triadic O-information to the corrected triadic O-information (which does not double-count ℋ<sub>∂</sub><sup>123</sup>({1, 2}{1, 3}{2, 3}) and does not add back in the atom ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3})), we can see that the addition of ℋ<sub>∂</sub><sup>123</sup>({1, 2, 3}) can lead to erroneous conclusions. Of all those triads that had a negative corrected O-information (i.e., had a greater synergistic structure than redundant structure), 61.7% had a positive O-information, which could only be attributable to the presence of the triple-synergy being (mis)interpreted as redundancy and overwhelming the true difference. This suggests that, for small systems, the O-information may not provide an unbiased estimator of redundancy/synergy balance.</p>
        </sec>
      </sec>
      <sec id="s13">
        <label>B.</label>
        <title>Characterizing Higher-Order Brain Structures.</title>
        <p>Having established the presence of beyond-pairwise redundancies and synergies in brain data and shown that standard, network-based approaches show an incomplete picture of the overall architecture, we now describe the distribution of redundancies and synergies across the human brain.</p>
        <p>We began by applying a higher-order generalization of the standard community detection approach using a hypergraph modularity maximization algorithm (<xref rid="r31" ref-type="bibr">31</xref>). This algorithm partitions collections of (potentially overlapping) sets of nodes called hyperedges into communities that have a high degree of internal integration and a lower degree of between-community integration. We selected all those triads that had a greater synergistic structure than any of the one million maximum entropy null triads (<italic toggle="yes">Materials and Methods</italic>), which yielded a set of 3,746 unique triads. From these, we constructed an unweighted hypergraph with 200 nodes and 3,746 hyperedges (casting each triad as a hyperedge incident on three nodes). We then performed 1,000 trials of the hypergraph clustering algorithm proposed by Kumar et al. (<xref rid="r31" ref-type="bibr">31</xref>), from which we built a consensus matrix that tracked how frequently two brain regions <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> were assigned to the same hypercommunity. We repeated the process for the 3,746 maximally redundant triads to create two partitions: a synergistic structure and a redundant structure.</p>
        <p>In <xref rid="fig02" ref-type="fig">Fig. 2<italic toggle="yes">A</italic></xref>, we show surface plots of the resulting communities computed from the concatenated time series comprising all ninety-five subjects and all 4 runs. The redundant structure (<italic toggle="yes">Left</italic>) is very similar to the canonical seven Yeo systems (<xref rid="r32" ref-type="bibr">32</xref>): We can see a well-developed DMN (orange), a distinct visual system (sky blue), a somato-motor strip (violet), and a fronto-parietal network (dark blue). In contrast, when considering the synergistic structure (<italic toggle="yes">Right</italic>), a strikingly different pattern is apparent. When we computed the normalized mutual information of all the subject-level redundancy partitions to the canonical Yeo systems, we found a high degree of correlation (NMI = 0.6196 ± 0.0117, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). The same analysis with the subject-level synergy partitions found a much lower degree of concordance (NMI = 0.2290 ± 0.0117, <italic toggle="yes">P</italic> &lt; 10<sup>−20</sup>). See <xref rid="fig02" ref-type="fig">Fig. 2<italic toggle="yes">C</italic></xref> for visualization. Synergistic connectivity appears more lateralized over <italic toggle="yes">Left</italic> and <italic toggle="yes">Right</italic> hemispheres (orange and violet communities respectively), although there is a high degree of symmetry along the cortical midline composed of apparently novel communities. These include a synergistic coupling between visual and limbic regions (sky blue) as well as an occipital subset of the DMN (green) and a curious, symmetrical set of regions combining somato-motor and DMN regions (red). See <xref rid="fig02" ref-type="fig">Fig. 2<italic toggle="yes">D</italic></xref> for visualization.</p>
        <fig position="float" id="fig02" fig-type="figure">
          <label>Fig. 2.</label>
          <caption>
            <p>Redundant and synergistic hypergraph community structure. (<italic toggle="yes">A</italic> and <italic toggle="yes">B</italic>) Surface plots of the two communities structures: On the <italic toggle="yes">Left</italic> is the redundant structure and on the <italic toggle="yes">Right</italic> is the synergistic structure. We can see that both patterns are largely symmetrical for both information-sharing modes, although the synergistic structure has two large, lateralized communities. (<italic toggle="yes">C</italic> and <italic toggle="yes">D</italic>) The coclassification matrices for redundant structure (<italic toggle="yes">Left</italic>) and the synergistic structure (<italic toggle="yes">Right</italic>). The higher the value of a pair, the more frequently the hypergraph modularity maximization (<xref rid="r31" ref-type="bibr">31</xref>) assigns those two regions to the same hypercommunity. The yellow squares indicate the seven canonical Yeo functional networks (<xref rid="r32" ref-type="bibr">32</xref>), and we can see that the higher-order redundant structure matches the bivariate Yeo systems well (despite consisting of information shared redundantly across three nodes). In contrast, the synergistic structure largely fails to match the canonical network structure at all. (<italic toggle="yes">E</italic>) For each of the 95 subjects and for each of the 1,000 permutation nulls used to significance test the NMI between subject-level community structure and the master level structure, we computed the log-ratio of the empirical NMI to the null NMI. For redundancy, there was not a single null, over any subject, that was greater than the associated empirical NMI. For the case of the synergy, only 0.6% of nulls were greater than their associated empirical NMI.</p>
          </caption>
          <graphic xlink:href="pnas.2300888120fig02" position="float"/>
        </fig>
        <p>These results show two things: The first is further confirmation that the canonical structures studied in an FC framework can be interpreted as reflecting primarily patterns of redundant information. The second is that higher-order synergies are structured in nonrandom ways, combining multiple brain regions into integrated systems that are usually thought to be independent when considering just correlation-based analyses. If the synergistic structure were reflecting mere noise, then we would not expect the high degree of symmetry and structure we observe.</p>
        <p>To test whether the patterns we observed were consistent across individuals, we reran the entire pipeline (PED of all triads, hypergraph clustering of redundant and synergistic triads, etc) for each of the 95 subjects separately. Then, for each subject, we computed the normalized mutual information (NMI) (<xref rid="r6" ref-type="bibr">6</xref>) between the subject-level partition and the relevant master partition (redundancy or synergy) created from the concatenated time series of all four scans from each of the ninety-five subjects. We significance-tested each comparison with a permutation null model. For each null, we permuted the subject-level community assignment vector of nodes, recomputing the NMI between the master partition and a shuffled subject-level partition (1,000 permutations). In the case of the redundant partition, we found that no subjects ever had a shuffled null that was greater than the empirical NMI: All had significant NMI (0.52 ± 0.07). In the case of the synergistic partition, 91 of the 95 subjects showed significant NMI (0.1 ± 0.03, <italic toggle="yes">P</italic> &lt; 0.05, Benjamini–Hochberg FDR corrected). These results (visualized in <xref rid="fig02" ref-type="fig">Fig. 2<italic toggle="yes">E</italic></xref>) suggest that both structures (redundant and synergistic) are broadly conserved across individuals; however, it appears that the synergistic partitions are generally more variable between subjects than the redundant partition (which hews closer to the master partition constructed by combining the data from all subjects).</p>
        <sec id="s14">
          <label>B.1.</label>
          <title>Redundancy-synergy gradient &amp; time-resolved analysis.</title>
          <p>Thus far, we have analyzed higher-order redundancy and synergy separately. To understand how they interact, we began by replicating the analysis of Luppi et al. (<xref rid="r33" ref-type="bibr">33</xref>). We counted how many times each brain region appeared in the set of 3,746 most synergistic and 3,746 most redundant triads. We then ranked each node to create two vectors which rank how frequently each region participates in high-redundancy and high-synergy configurations. By subtracting those two rank vectors, we get a measure of relative redundancy/synergy dominance. A value greater than zero indicates that a region’s relative redundancy (compared to all other regions) is greater than its relative synergy (compared to all other regions), and vice versa.</p>
          <p>By projecting the rank-differences onto the cortical surface (<xref rid="fig03" ref-type="fig">Fig. 3<italic toggle="yes">A</italic></xref>), we recover the same gradient-like pattern first reported by Luppi et al., with relatively redundant regions located in the primary sensory and motor cortex and relatively synergistic regions located in the multimodal and executive cortex. This replication is noteworthy, as Luppi et al., used an entirely different method of computing synergy (based on the information flow from past to future in pairs of brain regions), while we are looking at generalizations of static FC for which dynamic order does not matter. The fact that the same gradient appears when using both analytical methods strongly suggests that it is a robust feature of brain activity.</p>
          <fig position="float" id="fig03" fig-type="figure">
            <label>Fig. 3.</label>
            <caption>
              <p>Time-resolved analysis. (<italic toggle="yes">A</italic>) Surface plots for the distributions of relative synergies and relative redundancies across the human brain. These results match prior work by Luppi et al. (<xref rid="r33" ref-type="bibr">33</xref>), with the primary sensory and motor cortex being relatively redundant, while multimodal association areas being relatively synergistic. (<italic toggle="yes">B</italic>) Over the course of one subject’s scan (1100 TRs), the total redundant and synergistic structure varies over time, although never so much that the curves cross (i.e., there is never more redundant structure than synergistic structure present). (<italic toggle="yes">C</italic>) Instantaneous redundant and synergistic structures are anticorrelated (<italic toggle="yes">r</italic> = −0.83, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>). (<italic toggle="yes">D</italic>) Redundancy is positively correlated with the amplitude of bivariate cofluctuations (Pearson’s Pearson’s <italic toggle="yes">r</italic> = 0.6, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>), and (<italic toggle="yes">E</italic>) synergy is negatively correlated with cofluctuation amplitude (Pearson’s <italic toggle="yes">r</italic> = −0.43, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>). (<italic toggle="yes">F</italic>) For each TR, we show the difference in the rank-redundancy and rank-synergy for each node (red indicates a higher rank-redundancy than rank-synergy and vice versa for blue). When nodes are stratified by Yeo system (<xref rid="r32" ref-type="bibr">32</xref>) (gray, horizontal lines), it is clear that different systems alternate between high-redundancy and high-synergy configurations in different ways. (<italic toggle="yes">G</italic>) For every pair of columns in Panel F. we compute the Pearson correlation between them to construct a time × time similarity matrix, which we then clustered using the MRCC algorithm (<xref rid="r30" ref-type="bibr">30</xref>). Note that rows and columns are not in time order, but rather, reordered to reveal the state-structure of the time series. (<italic toggle="yes">H</italic>) Five example states (centroids of each community shown in Panel <italic toggle="yes">G</italic>) projected onto the cortical surface. It is clear that the instantaneous pattern of relative synergies and redundancies varies from the average structure presented in Panel <italic toggle="yes">A</italic>. For example, in states 3 and 4, the visual system is highly redundant (as in the average); however, in state 5, the visual system is synergistic.</p>
            </caption>
            <graphic xlink:href="pnas.2300888120fig03" position="float"/>
          </fig>
          <p>A limitation of the analysis by Luppi et al. is the restriction that only average values of synergy and redundancy are accessible: The results describe expected values over all TRs and obscure any local variability. The PED analysis using <italic toggle="yes">h</italic><sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> can be localized (Section <xref rid="s1" ref-type="sec">1</xref>) to individual frames. This allows us to see how the redundant and synergistic structures fluctuate over the course of a resting-state scan and how the distributions of relative synergies and redundancies vary over the cortex. <xref rid="fig03" ref-type="fig">Fig. 3<italic toggle="yes">B</italic></xref> shows how the redundant and synergistic structure fluctuate over the course of 1100 TRs taken from a single subject (for scans concatenated). This allows us to probe the information structure of previously identified patterns in-frame-wise dynamics. Analysis of instantaneous pairwise cofluctuations (also called edge time series) reveals a highly structured pattern, with periods of relative disintegration interspersed with high cofluctuation events (<xref rid="r34" ref-type="bibr">34</xref>, <xref rid="r35" ref-type="bibr">35</xref>). The distribution of these cofluctuations reflect various factors of cognition (<xref rid="r36" ref-type="bibr">36</xref>), generative structure (<xref rid="r37" ref-type="bibr">37</xref>), functional network organization (<xref rid="r28" ref-type="bibr">28</xref>), and individual differences (<xref rid="r38" ref-type="bibr">38</xref>). By correlating the instantaneous average whole-brain redundant and synergistic structures with instantaneous whole-brain cofluctuation amplitude (RSS), we can get an understanding of the informational structure of high-RSS events (<xref rid="fig03" ref-type="fig">Fig. 3 <italic toggle="yes">C</italic> and <italic toggle="yes">E</italic></xref>). We found that redundancy is positively correlated with cofluctuation RSS (Pearson’s <italic toggle="yes">r</italic> = 0.6, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>) and synergy is negatively correlated with cofluctuation amplitude (Pearson’s <italic toggle="yes">r</italic> = −0.43, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>). Given that synergy is known to drive bivariate functional connectivity (<xref rid="r34" ref-type="bibr">34</xref>), this is again consistent with the hypothesis that FC patterns largely reflect redundancy and are insensitive to higher-order synergies.</p>
          <p>With full PED analysis completed for every frame, it is possible to compute the instantaneous distribution of relative redundancies and synergies across the cortex for every TR. The resulting multidimensional time-series can be seen in <xref rid="fig03" ref-type="fig">Fig. 3<italic toggle="yes">F</italic></xref>. When sorted by Yeo systems (<xref rid="r32" ref-type="bibr">32</xref>), we can see that different systems show distinct relative redundancy/synergy profiles. The nodes in the somato-motor system had the highest median value (22.0 ± 73), followed by the visual system (14.0 ± 80), indicating that they were on-average relatively more redundant than synergistic. In contrast, the ventral attentional system had the lowest median value (−11.0 ± 66), indicating a relatively synergistic dynamic. Other systems seemed largely balanced: with median values near zero but a wide spread between them, such as the dorsal attention network (1.0 ± 70), fronto-parietal control system (−5.0 ± 56), and the DMN (−2.0 ± 67). These are systems that transiently shift from largely redundancy-dominated to synergy-dominated regimes in equal measure. Finally, the limbic system had small values and relatively little spread (−5.0 ± 18), indicating a system that never achieved either extreme.</p>
          <p>We then correlated every TR against every other frame to construct a weighted, signed recurrence network (<xref rid="r39" ref-type="bibr">39</xref>), which we could then cluster using the MRCC algorithm (<xref rid="r30" ref-type="bibr">30</xref>) (<xref rid="fig03" ref-type="fig">Fig. 3<italic toggle="yes">G</italic></xref>). This allowed us to assign every TR to one of nine discrete states, each of which can be represented by its centroid (for five examples see <xref rid="fig03" ref-type="fig">Fig. 3<italic toggle="yes">H</italic></xref>). We can see that these states are generally symmetrical but show markedly different patterns relative redundancy and synergy across the cortex, and some systems can change valance entirely. For example, in states three and four, the visual system is highly redundant (consistent with the average behavior), while in state five, the same regions are more synergy dominated. In the same vein, the somato-motor strip is highly redundant in state 4, but slightly synergy-biased in state 3. This shows that the dynamics of information processing are variable in time, with different areas of the cortex transiently becoming more redundant or more synergistic in concert.</p>
          <p>The sequence of states occupied at each TR is a discrete time series which we can analyze as a finite-state machine (for visualization, see <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic> Fig. S1</ext-link>). Shannon temporal mutual information found that the present state was significantly predictive of the future state (1.59 bit, <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>) and that the transitions between states were generally more deterministic (<xref rid="r40" ref-type="bibr">40</xref>) (2.29 bit <italic toggle="yes">P</italic> &lt; 10<sup>−50</sup>) than would be expected by chance. While the sample size is small (1,099 transitions), these results suggest that the transition between states is structured in nonrandom ways.</p>
        </sec>
      </sec>
    </sec>
    <sec sec-type="discussion" id="s15">
      <label>3.</label>
      <title>Discussion</title>
      <p content-type="flushleft">In this paper, we have explored a framework for extracting higher-order dependencies from data and applied it to fMRI recordings. We found that the human brain is rich in beyond-pairwise, synergistic structures, as well as redundant information copied over many brain regions. The PED-based approach provides two complementary approaches to assessing higher-order interactions in complex systems. The first approach is analytic (Section <xref rid="s5" ref-type="sec">A.2</xref>) and reveals how higher-order dependencies contribute to (and complicate) bivariate correlations between elements of a complex systems. Prior work on the PED has analytically shown that the bivariate mutual information between two elements incorporates nonlocal information that is redundantly present over more than two elements (<xref rid="r15" ref-type="bibr">15</xref>, <xref rid="r20" ref-type="bibr">20</xref>). This means that classic approaches to functional connectivity are nonspecific: The link between two elements does not reflect information uniquely shared by those two but double (or triple-counts) higher-order redundancies distributed over the system. We verified this empirically by comparing the distribution of higher-order (beyond pairwise) redundancies to a bivariate correlation network and found that the redundancies closely matched the classic network structure.</p>
      <p>These nonlocal redundancies shed light on a well-documented feature of bivariate functional connectivity networks: the transitivity of correlation (<xref rid="r41" ref-type="bibr">41</xref>). In functional connectivity networks, if <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> are correlated, as well as <italic toggle="yes">X</italic><sub><italic toggle="yes">j</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">k</italic></sub>, then there is a much higher than expected chance that <italic toggle="yes">X</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">X</italic><sub><italic toggle="yes">k</italic></sub> are correlated even though this is not theoretically necessary (<xref rid="r42" ref-type="bibr">42</xref>). Since the Pearson correlation related the mutual information under Gaussian assumptions (<xref rid="r21" ref-type="bibr">21</xref>), we claim that the observed transitivity of functional connectivity is a consequence of previously unrecognized, nonlocal redundancies copied over ensembles of nodes. This hypothesis is consistent with our findings that redundancies correlate with key features of functional network topology, including subgraph density and community structure.</p>
      <p>The second approach to PED analysis is empirical (Section <xref rid="s8" ref-type="sec">B</xref>). This approach requires operationalizing a definition of redundant entropy that can be used to estimate the values of the redundant and synergistic structures in bits. Here, we use the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> measure (<xref rid="r17" ref-type="bibr">17</xref>), which defines the entropy shared by a set of elements as the information that could be learned by observing any element alone. When analyzing resting state, fMRI data from ninety five individuals, we found strong evidence of higher-order synergies: information present in the joint states of multiple brain regions and only accessible when considering wholes rather than just parts. These synergies appear to be structured in part by the physical brain (for example, being largely symmetric across hemispheres) but also do not readily correspond to the standard functional connectivity networks previously explored in the literature. Since synergistic structures appear to be largely anticorrelated with the standard bivariate network structures, it is plausible that these synergistic systems represent an organization of human brain activity.</p>
      <p>These higher-order interactions represent a vast space of largely unexplored but potentially significant aspects of brain activity. The finding that the synergistic community structure was more variable across subjects than the redundant structure suggests that synergistic dependencies may reflect more unique, individualized differences, while the redundant structure (reflected in the functional connectivity) represents a more conserved architecture. The ability to expand beyond pairwise network models of the brain into the much richer space of beyond-pairwise structures offers the opportunity to explore previously inaccessible relationships between brain activity, cognition, and behavior.</p>
      <p>Since normal cognitive functioning requires the coordination of many different brain regions (<xref rid="r43" ref-type="bibr">43</xref><xref rid="r44" ref-type="bibr"/>–<xref rid="r45" ref-type="bibr">45</xref>), and pathological states are associated with disintegrated dynamics (<xref rid="r46" ref-type="bibr">46</xref><xref rid="r47" ref-type="bibr"/>–<xref rid="r48" ref-type="bibr">48</xref>), it is reasonable to assume that alterations to higher-order, synergistic coordination may also reflect clinically significant changes in cognition and health. Recent work has already indicated that changes in bivariate synergy track loss of consciousness under anesthesia and following traumatic and anoxic brain injury (<xref rid="r11" ref-type="bibr">11</xref>) suggesting that higher-order dependencies can encode clinically significant biomarkers. We hypothesize that beyond-pairwise synergies in particular may be worth exploring in the context of recognizing early signs of Alzheimer’s and other neurodegenerative diseases, as synergy requires the coordination of many regions simultaneously and may begin to show signs of fragmentation earlier than standard, functional connectivity-based patterns (which are dominated by nonlocal redundancies may obscure early fragmentation of the system).</p>
      <p>Finally, the localizable nature of the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> partial entropy function allows us a high degree of temporal precision when analyzing brain dynamics. The standard approach to time-varying connectivity is using a sliding-windows analysis; however, this approach blurs temporal features and obscures higher-frequency events (<xref rid="r49" ref-type="bibr">49</xref>). By being able to localize the redundancies and synergies in time, we can see that there is a complex interplay between both types of integration. When considering expected values, we find a distribution of redundancies and synergies that replicates the findings of Luppi et al. (<xref rid="r33" ref-type="bibr">33</xref>); however, when we localize the analysis in time, we find a high degree of variability between frames. It appears that there are not consistently redundant or synergistic brain regions (or ensembles), but rather, various brain regions can transiently participate in highly synergistic or highly redundant behaviors at different times. The structure of these dynamics appears to be nonrandom (based on the structure of the state-transition matrix); however, the significance of the various combinations of redundancy and synergy remains a topic for much future work. The fact that some systems (such as the visual system) can be either redundancy- or synergy-dominated at different times complicates the notion of a synergistic core. Instead, there may be a synergistic landscape of configurations that the system traverses, with different configurations of brain regions transiently serving as the core and providing a flexible architecture for neural computation in response to different demands.</p>
      <p>So far, all of the empirical results that we have discussed have hinged on very particular definitions of redundancy and synergy, which come from the underlying ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> measure and a different measure of redundant entropy such as ℋ<sub><italic toggle="yes">c</italic><italic toggle="yes">c</italic><italic toggle="yes">s</italic></sub> (<xref rid="r15" ref-type="bibr">15</xref>) or ℋ<sub><italic toggle="yes">m</italic><italic toggle="yes">i</italic><italic toggle="yes">n</italic></sub> (<xref rid="r20" ref-type="bibr">20</xref>) will bring with it different interpretations, and possibly different results. As future definitions of redundant entropy are inevitably developed, it will be of interest to see how the apparent distribution of redundancies and synergies in the brain changes. Different definitions of redundancy may produce different kinds of synergies, with their own distributions across the cortex. While this has been described as a fault with the information decomposition framework, we feel that it may be as much a feature as a bug: different definitions of redundancy and synergy may reveal different facets of structure in complex systems. Analogy may be made to the many different formal definitions of “complexity” that have been proposed over the years. There may not be a single, universally satisfying definition of complexity that is appropriate in all cases: Instead, different measures are understood to reveal different aspects of dynamics that may be more or less useful in particular circumstances for discussion, see refs. <xref rid="r50" ref-type="bibr">50</xref> and <xref rid="r51" ref-type="bibr">51</xref>. Here, we make an argument for “pragmatic pluralism” (<xref rid="r52" ref-type="bibr">52</xref>): many different approaches may together reveal aspects of the brain that single approaches can not.</p>
      <p>This analysis does have some limitations, however. The most significant is that the size of the partial entropy lattice grows explosively as the size of the system increases: A system with only eight elements will have a lattice with 5.6 × 10<sup>22</sup> unique partial entropy atoms. While our aggregated measures of redundant and synergistic structure can summarize the dependencies in a principled way, simply computing that many atoms is computationally prohibitive. In this paper, we took a large system of 200 nodes and calculated every triad and a large number of tetrads; however, this also quickly runs into combinatorial difficulties, as the number of possible groups of size <italic toggle="yes">k</italic> one can make from <italic toggle="yes">N</italic> elements grows with the binomial coefficient. Heuristic measures such as the O-information can help, although as we have seen, this measure can conflate redundancy and synergy in sometimes surprising ways. One possible avenue of future work could be to leverage optimization algorithms to find small, tractable subsets of systems that show interesting redundant or synergistic structure, as was done in refs. <xref rid="r53" ref-type="bibr">53</xref>, <xref rid="r54" ref-type="bibr">54</xref> and <xref rid="r16" ref-type="bibr">16</xref>. Alternately, coarse-graining approaches that can reduce the dimensionality of the system while preserving the informational or causal structure may allow the analysis of a compressed version of the system small enough to be tractable (<xref rid="r40" ref-type="bibr">40</xref>, <xref rid="r55" ref-type="bibr">55</xref>).</p>
      <p>The choice of hypergraph community detection method is also an area requiring further consideration. The hypermodularity maximization approach from Kumar et al. (<xref rid="r31" ref-type="bibr">31</xref>) has many similarities to community-detection approaches that are common in network neuroscience. However, it also inherits some of modularity’s limitations, including the issue of the resolution limit (<xref rid="r8" ref-type="bibr">8</xref>) and the lack of significance testing of partitions. Future work may focus on how different definitions of hypergraph communities may change these results (e.g., ref. <xref rid="r56" ref-type="bibr">56</xref> recently introduced a framework that allows for overlapping communities). Alternately, one could propose a simplicial complex-based approach, as in ref. <xref rid="r57" ref-type="bibr">57</xref>. The study of higher-order information in complex systems is still developing, with many avenues and possibilities to be explored.</p>
      <p>In the context of this study, the use of fMRI BOLD data presents some inherent limitations, such as a small number of samples (TRs) from which to infer probability distributions, and the necessity of binarizing a slow, continuous signal. Generalizing the logic of shared probability mass exclusions remains an area of on-going work (<xref rid="r58" ref-type="bibr">58</xref>), although for the time being, the ℋ<sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub> function requires discrete random variables. BOLD itself is also fundamentally a proxy measure of brain activity based on oxygenated blood flow and not a direct measure of neural activity. Applying this work to electrophysiological data M/EEG, which can be discretized in principled ways to enable information-theoretic analysis (<xref rid="r59" ref-type="bibr">59</xref>), and naturally discrete spiking neural data (<xref rid="r60" ref-type="bibr">60</xref>, <xref rid="r61" ref-type="bibr">61</xref>), will help deepen our understanding of how higher-order interactions contribute to cognition and behavior. The applicability of the PED to multiple scales of analysis highlights one of the foundational strengths of the approach (and information-theoretic frameworks more broadly): being based on the fundamental logic of inferences under conditions of uncertainty, the PED can be applied to a large number of complex systems (beyond just the brain), or to multiple scales within a single system to provide a detailed, and holistic picture of the system’s structure.</p>
    </sec>
    <sec sec-type="conclusions" id="s16">
      <label>4.</label>
      <title>Conclusions</title>
      <p content-type="flushleft">In this work, we have shown how the joint entropy of a complex system can be decomposed into atomic components of redundancy and synergy, revealing higher-order dependencies in the structure of the system. When applied to human brain data, this PED framework reveals previously unrecognized, higher-order structures in the human brain. We find that the well-known patterns of functional connectivity networks largely reflect redundant information copied over many brain regions. In contrast, the synergies for a kind of “shadow structure” that is largely independent from, or anticorrelated with, the bivariate network and has consequently remained less well explored. The patterns of redundancy and synergy over the cortex are dynamic across time, with different ensembles of brain regions transiently forming redundancy- or synergy-dominated structures. This space of beyond-pairwise dynamics is likely rich in previously unidentified links between brain activity and cognition. The PED can also be applied to problems beyond neuroscience and may provide a general tool with which higher-order structure can be studied in any complex system.</p>
    </sec>
    <sec sec-type="materials|methods" id="s17">
      <label>5.</label>
      <title>Materials and Methods</title>
      <p content-type="flushleft">Neural activity was recorded from adult human subjects using resting-state fMRI (<xref rid="r27" ref-type="bibr">27</xref>). Preprocessing has been previously described in ref. <xref rid="r37" ref-type="bibr">37</xref>. Further details are available in <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic>, Appendix 4A</ext-link>. Statistical analyses of triads and tetrads are available in <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic>, Appendix 4B.1-3</ext-link>. Details of community detection of the bivariate FC matrix can be found in <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new"><italic toggle="yes">SI Appendix</italic>, Appendix 4B.4</ext-link>. All subjects gave informed consent to protocols approved by the Washington University Institutional Review Board.</p>
    </sec>
    <sec sec-type="supplementary-material">
      <title>Supplementary Material</title>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Appendix 01 (PDF)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sapp.pdf">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S01 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd01.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S02 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd02.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S03 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd03.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S04 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd04.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S05 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd05.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S06 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd06.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S07 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd07.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S08 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd08.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S09 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd09.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S10 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd10.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S11 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd11.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S12 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd12.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S13 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd13.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S14 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd14.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S15 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd15.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S16 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd16.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S17 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd17.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S18 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd18.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S19 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd19.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S20 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd20.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S21 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd21.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S22 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd22.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S23 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd23.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S24 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd24.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S25 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd25.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S26 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd26.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S27 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd27.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S28 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd28.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S29 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd29.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Dataset S30 (CSV)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd30.csv">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Code S1 (TXT)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd31.txt">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Code S2 (TXT)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd32.txt">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Code S3 (TXT)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd33.txt">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material position="float" content-type="local-data">
        <caption>
          <p>Code S4 (TXT)</p>
        </caption>
        <media xlink:href="pnas.2300888120.sd34.txt">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Dr. Caio Seguin for valuable discussion throughout the process. T.F.V. would also like to thank Dr. Robin Ince and Dr. Abdullah Makkeh for valuable discussions around the topics of PED and <italic toggle="yes">I</italic><sub><italic toggle="yes">s</italic><italic toggle="yes">x</italic></sub>. T.F.V. and M.P. are supported by the NSF-NRT grant 1735095, Interdisciplinary Training in Complex Networks and Systems. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
      <sec id="s19">
        <title>Author contributions</title>
        <p>T.F.V. and O.S. designed research; T.F.V., M.P., and M.G.P. performed research; M.G.P. data visualization; J.F. preprocessed fMRI data; O.S. supervised project, edited manuscript; T.F.V., M.P., M.G.P., and J.F. analyzed data; and T.F.V. wrote the paper.</p>
      </sec>
      <sec sec-type="COI-statement" id="s20">
        <title>Competing interests</title>
        <p>The authors declare no competing interest.</p>
      </sec>
    </ack>
    <fn-group>
      <fn fn-type="other" id="fn2">
        <p>This article is a PNAS Direct Submission.</p>
      </fn>
    </fn-group>
    <sec sec-type="data-availability" id="s18">
      <title>Data, Materials, and Software Availability</title>
      <p>All study data are included in the article and/or <ext-link xlink:href="https://www.pnas.org/lookup/doi/10.1073/pnas.2300888120#supplementary-materials" ext-link-type="uri" xlink:show="new">supporting information</ext-link>. Previously published data were used for this work (<ext-link xlink:href="https://doi.org/10.1073/pnas.2109380118" ext-link-type="uri">https://doi.org/10.1073/pnas.2109380118</ext-link>) (<xref rid="r37" ref-type="bibr">37</xref>).</p>
    </sec>
    <ref-list>
      <ref id="r1">
        <label>1</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>G.</given-names>
<surname>Tononi</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Kötter</surname></string-name></person-group>, <article-title>The human connectome: A structural description of the human brain</article-title>. <source>PLoS Comput. Biol.</source>
<volume><bold>1</bold></volume>, <fpage>e42</fpage> (<year>2005</year>).<pub-id pub-id-type="pmid">16201007</pub-id></mixed-citation>
      </ref>
      <ref id="r2">
        <label>2</label>
        <mixed-citation publication-type="other">O. Sporns, <italic toggle="yes">Networks of the Brain</italic> (MIT Press, 2010).</mixed-citation>
      </ref>
      <ref id="r3">
        <label>3</label>
        <mixed-citation publication-type="other">A. Fornito, A. Zalesky, E. Bullmore, <italic toggle="yes">Fundamentals of Brain Network Analysis</italic> (Elsevier, 2016).</mixed-citation>
      </ref>
      <ref id="r4">
        <label>4</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>K. J.</given-names>
<surname>Friston</surname></string-name></person-group>, <article-title>Functional and effective connectivity in neuroimaging: A synthesis</article-title>. <source>Hum. Brain Mapp.</source>
<volume><bold>2</bold></volume>, <fpage>56</fpage>–<lpage>78</lpage> (<year>1994</year>).</mixed-citation>
      </ref>
      <ref id="r5">
        <label>5</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M. D.</given-names>
<surname>Fox</surname></string-name>
<etal/></person-group>, <article-title>The human brain is intrinsically organized into dynamic, anticorrelated functional networks</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>102</bold></volume>, <fpage>9673</fpage>–<lpage>9678</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15976020</pub-id></mixed-citation>
      </ref>
      <ref id="r6">
        <label>6</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Rubinov</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name></person-group>, <article-title>Complex network measures of brain connectivity: Uses and interpretations</article-title>. <source>NeuroImage</source>
<volume><bold>52</bold></volume>, <fpage>1059</fpage>–<lpage>1069</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">19819337</pub-id></mixed-citation>
      </ref>
      <ref id="r7">
        <label>7</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>R.</given-names>
<surname>Kötter</surname></string-name></person-group>, <article-title>Motifs in brain networks</article-title>. <source>PLOS Biol.</source>
<volume><bold>2</bold></volume>, <elocation-id>e369</elocation-id>. (<year>2004</year>).<pub-id pub-id-type="pmid">15510229</pub-id></mixed-citation>
      </ref>
      <ref id="r8">
        <label>8</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names>
<surname>Fortunato</surname></string-name></person-group>, <article-title>Community detection in graphs</article-title>. <source>Phys. Rep.</source>
<volume><bold>486</bold></volume>, <fpage>75</fpage>–<lpage>174</lpage> (<year>2010</year>).</mixed-citation>
      </ref>
      <ref id="r9">
        <label>9</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names>
<surname>Battiston</surname></string-name>
<etal/></person-group>, <article-title>The physics of higher-order interactions in complex systems</article-title>. <source>Nat. Phys.</source>
<volume><bold>17</bold></volume>, <fpage>1093</fpage>–<lpage>1098</lpage> (<year>2021</year>).</mixed-citation>
      </ref>
      <ref id="r10">
        <label>10</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F. E.</given-names>
<surname>Rosas</surname></string-name>
<etal/></person-group>, <article-title>Disentangling high-order mechanisms and high-order behaviours in complex systems</article-title>. <source>Nat. Phys.</source>
<volume><bold>18</bold></volume>, <fpage>476</fpage>–<lpage>477</lpage> (<year>2022</year>).</mixed-citation>
      </ref>
      <ref id="r11">
        <label>11</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>A. I.</given-names>
<surname>Luppi</surname></string-name>
<etal/></person-group>, <article-title>A synergistic workspace for human consciousness revealed by integrated information decomposition</article-title>. bioRxiv [Preprint] (<year>2020</year>). <ext-link xlink:href="https://www.biorxiv.org/content/10.1101/2020.11.25.398081v3.full" ext-link-type="uri" xlink:show="new">https://www.biorxiv.org/content/10.1101/2020.11.25.398081v3.full</ext-link> (Accessed 28 March 2023).</mixed-citation>
      </ref>
      <ref id="r12">
        <label>12</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Gatica</surname></string-name>
<etal/></person-group>, <article-title>High-order interdependencies in the aging brain</article-title>. <source>Brain Connect.</source>
<volume><bold>11</bold></volume>, <fpage>734</fpage>–<lpage>744</lpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33858199</pub-id></mixed-citation>
      </ref>
      <ref id="r13">
        <label>13</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>P. L.</given-names>
<surname>Williams</surname></string-name>, <string-name><given-names>R. D.</given-names>
<surname>Beer</surname></string-name></person-group>, <article-title>Nonnegative decomposition of multivariate information</article-title>. arXiv [Preprint] (<year>2010</year>). <ext-link xlink:href="http://arxiv.org/abs/1004.2515" ext-link-type="uri" xlink:show="new">http://arxiv.org/abs/1004.2515</ext-link> [math-ph, physics:physics, q-bio].</mixed-citation>
      </ref>
      <ref id="r14">
        <label>14</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. J.</given-names>
<surname>Gutknecht</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Wibral</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Makkeh</surname></string-name></person-group>, <article-title>Bits and pieces: Understanding information decomposition from part-whole relationships and formal logic</article-title>. <source>Proc. R. Soc. A: Math. Phys. Eng. Sci.</source>
<volume><bold>477</bold></volume>, <fpage>20210110</fpage> (<year>2021</year>).</mixed-citation>
      </ref>
      <ref id="r15">
        <label>15</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>R. A. A.</given-names>
<surname>Ince</surname></string-name></person-group>, <article-title>The partial entropy decomposition: Decomposing multivariate entropy and mutual information via pointwise common surprisal</article-title>. arXiv [Preprint] (<year>2017</year>). <ext-link xlink:href="http://arxiv.org/abs/1702.01591" ext-link-type="uri" xlink:show="new">http://arxiv.org/abs/1702.01591</ext-link> [cs, math, q-bio, stat] (Accessed 14 March 2021).</mixed-citation>
      </ref>
      <ref id="r16">
        <label>16</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Pope</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Faskowitz</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name></person-group>, <article-title>Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex</article-title>. <source>Commun. Biol.</source>
<volume><bold>6</bold></volume>, <fpage>451</fpage> (<year>2023</year>).<pub-id pub-id-type="pmid">37095282</pub-id></mixed-citation>
      </ref>
      <ref id="r17">
        <label>17</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Makkeh</surname></string-name>, <string-name><given-names>A. J.</given-names>
<surname>Gutknecht</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Wibral</surname></string-name></person-group>, <article-title>Introducing a differentiable measure of pointwise shared information</article-title>. <source>Phys. Rev. E</source>
<volume><bold>103</bold></volume>, <fpage>032149</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">33862718</pub-id></mixed-citation>
      </ref>
      <ref id="r18">
        <label>18</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Kolchinsky</surname></string-name></person-group>, <article-title>A novel approach to the partial information decomposition</article-title>. <source>Entropy</source>
<volume><bold>24</bold></volume>, <fpage>403</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35327914</pub-id></mixed-citation>
      </ref>
      <ref id="r19">
        <label>19</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names>
<surname>Rosas</surname></string-name>, <string-name><given-names>P. A. M.</given-names>
<surname>Mediano</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Gastpar</surname></string-name>, <string-name><given-names>H. J.</given-names>
<surname>Jensen</surname></string-name></person-group>, <article-title>Quantifying high-order interdependencies via multivariate extensions of the mutual information</article-title>. <source>Phys. Rev. E</source>
<volume><bold>100</bold></volume>, <fpage>032305</fpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31640038</pub-id></mixed-citation>
      </ref>
      <ref id="r20">
        <label>20</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>C.</given-names>
<surname>Finn</surname></string-name>, <string-name><given-names>J. T.</given-names>
<surname>Lizier</surname></string-name></person-group>, <article-title>Generalised measures of multivariate information content</article-title>. <source>Entropy</source>
<volume><bold>22</bold></volume>, <fpage>216</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">33285991</pub-id></mixed-citation>
      </ref>
      <ref id="r21">
        <label>21</label>
        <mixed-citation publication-type="other">T. M. Cover, J. A. Thomas, <italic toggle="yes">Elements of Information Theory</italic> (John Wiley&amp; Sons, 2012)</mixed-citation>
      </ref>
      <ref id="r22">
        <label>22</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O. M.</given-names>
<surname>Cliff</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Novelli</surname></string-name>, <string-name><given-names>B. D.</given-names>
<surname>Fulcher</surname></string-name>, <string-name><given-names>J. M.</given-names>
<surname>Shine</surname></string-name>, <string-name><given-names>J. T.</given-names>
<surname>Lizier</surname></string-name></person-group>, <article-title>Assessing the significance of directed and multivariate measures of linear dependence between time series</article-title>. <source>Phys. Rev. Appl.</source>
<volume><bold>3</bold></volume>, <fpage>013145</fpage> (<year>2021</year>).</mixed-citation>
      </ref>
      <ref id="r23">
        <label>23</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S.</given-names>
<surname>Watanabe</surname></string-name></person-group>, <article-title>Information theoretical analysis of multivariate correlation</article-title>. <source>IBM J. Res. Dev.</source>
<volume><bold>4</bold></volume>, <fpage>66</fpage>–<lpage>82</lpage> (<year>1960</year>).</mixed-citation>
      </ref>
      <ref id="r24">
        <label>24</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>S. A.</given-names>
<surname>Abdallah</surname></string-name>, <string-name><given-names>M. D.</given-names>
<surname>Plumbley</surname></string-name></person-group>, <article-title>A measure of statistical complexity based on predictive information with application to finite spin systems</article-title>. <source>Phys. Lett. A</source>
<volume><bold>376</bold></volume>, <fpage>275</fpage>–<lpage>281</lpage> (<year>2012</year>).</mixed-citation>
      </ref>
      <ref id="r25">
        <label>25</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. G.</given-names>
<surname>James</surname></string-name>, <string-name><given-names>C. J.</given-names>
<surname>Ellison</surname></string-name>, <string-name><given-names>J. P.</given-names>
<surname>Crutchfield</surname></string-name></person-group>, <article-title>Anatomy of a bit: Information in a time series observation</article-title>. <source>Chaos: Interdiscip. J. Nonlinear Sci</source>. <volume><bold>21</bold></volume>, <fpage>037109</fpage> (<year>2011</year>).</mixed-citation>
      </ref>
      <ref id="r26">
        <label>26</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>G.</given-names>
<surname>Tononi</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>G. M.</given-names>
<surname>Edelman</surname></string-name></person-group>, <article-title>A measure for brain complexity: Relating functional segregation and integration in the nervous system</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>91</bold></volume>, <fpage>5033</fpage>–<lpage>5037</lpage> (<year>1994</year>).<pub-id pub-id-type="pmid">8197179</pub-id></mixed-citation>
      </ref>
      <ref id="r27">
        <label>27</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. C.</given-names>
<surname>Van Essen</surname></string-name>, <string-name><given-names>S. M.</given-names>
<surname>Smith</surname></string-name>, <string-name><given-names>D. M.</given-names>
<surname>Barch</surname></string-name>, <string-name><given-names>T. E. J.</given-names>
<surname>Behrens</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Yacoub</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Ugurbil</surname></string-name></person-group>, <article-title>The WU-minn human connectome project: An overview</article-title>. <source>NeuroImage</source>
<volume><bold>80</bold></volume>, <fpage>62</fpage>–<lpage>79</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23684880</pub-id></mixed-citation>
      </ref>
      <ref id="r28">
        <label>28</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Faskowitz</surname></string-name>, <string-name><given-names>A. S.</given-names>
<surname>Teixeira</surname></string-name>, <string-name><given-names>S. A.</given-names>
<surname>Cutts</surname></string-name>, <string-name><given-names>R. F.</given-names>
<surname>Betzel</surname></string-name></person-group>, <article-title>Dynamic expression of brain functional systems disclosed by fine-scale analysis of edge time series</article-title>. <source>Netw. Neurosci.</source>
<volume><bold>5</bold></volume>, <fpage>405</fpage>–<lpage>433</lpage> (<year>2021</year>).<pub-id pub-id-type="pmid">34189371</pub-id></mixed-citation>
      </ref>
      <ref id="r29">
        <label>29</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J.-P.</given-names>
<surname>Onnela</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Saramäki</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Kertész</surname></string-name>, <string-name><given-names>K.</given-names>
<surname>Kaski</surname></string-name></person-group>, <article-title>Intensity and coherence of motifs in weighted complex networks</article-title>. <source>Phys. Rev. E</source>
<volume><bold>71</bold></volume>, <fpage>065103</fpage> (<year>2005</year>).</mixed-citation>
      </ref>
      <ref id="r30">
        <label>30</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L. G. S.</given-names>
<surname>Jeub</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Fortunato</surname></string-name></person-group>, <article-title>Multiresolution consensus clustering in networks</article-title>. <source>Sci. Rep.</source>
<volume><bold>8</bold></volume>, <fpage>3259</fpage> (<year>2018</year>).<pub-id pub-id-type="pmid">29459635</pub-id></mixed-citation>
      </ref>
      <ref id="r31">
        <label>31</label>
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><given-names>T.</given-names>
<surname>Kumar</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Vaidyanathan</surname></string-name>, <string-name><given-names>H.</given-names>
<surname>Ananthapadmanabhan</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Parthasarathy</surname></string-name>, <string-name><given-names>B.</given-names>
<surname>Ravindran</surname></string-name></person-group>, “<part-title>A new measure of modularity in hypergraphs: Theoretical insights and implications for effective clustering</part-title>” in <source>Complex Networks and Their Applications VIII</source>, <person-group person-group-type="editor"><string-name><given-names>H.</given-names>
<surname>Cherifi</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Gaito</surname></string-name>, <string-name><given-names>J. F.</given-names>
<surname>Mendes</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Moro</surname></string-name>, <string-name><given-names>L. M.</given-names>
<surname>Rocha</surname></string-name></person-group>, <role>Eds.</role> (<publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc>, <year>2020</year>), pp. <fpage>286</fpage>–<lpage>297</lpage>.</mixed-citation>
      </ref>
      <ref id="r32">
        <label>32</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>B. T.</given-names>
<surname>Yeo</surname></string-name>
<etal/></person-group>, <article-title>The organization of the human cerebral cortex estimated by intrinsic functional connectivity</article-title>. <source>J. Neurophysiol.</source>
<volume><bold>106</bold></volume>, <fpage>1125</fpage>–<lpage>1165</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21653723</pub-id></mixed-citation>
      </ref>
      <ref id="r33">
        <label>33</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. I.</given-names>
<surname>Luppi</surname></string-name>
<etal/></person-group>, <article-title>A synergistic core for human brain evolution and cognition</article-title>. <source>Nat. Neurosci.</source>
<volume><bold>25</bold></volume>, <fpage>771</fpage>–<lpage>782</lpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35618951</pub-id></mixed-citation>
      </ref>
      <ref id="r34">
        <label>34</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F. Z.</given-names>
<surname>Esfahlani</surname></string-name>
<etal/></person-group>, <article-title>High-amplitude cofluctuations in cortical activity drive functional connectivity</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>117</bold></volume>, <fpage>28393</fpage>–<lpage>28401</lpage> (<year>2020</year>).<pub-id pub-id-type="pmid">33093200</pub-id></mixed-citation>
      </ref>
      <ref id="r35">
        <label>35</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>R.</given-names>
<surname>Betzel</surname></string-name>
<etal/></person-group>, <article-title>Hierarchical organization of spontaneous co-fluctuations in densely-sampled individuals using fMRI</article-title>. <source>Netw. Neurosci</source>. (<year>2023</year>). <pub-id pub-id-type="doi">10.1162/netn_a_00321</pub-id>.</mixed-citation>
      </ref>
      <ref id="r36">
        <label>36</label>
        <mixed-citation publication-type="other">J. C. Tanner <italic toggle="yes">et al.</italic>, Synchronous high-amplitude co-fluctuations of functional brain networks during movie-watching (2022).</mixed-citation>
      </ref>
      <ref id="r37">
        <label>37</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Pope</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Fukushima</surname></string-name>, <string-name><given-names>R. F.</given-names>
<surname>Betzel</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name></person-group>, <article-title>Modular origins of high-amplitude cofluctuations in fine-scale functional connectivity dynamics</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>118</bold></volume>, <fpage>e2109380118</fpage> (<year>2021</year>).<pub-id pub-id-type="pmid">34750261</pub-id></mixed-citation>
      </ref>
      <ref id="r38">
        <label>38</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. F.</given-names>
<surname>Betzel</surname></string-name>, <string-name><given-names>S. A.</given-names>
<surname>Cutts</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Greenwell</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Faskowitz</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name></person-group>, <article-title>Individualized event structure drives individual differences in whole-brain functional connectivity</article-title>. <source>NeuroImage</source>
<volume><bold>252</bold></volume>, <fpage>118993</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35192942</pub-id></mixed-citation>
      </ref>
      <ref id="r39">
        <label>39</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name></person-group>, <article-title>Network analysis of time series: Novel approaches to network neuroscience</article-title>. <source>Front. Neurosci.</source>
<volume><bold>15</bold></volume>, <fpage>787068</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35221887</pub-id></mixed-citation>
      </ref>
      <ref id="r40">
        <label>40</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. P.</given-names>
<surname>Hoel</surname></string-name>, <string-name><given-names>L.</given-names>
<surname>Albantakis</surname></string-name>, <string-name><given-names>G.</given-names>
<surname>Tononi</surname></string-name></person-group>, <article-title>Quantifying causal emergence shows that macro can beat micro</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>110</bold></volume>, <fpage>19790</fpage>–<lpage>19795</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24248356</pub-id></mixed-citation>
      </ref>
      <ref id="r41">
        <label>41</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Zalesky</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Fornito</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Bullmore</surname></string-name></person-group>, <article-title>On the use of correlation as a measure of network connectivity</article-title>. <source>NeuroImage</source>
<volume><bold>60</bold></volume>, <fpage>2096</fpage>–<lpage>2106</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">22343126</pub-id></mixed-citation>
      </ref>
      <ref id="r42">
        <label>42</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E.</given-names>
<surname>Langford</surname></string-name>, <string-name><given-names>N.</given-names>
<surname>Schwertman</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Owens</surname></string-name></person-group>, <article-title>Is the property of being positively correlated transitive?</article-title>
<source>Am. Stat.</source>
<volume><bold>55</bold></volume>, <fpage>322</fpage>–<lpage>325</lpage> (<year>2001</year>).</mixed-citation>
      </ref>
      <ref id="r43">
        <label>43</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>P.</given-names>
<surname>Barttfeld</surname></string-name>
<etal/></person-group>, <article-title>Signature of consciousness in the dynamics of resting-state brain activity</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source>
<volume><bold>112</bold></volume>, <fpage>887</fpage>–<lpage>892</lpage> (<year>2015</year>).<pub-id pub-id-type="pmid">25561541</pub-id></mixed-citation>
      </ref>
      <ref id="r44">
        <label>44</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Demertzi</surname></string-name>
<etal/></person-group>
<article-title>Human consciousness is supported by dynamic complex patterns of brain signal coordination</article-title>. <source>Sci. Adv</source>. <volume><bold>5</bold></volume>, <fpage>eaat7603</fpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30775433</pub-id></mixed-citation>
      </ref>
      <ref id="r45">
        <label>45</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. M.</given-names>
<surname>Shine</surname></string-name>
<etal/></person-group>, <article-title>The dynamics of functional brain networks: Integrated network states during cognitive task performance</article-title>. <source>Neuron</source>
<volume><bold>92</bold></volume>, <fpage>544</fpage>–<lpage>554</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27693256</pub-id></mixed-citation>
      </ref>
      <ref id="r46">
        <label>46</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>R. M.</given-names>
<surname>Ahmed</surname></string-name>
<etal/></person-group>, <article-title>Neuronal network disintegration: Common pathways linking neurodegenerative diseases</article-title>. <source>J. Neurol. Neurosurg. Psychiatry.</source>
<volume><bold>87</bold></volume>, <fpage>1234</fpage>–<lpage>1241</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">27172939</pub-id></mixed-citation>
      </ref>
      <ref id="r47">
        <label>47</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. S.</given-names>
<surname>Damoiseaux</surname></string-name>, <string-name><given-names>K. E.</given-names>
<surname>Prater</surname></string-name>, <string-name><given-names>B. L.</given-names>
<surname>Miller</surname></string-name>, <string-name><given-names>M. D.</given-names>
<surname>Greicius</surname></string-name></person-group>, <article-title>Functional connectivity tracks clinical deterioration in Alzheimer’s disease</article-title>. <source>Neurobiol. Aging</source>
<volume><bold>33</bold></volume>, <fpage>828.e19</fpage>–<lpage>828.e30</lpage>.</mixed-citation>
      </ref>
      <ref id="r48">
        <label>48</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A. I.</given-names>
<surname>Luppi</surname></string-name>
<etal/></person-group>, <article-title>Consciousness-specific dynamic interactions of brain integration and functional diversity</article-title>. <source>Nat. Commun.</source>
<volume><bold>10</bold></volume>, <fpage>1</fpage>–<lpage>12</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
      </ref>
      <ref id="r49">
        <label>49</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>F.</given-names>
<surname>Zamani Esfahlani</surname></string-name>
<etal/></person-group>, <article-title>Edge-centric analysis of time-varying functional brain networks with applications in autism spectrum disorder</article-title>. <source>NeuroImage</source>
<volume><bold>263</bold></volume>, <fpage>119591</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">36031181</pub-id></mixed-citation>
      </ref>
      <ref id="r50">
        <label>50</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>D. P.</given-names>
<surname>Feldman</surname></string-name>, <string-name><given-names>J. P.</given-names>
<surname>Crutchfield</surname></string-name></person-group>, <article-title>Measures of statistical complexity: Why?</article-title>
<source>Phys. Lett. A</source>
<volume><bold>238</bold></volume>, <fpage>244</fpage>–<lpage>252</lpage> (<year>1998</year>).</mixed-citation>
      </ref>
      <ref id="r51">
        <label>51</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name></person-group>, <article-title>Flickering emergences: The question of locality in information-theoretic approaches to emergence</article-title>. <source>Entropy</source>
<volume><bold>25</bold></volume>, <fpage>54</fpage> (<year>2023</year>).</mixed-citation>
      </ref>
      <ref id="r52">
        <label>52</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>J. P.</given-names>
<surname>Vandenbroucke</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Broadbent</surname></string-name>, <string-name><given-names>N.</given-names>
<surname>Pearce</surname></string-name></person-group>, <article-title>Causality and causal inference in epidemiology: The need for a pluralistic approach</article-title>. <source>Int. J. Epidemiol.</source>
<volume><bold>45</bold></volume>, <fpage>1776</fpage>–<lpage>1786</lpage> (<year>2016</year>).<pub-id pub-id-type="pmid">26800751</pub-id></mixed-citation>
      </ref>
      <ref id="r53">
        <label>53</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>L.</given-names>
<surname>Novelli</surname></string-name>, <string-name><given-names>P.</given-names>
<surname>Wollstadt</surname></string-name>, <string-name><given-names>P.</given-names>
<surname>Mediano</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Wibral</surname></string-name>, <string-name><given-names>J. T.</given-names>
<surname>Lizier</surname></string-name></person-group>, <article-title>Large-scale directed network inference with multivariate transfer entropy and hierarchical statistical testing</article-title>. <source>Netw. Neurosci.</source>
<volume><bold>3</bold></volume>, <fpage>827</fpage>–<lpage>847</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">31410382</pub-id></mixed-citation>
      </ref>
      <ref id="r54">
        <label>54</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>P.</given-names>
<surname>Wollstadt</surname></string-name>, <string-name><given-names>S.</given-names>
<surname>Schmitt</surname></string-name>, <string-name><given-names>M.</given-names>
<surname>Wibral</surname></string-name></person-group>, <article-title>A rigorous information-theoretic definition of redundancy and relevancy in feature selection based on (partial) information decomposition</article-title>. arXiv [Preprint] (<year>2021</year>). <ext-link xlink:href="http://arxiv.org/abs/2105.04187" ext-link-type="uri" xlink:show="new">http://arxiv.org/abs/2105.04187</ext-link> [cs, math] (Accessed 3 June 2021).</mixed-citation>
      </ref>
      <ref id="r55">
        <label>55</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Hoel</surname></string-name></person-group>, <article-title>Emergence as the conversion of information: A unifying theory</article-title>. <source>Philos. Trans. Royal Soc.: A Math. Phys. Eng. Sci</source>. <volume><bold>380</bold></volume>, <fpage>20210150</fpage> (<year>2022</year>).</mixed-citation>
      </ref>
      <ref id="r56">
        <label>56</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>M.</given-names>
<surname>Contisciani</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Battiston</surname></string-name>, <string-name><given-names>C.</given-names>
<surname>De Bacco</surname></string-name></person-group>, <article-title>Inference of hyperedges and overlapping communities in hypergraphs</article-title>. <source>Nat. Commun.</source>
<volume><bold>13</bold></volume>, <fpage>7229</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">36433942</pub-id></mixed-citation>
      </ref>
      <ref id="r57">
        <label>57</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>A.</given-names>
<surname>Santoro</surname></string-name>, <string-name><given-names>F.</given-names>
<surname>Battiston</surname></string-name>, <string-name><given-names>G.</given-names>
<surname>Petri</surname></string-name>, <string-name><given-names>E.</given-names>
<surname>Amico</surname></string-name></person-group>, <article-title>Higher-order organization of multivariate time series</article-title>. <source>Nat. Phys.</source>
<volume><bold>19</bold></volume>, <fpage>221</fpage>–<lpage>229</lpage> (<year>2023</year>).</mixed-citation>
      </ref>
      <ref id="r58">
        <label>58</label>
        <mixed-citation publication-type="webpage"><person-group person-group-type="author"><string-name><given-names>K.</given-names>
<surname>Schick-Poland</surname></string-name>
<etal/></person-group>, <article-title>A partial information decomposition for discrete and continuous variables</article-title>. arXiv [Preprint] (<year>2021</year>). <ext-link xlink:href="http://arxiv.org/abs/2106.12393" ext-link-type="uri" xlink:show="new">http://arxiv.org/abs/2106.12393</ext-link> [cs, math] (Accessed 23 January 2022).</mixed-citation>
      </ref>
      <ref id="r59">
        <label>59</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T.</given-names>
<surname>Varley</surname></string-name>, <string-name><given-names>O.</given-names>
<surname>Sporns</surname></string-name>, <string-name><given-names>A.</given-names>
<surname>Puce</surname></string-name>, <string-name><given-names>J.</given-names>
<surname>Beggs</surname></string-name></person-group>, <article-title>Differential effects of propofol and ketamine on critical brain dynamics</article-title>. <source>PLoS Comput. Biol.</source>
<volume><bold>16</bold></volume>, <fpage>e1008418</fpage> (<year>2020</year>).<pub-id pub-id-type="pmid">33347455</pub-id></mixed-citation>
      </ref>
      <ref id="r60">
        <label>60</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name></person-group>, <article-title>Decomposing past and future: Integrated information decomposition based on shared probability mass exclusions</article-title>. <source>PLOS ONE</source>
<volume><bold>18</bold></volume>, <fpage>e0282950</fpage> (<year>2023</year>).<pub-id pub-id-type="pmid">36952508</pub-id></mixed-citation>
      </ref>
      <ref id="r61">
        <label>61</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><given-names>E. L.</given-names>
<surname>Newman</surname></string-name>, <string-name><given-names>T. F.</given-names>
<surname>Varley</surname></string-name>, <string-name><given-names>V. K.</given-names>
<surname>Parakkattu</surname></string-name>, <string-name><given-names>S. P.</given-names>
<surname>Sherrill</surname></string-name>, <string-name><given-names>J. M.</given-names>
<surname>Beggs</surname></string-name></person-group>, <article-title>Revealing the dynamics of neural information processing with multivariate information decomposition</article-title>. <source>Entropy</source>
<volume><bold>24</bold></volume>, <fpage>930</fpage> (<year>2022</year>).<pub-id pub-id-type="pmid">35885153</pub-id></mixed-citation>
      </ref>
    </ref-list>
    <sec sec-type="supplementary-material" id="s21">
      <title>Supporting Information</title>
    </sec>
  </back>
</article>
