<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" dtd-version="1.3">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
      <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
      <journal-title-group>
        <journal-title>Scientific Reports</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2045-2322</issn>
      <publisher>
        <publisher-name>Nature Publishing Group UK</publisher-name>
        <publisher-loc>London</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">35505233</article-id>
      <article-id pub-id-type="pmc">9065073</article-id>
      <article-id pub-id-type="publisher-id">11367</article-id>
      <article-id pub-id-type="doi">10.1038/s41598-022-11367-6</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Correlates of individual voice and face preferential responses during resting state</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Eckstein</surname>
            <given-names>Kathrin N.</given-names>
          </name>
          <address>
            <email>kathrin.eckstein@med.uni-tuebingen.de</email>
          </address>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Wildgruber</surname>
            <given-names>Dirk</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ethofer</surname>
            <given-names>Thomas</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Brück</surname>
            <given-names>Carolin</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Jacob</surname>
            <given-names>Heike</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Erb</surname>
            <given-names>Michael</given-names>
          </name>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Kreifelts</surname>
            <given-names>Benjamin</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10392.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1447</institution-id><institution>Department of Psychiatry and Psychotherapy, Tübingen Center for Mental Health (TüCMH), </institution><institution>University of Tübingen, </institution></institution-wrap>Calwerstrasse 14, 72076 Tübingen, Germany </aff>
        <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.10392.39</institution-id><institution-id institution-id-type="ISNI">0000 0001 2190 1447</institution-id><institution>Department for Biomedical Magnetic Resonance, </institution><institution>University of Tübingen, </institution></institution-wrap>Tübingen, Germany </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>3</day>
        <month>5</month>
        <year>2022</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>3</day>
        <month>5</month>
        <year>2022</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2022</year>
      </pub-date>
      <volume>12</volume>
      <elocation-id>7117</elocation-id>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>12</month>
          <year>2021</year>
        </date>
        <date date-type="accepted">
          <day>15</day>
          <month>4</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© The Author(s) 2022</copyright-statement>
        <license>
          <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
        </license>
      </permissions>
      <abstract id="Abs1">
        <p id="Par1">Human nonverbal social signals are transmitted to a large extent by vocal and facial cues. The prominent importance of these cues is reflected in specialized cerebral regions which preferentially respond to these stimuli, e.g. the temporal voice area (TVA) for human voices and the fusiform face area (FFA) for human faces. But it remained up to date unknown whether there are respective specializations during resting state, i.e. in the absence of any cues, and if so, whether these representations share neural substrates across sensory modalities. In the present study, resting state functional connectivity (RSFC) as well as voice- and face-preferential activations were analysed from functional magnetic resonance imaging (fMRI) data sets of 60 healthy individuals. Data analysis comprised seed-based analyses using the TVA and FFA as regions of interest (ROIs) as well as multi voxel pattern analyses (MVPA). Using the face- and voice-preferential responses of the FFA and TVA as regressors, we identified several correlating clusters during resting state spread across frontal, temporal, parietal and occipital regions. Using these regions as seeds, characteristic and distinct network patterns were apparent with a predominantly convergent pattern for the bilateral TVAs whereas a largely divergent pattern was observed for the bilateral FFAs. One region in the anterior medial frontal cortex displayed a maximum of supramodal convergence of informative connectivity patterns reflecting voice- and face-preferential responses of both TVAs and the right FFA, pointing to shared neural resources in supramodal voice and face processing. The association of individual voice- and face-preferential neural activity with resting state connectivity patterns may support the perspective of a network function of the brain beyond an activation of specialized regions.</p>
      </abstract>
      <kwd-group kwd-group-type="npg-subject">
        <title>Subject terms</title>
        <kwd>Neuroscience</kwd>
        <kwd>Auditory system</kwd>
        <kwd>Cognitive neuroscience</kwd>
        <kwd>Sensory processing</kwd>
        <kwd>Social neuroscience</kwd>
        <kwd>Visual system</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>
            <institution-wrap>
              <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009397</institution-id>
              <institution>Medizinischen Fakultät, Eberhard Karls Universität Tübingen</institution>
            </institution-wrap>
          </funding-source>
          <award-id>367-0-0</award-id>
          <principal-award-recipient>
            <name>
              <surname>Eckstein</surname>
              <given-names>Kathrin N.</given-names>
            </name>
          </principal-award-recipient>
        </award-group>
      </funding-group>
      <funding-group>
        <award-group>
          <funding-source>
            <institution>Universitätsklinikum Tübingen (8868)</institution>
          </funding-source>
        </award-group>
        <open-access>
          <p>Open Access funding enabled and organized by Projekt DEAL.</p>
        </open-access>
      </funding-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>© The Author(s) 2022</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Introduction</title>
      <p id="Par2">Voices and faces are among the most salient cues in human life. This is reflected in the existence of specialized cerebral modules which are hierarchically organized and specifically tuned to respond to these cues. Core components for the primary identification of human voices and faces are the temporal voice area (TVA) for voices<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup> and the fusiform face area (FFA) for faces<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>. While not exclusively activated by these signals, they exhibit clearly voice- and face-preferential responses, respectively. The FFA together with the occipital face area (OFA) respond mainly to invariant facial features (e.g. gender)<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Further processing of dynamic face aspects, and integration of signals from voices and faces involves the posterior superior temporal sulcus (pSTS) and the thalamus<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup>. The emotional information often present in faces and voices (e.g. in facial expressions and emotional prosody) additionally converges in the amygdala<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. Further processing of such emotional information involves further regions such as the inferior frontal cortex (IFC) and orbitofrontal cortex (OFC)<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. Convergent with the particular importance of voices and faces in human social communication, recent studies indicated that the responsivity to the preferred cues of the basic modules for identification of human voices and faces is moderated by interindividual differences in social signal processing, e.g. social anxiety<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, and emotional intelligence<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, even in the absence of emotional information. In some cases, as described above e.g. for the pSTS and thalamus, the hemodynamic correlates of cerebral processing of signals from different sensory modalities overlap. This phenomenon will be termed supramodal throughout this manuscript.</p>
      <p id="Par3">While a plethora of neuroimaging studies delineated the neural networks that are active when we see faces or hear voices, it remains a completely open question if the brain’s activity patterns also reflect the individual cerebral responsivity to voices and faces in the absence of these cues and if these representations may share neural substrates across sensory modalities.</p>
      <p id="Par4">During the past three decades, the resting brain has become a major research focus as it became clear that spontaneous physiological low-frequency fluctuations in brain activity occur non-randomly but simultaneously in various, partially overlapping neural networks in the absence of any cues or stimulation or cognitive/emotional task<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Nevertheless, these fluctuation patterns are not independent from individual traits or diseases, as they have been shown to correlate with various aspects of behavioural tendencies<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>, personality<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>, psychopathology<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, and psychiatric disease (e.g. dementia and schizophrenia<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>) also demonstrating that resting state data can be used to expand the neuroimaging perspective on their cerebral representation in a complementary manner with the potential to detect links between the neural networks underlying various perceptual, cognitive or emotional functions not apparent in stimulation-based designs.</p>
      <p id="Par5">In the area of face and voice processing, correlations of resting state functional connectivity (RSFC) with behavioural outcomes, e.g. performance in various face- and voice-processing tasks have been observed<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR30">30</xref></sup>. One study compared functional connectivity patterns during resting state and a passive viewing task and found for both conditions similar networks including posterior fusiform gyrus, inferior occipital gyrus and superior temporal sulcus<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. In this work the informative RSFC patterns were found exclusively within the network of modality-specific preferential processing areas<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Two studies combined RSFC in the face processing network with behavioural performance in a face identification task and an emotional face matching task, respectively<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup> and found RSFC patterns between modality-specific preferential processing areas but also with other parts of the brain<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. One study in children revealed that performance in an auditory emotional prosody recognition task was predicted by stronger connectivity between the inferior frontal gyrus and motor regions. Here, informative RSFC patterns were found exclusively outside the modality-specific preferential processing networks<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</p>
      <p id="Par6">In the present study, we intended to determine the neural correlates of voice- and face-preferential responses in the absence of voices and faces in the resting state. Furthermore, we aimed to identify brain areas with RSFC patterns supramodally reflecting preferential responses to both, voices and faces. To this end, 60 healthy individuals underwent functional magnetic resonance imaging (fMRI) at rest and during stimulation with voices, faces and various other classes of acoustic and visual stimuli. Individual voice- and face-preferential responses were correlated with RSFC employing multi voxel pattern analyses (MVPA) and seed-based analyses focused on TVA and FFA.</p>
    </sec>
    <sec id="Sec2">
      <title>Materials and methods</title>
      <sec id="Sec3">
        <title>Participants</title>
        <p id="Par7">60 healthy individuals (mean age 25.8 years, s.d. = 4.5 years, 30 female) participated at the University of Tübingen. All of the participants were native German speakers and right-handed, as assessed with the Edinburgh Inventory<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. None of the participants was taking any regular medication, or had a history of substance abuse, or psychiatric or neurological illness. Hearing was normal, vision normal or corrected to normal in all participants. The study was performed according to the Code of Ethics of the World Medical Association (Declaration of Helsinki) and the protocol of human investigation was approved by the local ethics committees where the study was performed (i.e., the medical faculties of the Universities of Tübingen and Greifswald). All individuals gave their written informed consent prior to their participation in the study.
</p>
      </sec>
      <sec id="Sec4">
        <title>Stimuli and experimental design</title>
        <p id="Par8">Two fMRI experiments were performed to localize face-sensitive<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> and voice-sensitive<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> brain areas as described in previous publications<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>: For the face-sensitivity experiment, pictures from four different categories (faces, houses, objects, and natural scenes) were employed within a block design. All stimuli used in the experiment were black-and-white photographs unknown to the participants<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The shown face stimuli had no obvious emotional connotation, but rather showed neutral facial expressions. The house stimuli were multilevel apartment houses from different materials (brick, wooden, concrete). As object stimuli different everyday life items were used (e.g. flat iron, spoon, T-shirt). The fourth category of natural scenes represented different countryside pictures (e.g. mountain, coastline, waterfall). Each block and category contained 20 stimuli<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Within blocks, the stimuli were presented in random order for 300 ms. Stimuli were separated by 500 ms periods of fixation [1 block = 20 stimuli × (300 ms picture + 500 ms fixation) = 16 s]. Eight blocks of each category pseudorandomized within the experiment were shown separated by short ~ 1.5 s rest periods<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. A one-back task was employed, in which the participants had to press a button on a fibre optic system (LumiTouch, Photon Control, Burnaby, Canada) with their right index finger when they saw a picture twice in a row, to ascertain constant attention<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The appearance of repeated stimuli was pseudorandomized ensuring a distribution across the entire experiment. Visual stimuli were back-projected onto a screen placed in the magnet bore behind the participant’s head and viewed by the participant through a mirror system mounted onto the head coil.</p>
        <p id="Par9">The voice-sensitivity experiment was developed based on the study by Belin et al.<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> in form of a block design experiment with 24 stimulation blocks and 12 silent periods (each 8 s) in a passive-listening design without an explicit task. Between the blocks were short periods without sound (2 s). Participants were instructed to listen attentively with their eyes closed. The stimulus material comprised 12 blocks of human vocal sounds (speech, sighs, laughs, cries), 6 blocks of animal sounds (e.g., gallops, various cries) and 6 blocks of environmental sounds (e.g., cars, planes, doors, telephones). Stimuli were normalized with respect to mean acoustic energy<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Sound and silence blocks were pseudorandomized across the experiment with the restriction that with the restriction that no two blocks of silence directly followed each other.</p>
        <p id="Par10">Both experimental designs have been validated in previous studies<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. Further details on the stimulus material and experimental designs have been reported elsewhere<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p>
        <p id="Par11">For the resting state measurements (duration about 7 min and 15 s), the participants were instructed to keep their eyes closed with no further task.</p>
      </sec>
      <sec id="Sec5">
        <title>Image acquisition</title>
        <p id="Par12">MRI data were acquired with a TRIO 3T and a PRISMA scanner (Siemens, Erlangen, Germany). Structural T1-weighted images (176 slices, TR = 2300 ms, TE = 2.96 ms, TI = 1100 ms, voxel size: 1 × 1 × 1 mm<sup>3</sup>) and functional images (30 axial slices captured in sequential descending order, 3 mm thickness + 1 mm gap, TR = 1.7 s, TE = 30 ms, voxel size: 3 × 3 × 4 mm<sup>3</sup>, field of view 192 × 192 mm<sup>2</sup>, 64 × 64 matrix, flip angle 90°) were recorded. For the resting state measurements, 245 images were recorded. The activation tasks were performed after completion of the resting state measurements to avoid carry-over effects. The time series comprised 368 images for the face experiment and 232 images for the voice experiment and 250 images for the resting state measurement. A field map with 36 slices (slice thickness 3 mm, TR = 400 ms, TE(1) = 5.19 ms, TE(2) = 7.65 ms) was recorded.</p>
      </sec>
      <sec id="Sec6">
        <title>Analysis of fMRI data</title>
        <p id="Par13">Statistical parametric mapping software (SPM8, Wellcome Department of Imaging Neuroscience, London, <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) was used to analyse the imaging data. Pre-processing generally included the removal of the first five EPI images from each run to exclude measurements preceding T1 equilibrium.</p>
        <sec id="Sec7">
          <title>Face- and voice-sensitivity experiments</title>
          <p id="Par14">The preprocessing procedure consisted of realignment, unwarping using a static field map, coregistration of anatomical and functional images, segmentation of the anatomical images, normalization into MNI space (Montreal Neurological Institute<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>) with a resampled voxel size of 3 × 3 × 3 mm<sup>3</sup>, temporal smoothing with a high-pass filter (cut-off frequency of 1/128 Hz) and spatial smoothing employing a Gaussian kernel (8 mm full width at half maximum, FWHM). The response to the single categories (faces (F), houses (H), objects (O), and natural scenes (S) in the face localizer as well as vocal sounds (V), animal sounds (A), and environmental sounds (E) in the voice localizer were independently modelled with a box-car function corresponding to the duration of the stimulation blocks (16 s in the face localizer and 8 s in the voice localizer) convolved with the hemodynamic response function (HRF). The error term was calculated as a first order autoregressive process with a coefficient of 0.2 and a white noise component accounting for serial autocorrelations<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. To minimize motion-associated error variance, the six motion parameters (i.e. translation and rotation on the x-, y-, and z-axes) were included in the single subject models as covariates.</p>
          <p id="Par15">Contrast images were constructed using data from the first-level general linear models [face-sensitivity: F &gt; (H, O, S); voice-sensitivity: V &gt; (A, E)] for each subject. Taking these contrast images as sources, a second-level random-effect analysis was performed with one-sample t-tests to define the face-sensitive fusiform face area (FFA) and the voice-sensitive temporal voice area (TVA) as functional regions of interest (ROI) for further analyses. Statistical significance of activations was assessed at p &lt; 0.001, uncorrected at voxel level and with FWE correction for multiple comparisons at cluster level with p &lt; 0.05. For the definition of the FFA, the fusiform gyrus was taken as a priori anatomical ROI; for definition of the TVA, the temporal gyri and the temporal pole were selected. For definition of the functional ROIs (i.e. FFA and TVA), FWE-cluster level correction was performed across these a priori anatomical ROIs using small volume correction (SVC<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>). We picked the maximum activation in the fusiform gyrus for the FFA and in the temporal lobe for the TVA respectively, and defined the surrounding 100 most sensitive voxels as masks for the functional ROIs. Within these ROIs individual voice- and face-preferential responses were assessed using minimum difference criteria (for voices V &gt; max[A, E], for faces F &gt; max[H, O, S])<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Intercorrelations of the four regressors were evaluated. Differences in the face- and voice-sensitive and -preferential responses between both hemispheres and interhemispheric differences in cue-sensitivity and -preferentiality between TVA and FFA were post hoc tested using two-sided paired t-tests with Bonferroni correction.</p>
        </sec>
        <sec id="Sec8">
          <title>Resting state functional connectivity analysis</title>
          <p id="Par16">For RSFC analyses we used the CONN toolbox (v 16b<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>) implemented in SPM8. The spatial preprocessing was performed analogously to the procedure described for the face- and voice-sensitivity experiments. Denoising included linear regression of the following confounding effects: White matter and CSF components (6P each), effect of rest (2P, temporal component and first order derivates) and motion regression (12 regressors: 6 motion parameters and 6 first-order temporal derivates) and band-pass filtering (0.008–0.09 Hz). Linear detrending was added to remove linear trends.</p>
          <p id="Par17">The participants’ movement parameters, their first order derivatives and the BOLD signal from white matter, cerebrospinal fluid and effect of rest (each with five temporal components) were included in the analysis as covariates to reduce their confounding influences. In the individual first-level analyses, bivariate correlation coefficients were calculated as linear measures of functional connectivity for the ensuing analyses. Coefficients were Z transformed to achieve comparability for group-level analyses, and gender, age and scanner were included as regressors of no interest. The Automated Anatomic Labelling (AAL) toolbox<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> was used for the definition of anatomical regions in MNI space. The analysis targeted the correlation of individual resting state functional connectivity (RSFC) with face-/voice-preferential responses both with defined regions of interest (ROIs) and at whole brain level. To this end, analyses were done on two different levels: ROI-to-voxel analyses should detect associations between individual voice- and face-preferential responses of the ROIs and their RSFC with other brain regions. Here, the significance of observed connectivity patterns was assessed using a threshold of p &lt; 0.001 at voxel level, two-tailed with FWE correction (p &lt; 0.05) for multiple comparisons at cluster level. Results were Bonferroni-corrected for the numbers of regressors (4) and ROIs (4), so that the effective cluster threshold amounted to p &lt; 0.00315. Second, a spatial hypothesis-free strategy was implemented using voxel-to-voxel multivariate multi voxel pattern analyses (MVPA). Here, for each voxel separately, a low-dimensional multivariate representation of the connectivity pattern between this voxel and all other voxels in the brain was calculated. This representation was based on a principal component analysis of the inter-subject variability of each separate voxel’s connectivity pattern enabling the investigation of differences across subjects using second-level multivariate analyses. The number of principal components was set to three and number of dimensions was set to 64 (dimensionality reduction)<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. The goal of the group-MVPA approach was to detect whole brain resting state functional connectivity patterns correlating with individual voice-preferential responses of the TVA (i.e., V &gt; max[A,E]) and face-preferential responses (i.e., F &gt; max[H,O,S]) of the FFA. These individual estimates were used as group level regressors in the RSFC analyses (four regressors: two for the FFAs, two for the TVAs). Results were evaluated at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with additional Bonferroni-correction for the number of tested regressors (4) resulting in an effective cluster threshold of p &lt; 0.0125. Findings of the MVPA were further analysed using the significant clusters as seeds for ensuing seed-to-voxel analyses. Convergence of RSFC patterns between different seeds was tested using conjunction analyses with a minimum statistic<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Results were assessed at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05.</p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Results</title>
      <sec id="Sec10">
        <title>ROI characteristics</title>
        <p id="Par18">The activation pattern of the right and left FFA showed a significant sensitivity for faces (rFFA t = 9.321, p &lt; 0.001 and lFFA t = 7.585, p &lt; 0.001), whereas significant face-preferential responses were observed in the right FFA (t = 4.344, p &lt; 0.0001), but not the left FFA (t = 0.624, p = 0.535). The bilateral TVAs were highly sensitive to and preferential for voices (sensitivity: rTVA t = 18.265, p &lt; 0.0001 and lTVA t = 17.457, p &lt; 0.001; preferentiality: rTVA t = 14.456, p &lt; 0.001 and lTVA t = 14.023, p &lt; 0.001). ROI characteristics are graphically displayed in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The ROIs’ preferential responses to their preferred cues were significantly correlated within modality (voices: r(58) = 0.74, p &lt; 0.001; faces: r(58) = 0.60, p &lt; 0.001) but not across modalities (all abs(r(58)) &lt; 0.12, all p &gt; 0.05).<fig id="Fig1"><label>Figure 1</label><caption><p>Face and voice processing areas. (<bold>a</bold>) The fusiform face area (rFFA in green, lFFA in blue), and (<bold>b</bold>) the temporal voice area (rTVA in red, lTVA in yellow), rendered onto the mean anatomical scan of the study population. The functional ROIs (i.e. FFA and TVA) were identified selecting the maximum activation in the fusiform gyrus and in the temporal lobe respectively, and defining the surrounding 100 most sensitive voxels as masks. Face-sensitivity is given as F &gt; (H, O, S), voice-sensitivity as V &gt; (A, E) for each subject. Individual voice- and face-preferential responses were assessed using minimum difference criteria (for voices V &gt; max[A, E], for faces F &gt; max[H, O, S]). The bars depict mean voice- and face-sensitivity of each region. Bold frames indicate that these regions respond also preferentially to the highlighted cues as compared to each of the experimental comparators (for details see “<xref rid="Sec2" ref-type="sec">Materials and methods</xref>” section). Coordinates refer to MNI space. <italic>R</italic> right, <italic>L</italic> left. Error bars indicate the standard error of the mean. Additional material is available in the Supplement: Supplemental Fig. <xref rid="MOESM1" ref-type="media">1</xref> depicts whole brain slices of the functional ROIs FFA and TVA.</p></caption><graphic xlink:href="41598_2022_11367_Fig1_HTML" id="MO1"/></fig></p>
        <p id="Par19">Comparison between the right and left hemisphere revealed no significant difference for voice-sensitivity or -preferentiality (all t &lt; 2.03), all p &gt; 0.187), but significant differences for face-sensitivity and -preferentiality in favour of the right hemisphere (all t &gt; 3.75, all p &lt; 0.004). Comparison of hemispheric differences in modality-specific differences in cue sensitivity and preferentiality between TVA and FFA corroborated the difference between the sensory modalities, both for sensitivity and for preferentiality (all t &gt; 3.93, p &lt; 0.002), i.e. a greater hemispheric difference in face-sensitivity and -preferentiality than in voice-sensitivity and -preferentiality.</p>
      </sec>
      <sec id="Sec11">
        <title>ROI-to-voxel analysis</title>
        <p id="Par20">In this analysis, only individual voice-preferential responses of the lTVA were significantly associated with RSFC between the lTVA and a cluster in the right supramarginal gyrus extending into the inferior parietal gyrus (peak: − 57x − 66y 27z; 143 voxels; p(FWE-corr.) = 0.0018).</p>
      </sec>
      <sec id="Sec12">
        <title>Multi-voxel pattern analysis (MVPA)</title>
        <p id="Par21">Using rFFA face-preferential responses as regressor, we identified one informative cluster in the right middle frontal gyrus extending into the precentral gyrus. For the lFFA two clusters in the left caudate nucleus/olfactory gyrus and left superior temporal pole were evident. For the rTVA and lTVA voice-preferential responses four overlapping clusters emerged: in the left superior occipital gyrus, the right inferior parietal gyrus, the right superior temporal gyrus and the right frontal inferior orbital gyrus. For rTVA voice-preferentiality two additional clusters were detected in the left middle occipital gyrus and the right thalamus, for the left TVA two additional clusters were located in the left frontal superior gyrus and the right parietal superior gyrus. A detailed description of the clusters can be found in Table <xref rid="Tab1" ref-type="table">1</xref>. A graphical representation is displayed in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Multi-voxel pattern analysis (MVPA).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Anatomical structure</th><th align="left">Peak voxel</th><th align="left">Cluster size (vx)</th><th align="left">p</th></tr></thead><tbody><tr><td align="left">rFFA</td><td align="left">R middle frontal gyrus/precentral gyrus</td><td align="left"> + 45 + 09 + 45</td><td align="left">14</td><td char="." align="char">0.00192</td></tr><tr><td align="left" rowspan="2">lFFA</td><td align="left">L caudate nucleus and olfactory gyri</td><td align="left"> − 06 + 06 − 15</td><td align="left">45</td><td char="." align="char">0.000036</td></tr><tr><td align="left">L superior temporal pole</td><td align="left"> − 27 + 06 − 30</td><td align="left">15</td><td char="." align="char">0.001948</td></tr><tr><td align="left" rowspan="6">rTVA</td><td align="left"><bold>L superior occipital gyrus</bold></td><td align="left"> − 24 − 81 + 33</td><td align="left">91</td><td char="." align="char">0.000036</td></tr><tr><td align="left"><bold>R inferior parietal gyrus</bold></td><td align="left"> + 36 − 51 + 42</td><td align="left">49</td><td char="." align="char">0.000036</td></tr><tr><td align="left"><bold>R superior temporal gyrus</bold></td><td align="left"> + 54 − 36 + 15</td><td align="left">13</td><td char="." align="char">0.006604</td></tr><tr><td align="left"><bold>R frontal inferior orbital gyrus</bold></td><td align="left"> + 48 + 36 − 06</td><td align="left">50</td><td char="." align="char">0.000036</td></tr><tr><td align="left">L middle occipital gyrus</td><td align="left"> − 36 − 81 + 48</td><td align="left">42</td><td char="." align="char">0.000036</td></tr><tr><td align="left">R thalamus</td><td align="left"> + 06 − 21 + 21</td><td align="left">14</td><td char="." align="char">0.003756</td></tr><tr><td align="left" rowspan="6">lTVA</td><td align="left"><bold>L superior occipital gyrus</bold></td><td align="left"> − 27 − 81 + 33</td><td align="left">51</td><td char="." align="char">0.000036</td></tr><tr><td align="left"><bold>R inferior parietal gyrus</bold></td><td align="left"> + 36 − 42 + 36</td><td align="left">27</td><td char="." align="char">0.00004</td></tr><tr><td align="left"><bold>R superior temporal gyrus</bold></td><td align="left"> + 57 − 33 + 15</td><td align="left">24</td><td char="." align="char">0.00006</td></tr><tr><td align="left"><bold>R frontal inferior orbital gyrus</bold></td><td align="left"> + 51 + 39 − 09</td><td align="left">26</td><td char="." align="char">0.00004</td></tr><tr><td align="left">L frontal superior gyrus</td><td align="left"> − 15 + 06 + 69</td><td align="left">17</td><td char="." align="char">0.000716</td></tr><tr><td align="left">R parietal superior gyrus</td><td align="left"> + 15 − 78 + 48</td><td align="left">13</td><td char="." align="char">0.00622</td></tr></tbody></table><table-wrap-foot><p>RSFC correlates of individual voice- and face-preferential responses in TVA and FFA.</p><p>Voxel-wise threshold was set to p &lt; 0.001 and whole brain FWE-corrected at cluster level with additional Bonferroni-correction for the number of tested regressors (4) leading to an effective cluster threshold of p &lt; 0.0125.</p><p><italic>R</italic> right, <italic>L</italic> left.</p><p>Overlapping clusters for the rTVA and lTVA are marked in bold. Voxel size 3 × 3 × 4 mm<sup>3</sup>.</p></table-wrap-foot></table-wrap><fig id="Fig2"><label>Figure 2</label><caption><p>Multi-voxel pattern analyses: correlates of voice- and face-preferential responses. Areas of the RSFC patterns which significantly correlate with the individual responses to the preferred cues of FFA and TVA (red/yellow: right/left TVA voice-preferential responses (<bold>a–f</bold>); orange: overlap of right/left TVA voice-preferentiality correlates (<bold>b,c,e,f</bold>); green/blue: right/left FFA face-preferentiality correlates (<bold>a,d</bold>). Underlying RSFC patterns of informative clusters not shown here. Results are shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with additional Bonferroni-correction for the number of tested regressors (4) leading to an effective cluster threshold of p &lt; 0.0125.</p></caption><graphic xlink:href="41598_2022_11367_Fig2_HTML" id="MO2"/></fig></p>
        <p id="Par22">For the four overlapping clusters informative of both the rTVA and lTVA voice-preferential responses common regions were calculated and further on used as seeds. The characteristics of the resulting clusters are described in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Multi-voxel pattern analysis (MVPA).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Anatomical structure</th><th align="left">Peak voxel</th><th align="left">Cluster size (vx)</th><th align="left">p</th></tr></thead><tbody><tr><td align="left" rowspan="4">rlTVA</td><td align="left">L superior occipital gyrus</td><td align="left"> − 27 − 81 + 33</td><td align="left">42</td><td char="." align="char">0.000009</td></tr><tr><td align="left">R inferior parietal gyrus</td><td align="left"> + 36 − 45 + 42</td><td align="left">14</td><td char="." align="char">0.000939</td></tr><tr><td align="left">R superior temporal gyrus</td><td align="left"> + 54 − 36 + 15</td><td align="left">9</td><td char="." align="char">0.019070</td></tr><tr><td align="left">R frontal inferior orbital gyrus</td><td align="left"> + 51 + 39 − 09</td><td align="left">23</td><td char="." align="char">0.000019</td></tr></tbody></table><table-wrap-foot><p>Overlapping clusters informative of both the rTVA and lTVA voice-preferential responses.</p><p>Voxel-wise threshold was set to p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05. Voxel size 3 × 3 × 4 mm<sup>3</sup>.</p><p><italic>R</italic> right, <italic>L</italic> left.</p></table-wrap-foot></table-wrap></p>
        <p id="Par23">Significant clusters were used as seeds for subsequent post-hoc explanatory seed-to-voxel analyses.</p>
        <p id="Par24">For the TVAs the convergence of informative MVPA clusters was accompanied by a relatively strong convergence of their RSFC patterns in contrast to the FFAs’ RSFC patterns. Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref> give an overview of convergent RSFC clusters across all informative regions observed in the MVPA analysis. Convergent clusters for the bilateral TVAs are listed in Table <xref rid="Tab3" ref-type="table">3</xref>, exemplary graphical representations are given in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Convergent RSFC patterns informative of bilateral TVA voice-preferential responses.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Seed cluster</th><th align="left">Direction of correlation</th><th align="left">Anatomical structure</th><th align="left">Peak voxel</th><th align="left">Cluster size (vx)</th><th align="left">p</th></tr></thead><tbody><tr><td align="left" rowspan="2">L superior occipital gyrus (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a)</td><td align="left">Negative</td><td align="left">R and L calcarine cortex/R and L lingual gyri/R and L cuneus/R and L superior, middle and inferior occipital gyri</td><td align="left">36 − 63 − 9</td><td align="left">1311</td><td char="." align="char"> &lt; 0.001</td></tr><tr><td align="left">Positive</td><td align="left">L middle frontal gyrus/L inferior frontal gyrus, pars triangularis</td><td align="left"> − 42 36 36</td><td align="left">75</td><td char="." align="char">0.017</td></tr><tr><td align="left">R inferior parietal gyrus</td><td align="left">Negative</td><td align="left">R precuneus</td><td align="left">21 − 54 27</td><td align="left">79</td><td char="." align="char">0.011</td></tr><tr><td align="left">R superior temporal gyrus</td><td align="left"/><td align="left">No suprathreshold cluster</td><td align="left"/><td align="left"/><td char="." align="char"/></tr><tr><td align="left" rowspan="7">R frontal inferior orbital gyrus (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b)</td><td align="left">Positive</td><td align="left">R and L cingulum/R and L supplementary motor area</td><td align="left"> − 6 3 42</td><td align="left">393</td><td char="." align="char"> &lt; 0.001</td></tr><tr><td align="left">Positive</td><td align="left"><p>R insula/R Rolandic operculum/R frontal inferior operculum/R putamen</p><p>R superior temporal pole</p></td><td align="left">27 21 3</td><td align="left">211</td><td char="." align="char"> &lt; 0.001</td></tr><tr><td align="left">Positive</td><td align="left">R supramarginal gyrus/R superior temporal lobe/R postcentral lobe</td><td align="left">69 − 24 24</td><td align="left">187</td><td char="." align="char"> &lt; 0.001</td></tr><tr><td align="left">Positive</td><td align="left">L Insula/L Rolandic operculum/L frontal inferior operculum/L superior temporal pole</td><td align="left"> − 36 3 3</td><td align="left">109</td><td char="." align="char">0.002</td></tr><tr><td align="left">Positive</td><td align="left">L superior parietal gyrus/L precuneus</td><td align="left"> − 15 − 48 57</td><td align="left">60</td><td char="." align="char">0.031</td></tr><tr><td align="left">Negative</td><td align="left">R medial orbital gyrus/R superior frontal gyrus, medial/R superior frontal gyrus, orbital</td><td align="left">12 72 − 3</td><td align="left">89</td><td char="." align="char">0.006</td></tr><tr><td align="left">Negative</td><td align="left">R and L cerebellum lobule IX</td><td align="left">3 − 54 − 45</td><td align="left">55</td><td char="." align="char">0.042</td></tr></tbody></table><table-wrap-foot><p>Results are shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05. Voxel size 3 × 3 × 4 mm<sup>3</sup>.</p><p><italic>R</italic> right, <italic>L</italic> left.</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Number of intramodal convergent RSFC clusters using the MVPA clusters with the RSFC correlates of individual voice-preferential responses in the right and left TVA.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">MVPA<break/>cluster</th><th align="left">rTVA<break/>L sup occ</th><th align="left">rTVA<break/>R inf par</th><th align="left">rTVA<break/>R sup temp</th><th align="left">rTVA<break/>R front inf</th><th align="left">rTVA<break/>L mid occ − 36 − 81 48</th><th align="left">rTVA<break/>R thalamus 6 − 21 21</th><th align="left">lTVA<break/>L sup occ</th><th align="left">lTVA<break/>R inf par</th><th align="left">lTVA<break/>R sup temp</th><th align="left">lTVA<break/>R front inf</th><th align="left">lTVA<break/>L front sup − 15 6 69</th></tr></thead><tbody><tr><td align="left"><p>rTVA</p><p>L sup occ</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>rTVA</p><p>R inf par</p></td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>rTVA</p><p>R sup temp</p></td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>rTVA</p><p>R front inf</p></td><td align="left">–</td><td align="left">–</td><td align="left"><italic>1</italic></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>rTVA</p><p>L mid occ − 36 − 81 48</p></td><td align="left"><italic>2</italic></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>rTVA</p><p>R thalamus 6 − 21 21</p></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>L sup occ</p></td><td align="left"><bold>2</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>R inf par</p></td><td align="left">–</td><td align="left"><bold>1</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>R sup temp</p></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>R front inf</p></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"><bold>7</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"/><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>L front sup − 15 6 69</p></td><td align="left"><italic>2</italic></td><td align="left">–</td><td align="left">–</td><td align="left"><italic>2</italic></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"><italic>2</italic></td><td align="left"/></tr><tr><td align="left"><p>lTVA</p><p>R pariet sup 15 − 78 48</p></td><td align="left"><italic>1</italic></td><td align="left">–</td><td align="left">–</td><td align="left"><italic>1</italic></td><td align="left">–</td><td align="left">–</td><td align="left"><italic>1</italic></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>Results are shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05.</p><p><italic>R</italic> right, <italic>L</italic> left.</p><p>Voice-preferentiality is encoded in bold and italics. The bold signifies significant clusters between the corresponding right and left TVA region, italics between different TVA regions. Voxel size 3 × 3 × 4 mm<sup>3</sup>.</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Number of supramodal convergent RSFC clusters using MVPA clusters with the RSFC correlates of individual face-preferential responses in the right and left FFA with RSFC correlates of individual voice-preferential responses in the right and left TVA.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">MVPA<break/>cluster</th><th align="left">rTVA<break/>L sup occ</th><th align="left">rTVA<break/>R inf par</th><th align="left">rTVA<break/>R sup temp</th><th align="left">rTVA<break/>R front inf</th><th align="left">rTVA<break/>L mid occ − 36 − 81 48</th><th align="left">rTVA<break/>R thalamus 6 − 21 21</th><th align="left">lTVA<break/>L sup occ</th><th align="left">lTVA<break/>R inf par</th><th align="left">lTVA<break/>R sup temp</th><th align="left">lTVA<break/>R front inf</th><th align="left">lTVA<break/>L front sup − 15 6 69</th><th align="left">lTVA<break/>R pariet sup 15 − 78 48</th></tr></thead><tbody><tr><td align="left"><p>rFFA</p><p>R mid front</p></td><td align="left"><bold>4</bold></td><td align="left">–</td><td align="left">–</td><td align="left"><bold>1</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"><bold>1</bold></td><td align="left">–</td></tr><tr><td align="left"><p>lFFA</p><p>L caudate − 6 6 − 15</p></td><td align="left"><bold>1</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left"><bold>1</bold></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left"><p>lFFA</p><p>L sup temp − 27 6 − 30</p></td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>Results are shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05.</p><p><italic>R</italic> right, <italic>L</italic> left.</p><p>Supramodal conjunctions are encoded in bold. Voxel size 3 × 3 × 4 mm<sup>3</sup>.</p></table-wrap-foot></table-wrap><fig id="Fig3"><label>Figure 3</label><caption><p>Convergence of informative RSFC patterns for bilateral TVA voice-preferentiality using (<bold>a</bold>) the left superior occipital gyrus and (<bold>b</bold>) the right inferior orbital gyrus as seed region. Top brain section: Exemplary MVPA cluster informative of right and left TVA voice-preferentiality used as seed region [see also Fig. <xref rid="Fig2" ref-type="fig">2</xref>b,c]. Bottom brain section: The seed’s convergent informative RSFC patterns regarding individual voice-preferential responses of the right (red) and left (yellow) TVA (orange: overlap of informative patterns for right and left TVA voice-preferentiality) as evaluated post hoc. Results shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05. Coordinates refer to MNI space. The diagrams on the right illustrate the underlying association of voice-preferential responses and RSFC.</p></caption><graphic xlink:href="41598_2022_11367_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par25">In contrast to these results, for the FFAs, in addition to the lower number of informative clusters in the MVPA analysis, the RSFC pattern was largely divergent as exemplarily shown for two MVPA clusters informative of FFA face-selective responses (rFFA: right middle frontal gyrus extending into the precentral gyrus, lFFA: left caudate nucleus and olfactory gyrus). Only one significant common cluster was observed in the right supramarginal gyrus extending into the inferior parietal gyrus (peak: 57x − 27 &lt; 45z; 81 voxels, p(FWE-corr.) = 0.010) using the of the right R middle frontal gyrus/precentral gyrus (rFFA) and the left caudate nucleus and olfactory gyri (− 6 6 − 15, lFFA) as seeds. The results are illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Divergence of informative RSFC patterns for bilateral FFA face-preferentiality. Individual face-preferential responses were assessed using minimum difference criteria (F &gt; max[H, O, S]). Top and bottom middle: Two exemplary MVPA clusters informative of right (green) and left (blue) FFA face-preferentiality used as seed regions [see also Fig. <xref rid="Fig2" ref-type="fig">2</xref>b]. Centre middle: The seeds’ RSFC patterns associated with individual right (green) and left (blue) FFA face-preferentiality as evaluated post hoc. Results shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05. Coordinates refer to MNI space. The diagrams on the right and left sides illustrate the underlying association of face-preferential responses and RSFC.</p></caption><graphic xlink:href="41598_2022_11367_Fig4_HTML" id="MO4"/></fig></p>
      </sec>
      <sec id="Sec13">
        <title>Supramodal convergence of informative RSFC patterns</title>
        <p id="Par26">The combination of RSFC correlates of individual face-preferential responses in the right and left FFA with RSFC correlates of individual voice-preferential responses in the right and left TVA can decipher supramodal convergence of RSFC patterns, i.e. combining voice- and face-preferentiality. In our case, this was evident in eight clusters (Table <xref rid="Tab5" ref-type="table">5</xref>). The convergence was more prominent using right-hemispheric voice- and face-preferentiality regressors with five common clusters, whereas for the left-hemispheric regressors only one supramodal cluster was found. Two clusters derived from regressors of contralateral hemispheres.</p>
        <p id="Par27">Only one region in the anterior region of the rostral mediofrontal cortex (arMFC) exhibited supramodal convergence of informative RSFC patterns for more than two regressors: Convergence of the RSFC of the rlTVA cluster in the left superior occipital gyrus with the lTVA cluster in the left frontal superior gyrus and the rFFA cluster in the right middle frontal gyrus delineated one common region in the medial frontal gyrus (including the left orbital gyrus and the anterior cingulum as well as the right and left medial frontal gyrus, peak: 0 × 54y 9z; 83 voxels, p(FWE-corr.) = 0.011) indicative of right and left TVA voice-preferentiality as well as rFFA face-preferentiality (see also Fig. <xref rid="Fig5" ref-type="fig">5</xref>).<fig id="Fig5"><label>Figure 5</label><caption><p>RSFC patterns informative of voice- and face-preferentiality: supramodal convergence using three regressors. Individual voice- and face-preferential responses were assessed using minimum difference criteria (for voices V &gt; max[A, E], for faces F &gt; max[H, O, S]). Top brain section: MVPA clusters used as seed regions [see also Fig. <xref rid="Fig2" ref-type="fig">2</xref>b,d] with the rTVA (red) and lTVA (yellow; overlapping region in orange) voice-preferentialities, as well as the rFFA face-preferentiality (green) as regressors. Bottom brain section: The seeds’ convergent RSFC patterns regarding individual voice-preferential responses of the right (red) and left (yellow) TVA and face-preferential responses of the right FFA (green), (purple: supramodal overlap). Results shown at a voxel-wise threshold of p &lt; 0.001 and whole brain FWE-corrected at cluster level with a cluster threshold of p &lt; 0.05. Coordinates refer to MNI space. The diagrams on the right illustrate the underlying association of voice-preferential responses and RSFC.</p></caption><graphic xlink:href="41598_2022_11367_Fig5_HTML" id="MO5"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Discussion</title>
      <p id="Par28">Combining seminal experiments used to localize voice- as well as face-preferential areas in the human brain and resting state fMRI, this study provides the first description of hemodynamic functional connectivity patterns in the resting state that are associated with voice- and face-preferential cerebral responses at the primary level of the TVA and FFA.</p>
      <p id="Par29">Using functional connectivity in the resting state, we identified several clusters correlating with voice- and face-preferentiality of the TVA and FFA. For the rFFA one right frontal/precentral cluster was evident, for the lFFA two clusters, one in the left caudate/olfactory gyrus and one in the left superior temporal pole. Using the voice-preferentiality of the rTVA and lTVA as regressors, four common clusters emerged. These were widely distributed the occipital, parietal, frontal and temporal cortex. For the rTVA two additional clusters in the left occipital cortex and the right thalamus, and for the lTVA in the left frontal and right parietal cortex areas emerged. In explanatory seed-to-voxel analyses, the underlying connectivity patterns diverged markedly between the voice and face processing systems. Whereas for the TVAs a largely convergent pattern of clusters was observed, among others in the occipital gyrus and bilateral insulae, the patterns for the FFAs were mainly divergent and yielded only one common region in the right supramarginal gyrus extending into the inferior parietal gyrus.</p>
      <p id="Par30">Moreover, we identified brain areas with RSFC patterns supramodally reflecting preferential responses to both, voices and faces. One area in the anterior rostral mediofrontal cortex (arMFC) displayed a maximum of convergent RSFC patterns: its RSFC was indicative of individual voice-preferential responses of both TVAs and face-preferential responses of the right FFA.</p>
      <p id="Par31">Our results strengthen the view that cerebral voice and face processing is an evolutionary important and therefore highly preserved mechanism, which is not only evidenced by several stages of very specialized processing in the brain, starting with the regions of TVA<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup> and FFA<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>, but is also reflected in other networks, i.e. the resting state network that—per se—work independent from the aforementioned voice and face processing system. Because during resting state participants were asked to lie quiescent without specific thought. But the independence could be impaired, in case the participants would have thought of human voices and faces during the resting state measurement. To minimize this risk, we designed the experimental sequence with the resting state block first followed by the task-related parts.</p>
      <p id="Par32">The finding of a correlation of voice- and face-activation patterns with resting state parameters fits in quite well with the currently still limited literature applying both resting state and voice/face processing measurements. Previous studies found diverging regions either exclusively in the modality-specific processing areas<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>, both in modality specific areas and other parts of the brain<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>, or networks in the inferior frontal gyrus and motor regions which are not directly connected to modality specific processing<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. It needs to be acknowledged however that a broad range of diverse data analysis techniques were used in those studies<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> which may account for the disparities to some extent. Our comprehensive analysis on RSFC networks associated with voice- and face-preferentiality revealed large networks across whole brain, underpinning the notion that response patterns generated in basic voice and face processing modules during the perception of these cues find a reflection in the coactivation of widespread cerebral networks at rest potentially indicating processes connected to voice and face perception or a neural preparedness to respond to these stimuli. Speaking figuratively, the direct responses to stimulation with voices and faces can be imagined as the top of the iceberg, the underlying resting state network structure as the part below the surface of the sea.</p>
      <p id="Par33">It is known from the literature that resting state patterns reflect individual traits. In fact, resting state functional connectivity has been shown to be associated with behavioural tendencies, personality or states of psychiatric disease, e.g. personality traits<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>, moral behaviour<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, violence proneness<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, or the diagnosis of dementia or schizophrenia<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. These results support the view that resting state patterns may reflect an adaptive system indicative of different brain states and function. One could speculate about the connection between basal voice and face processing systems, as assessed in our work, and higher order social functioning (e.g., emotional communication, empathy, theory of mind or moral behaviour), as effective voice and face perception appears as a prerequisite of the former to a certain degree. Certainly, however, this link remains speculative presently.</p>
      <p id="Par34">The novel and distinctive feature of this study is the combination of resting state and stimulation-based fMRI measurements for the visual and the auditory system. The resting state pattern, i.e. a stimulation-free measurement, correlates with the propensity to respond to certain stimuli. Up to now, this form of association has only scarcely been addressed. A similar approach revealed non-state-dependent cerebral markers of biased perception in social anxiety<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Another meta-analytic study focused on similarities in resting state functional connectivity patterns and coactivation network configurations. Using an online database activation patterns of several different tasks were pooled together. A high correlation between coactivation during task and resting-state correlation was detected<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. In patients with first episode schizophrenia overlapping dysfunctions in the prefrontotemporal pathway were evident<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Our study can serve as starting point for further combined analyses of resting state connectivity and activation patterns in stimulation-based designs from a network perspective with a much more precise task design.</p>
      <p id="Par35">Convergent with previous research which provided evidence for a greater functional similarity between the hemispheres in the cerebral voice processing system than the face processing system<sup><xref ref-type="bibr" rid="CR46">46</xref>–<xref ref-type="bibr" rid="CR48">48</xref></sup>, in our study, both TVAs responded to voices in a sensitive (i.e. mixed contrast V &gt; (A, E)) and preferential (i.e. minimum contrast, V &gt; max(A, E)) manner. In contrast, in the face processing system only the right FFA responds both in a sensitive and preferential way to faces, whereas the response of the left FFA is only face-sensitive. We substantiated these results comparing voice- and face-sensitivity and -preferentiality of both hemispheres with lack of hemispheric differences in voice-preferentiality, but significant hemispheric differences in the face processing system with greater face-preferentiality in the right hemisphere. This finding is in line with previous results showing stronger and more consistent activation through faces in comparison to other stimulus categories in the right FFA compared to the left FFA<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>. The dominance of the right hemisphere in face-related responses is not restricted to the FFA, but is also reflected in larger activation areas to faces in the right occipitotemporal cortex and the right amygdala and an exclusive activation of the right inferior frontal gyrus<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Beyond this reliably replicated evidence, we found corresponding patterns in resting state measurements: The resting state patterns predicting the face- and voice-sensitivity/-preferentiality, respectively, differed showing a convergent pattern for the voice processing system and a largely divergent pattern for the face processing system as evidenced by the difference in significant overlaps of the informative connectivity patterns between the TVAs as compared to the FFAs. Thus, we conclude that the different qualities of seeing faces and hearing voices do not work analogously, but that these two systems function in a unique and distinct way, with a higher hemispheric functional similarity of the voice processing modules in comparison to the face processing system.</p>
      <p id="Par36">In our supramodal approach combining voice and face processing networks with three regressors, one common region in the medial frontal cortex correlated both voice- and face-preferentiality during resting state. The medial frontal cortex is known to be activated in higher order social cognitive processing, the anterior rostral part especially in mentalizing tasks<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. Additionally, it is involved in complex emotion processing<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>, independent of the presentation form, e.g. visually via faces or bodies or acoustically via voices<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. The activation of a region related to the processing of stimuli from different sensory modalities gives rise to the problem of interpreting the results. Throughout this manuscript we use the term supramodal for the locally overlapping cerebral activation by signals from different sensory modalities which can be identified using conjunction analyses, e.g. for mapping multisensory integration<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>. Limitations of the technique are that in our case the common region constitutes only a small part in comparison to the complete connectivity pattern from each source, and that the local overlap not mandatorily represents a direct interaction or integration of signals from both sources, but might indicate that the overlap region is simply linked to processing information from several sensory modalities.</p>
      <p id="Par37">While the medial frontal cortex is not consistently activated in stimulation experiments designed to localize voice- or face-specific brain areas, this notion would still appear quite plausible as effective processing of voices and faces might well be required as basis for a variety of higher order social communication functions. In line with this, frontal areas were involved in the processing of incongruent but not congruent audiovisual emotional stimuli<sup><xref ref-type="bibr" rid="CR55">55</xref>,<xref ref-type="bibr" rid="CR56">56</xref></sup> and revealed emotion-specific activation regardless of the sensory modality of the emotional cue<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Whereas many studies assessing higher order social processing employed emotional stimuli<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>, it is quite notable, that we found a convergence in this region even based on experimental designs without explicit emotional connotations. Limitations concerning the assessment of neutral vs. emotional stimuli are discussed below.</p>
      <p id="Par38">This seems to corroborate the notion that higher order social cognitive processes are linked to basic voice and face perception irrespective of emotional information communicated via these stimuli. On the other hand, one might argue that there is no such thing as a voice or a face completely devoid of emotional information in two ways: First, also stimuli not intended to carry emotional information by their sender may well contain subliminal emotional cues and, second, even a putative completely neutral voice or face may automatically be scanned for emotional information and therefore become linked to emotion processing irrespective of its lack of emotional cues. Previous results hint for a variability in the emotional perception of voices and faces depending on the previously experienced sensory input<sup><xref ref-type="bibr" rid="CR57">57</xref>,<xref ref-type="bibr" rid="CR58">58</xref></sup>.</p>
      <p id="Par39">The posterior superior temporal sulcus (pSTS), which has been shown to integrate simultaneously presented auditory and visual stimuli<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, did not show an overlap of connectivity patterns indicating both voice- and face-preferential responses. So, the pSTS’s role in combined face and voice processing might be more closely linked to the sensory integration of these stimuli during their simultaneous perception and thus not be detectable in the resting state.</p>
      <p id="Par40">Our work builds on the manifold confirmed and pioneering findings of regions that are preferentially activated by human stimuli in comparison to environmental cues, i.e. especially the voice-preferential activation of the TVA and the face-preferential activation of the FFA<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>. And it broadens the perspective from specialized regions for different tasks to a network perspective of regions exhibiting preferential responses both during and in the absence of human nonverbal cues. One could speculate that the relevance of this finding lies in the reflection of relevant social situations during resting state, possibly including imagination of nonverbal cues. But to corroborate these ideas, further research is necessary.</p>
      <p id="Par41">The unique quality of our data stems from the combination of these individual cerebral processing characteristics of social stimuli with resting state functional connectivity maps in a relatively large cohort. And it adds to the growing number of findings which advocate a readjustment of our view from specialized regions in the brain responding to certain stimuli to a larger network perspective involving a multitude of regions across the whole brain in the presence and absence of tasks or/and stimuli. The specificity of the activation is mediated not by the activation of single specialized regions itself but by the combination of simultaneously activated networks and therefore strengthens the view of a network perspective.</p>
      <p id="Par42">As gender-specific connectivity patterns were observed e.g. in the correlation of RSFC with personality traits<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, this aspect represents a limitation of the present study which focused on gender- and age-independent connectivity patterns. Due to the limited sample size, we did not perform subgroup analyses. Moreover, we did not assess and therefore were able to correct for personality trait measures, such as the five-factor model of personality, which was shown to be associated with RSFC patterns<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and might therefore also represent a moderator of the RSFC patterns associated with the propensity to respond to human voices and faces.</p>
      <p id="Par43">Although based on seminal standard experiments to assess voice- and face-sensitive and -preferential responses enabling direct comparisons with many previous studies, certain design-specific factors may have influenced the outcome of our study and should therefore be addressed in further research: For one, the task-set differed considerably between the voice and face processing experiments (passive listening vs. one-back task) with potential influence on the attentional status. As a further limitation we would like to address the problem of the assessment of human stimuli as neutral vs. emotional. Though not included in the experiments as explicit factor employing face pictures with predominantly neutral expression, low-level emotional information in the experimental stimuli may have impacted the RSFC patterns predictive of cerebral voice- and face-preferentiality.</p>
      <p id="Par44">As a conclusion, these results emphasize that the individual cerebral propensity to respond to human voices and faces is reflected in the brain’s activation patterns also in the absence of these cues as a possible neural corelate of mental reflections on relevant social situations including imagination of nonverbal cues during “resting” state. The stronger convergence of informative connectivity patterns for the TVAs’ cue selectivity in contrast to the FFAs’ may indicate a higher hemispheric functional similarity of the voice processing modules. The supramodal convergence of such informative connectivity patterns, in turn, points to the anterior medial prefrontal cortex as shared neural resource in supramodal voice and face processing or potentially nonverbal communication. Similar to the underwater perspective on an iceberg, this experimental approach may open up interesting avenues to the investigation of voice and face processing. In this regard, the resting state connectivity patterns correlating with individual voice and face selectivity may aid the understanding of cerebral voice and face preference from a network perspective.</p>
    </sec>
    <sec sec-type="supplementary-material">
      <title>Supplementary Information</title>
      <sec id="Sec15">
        <p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2022_11367_MOESM1_ESM.pdf"><caption><p>Supplementary Figure 1.</p></caption></media></supplementary-material></p>
      </sec>
    </sec>
  </body>
  <back>
    <fn-group>
      <fn>
        <p>
          <bold>Publisher's note</bold>
        </p>
        <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
      </fn>
    </fn-group>
    <sec>
      <title>Supplementary Information</title>
      <p>The online version contains supplementary material available at 10.1038/s41598-022-11367-6.</p>
    </sec>
    <ack>
      <title>Acknowledgements</title>
      <p>This work was supported by the Clinician Scientist program of the University of Tübingen to KNE (Grant Number 367-0-0). We acknowledge support by Open Access Publishing Fund of University of Tübingen. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
    </ack>
    <notes notes-type="author-contribution">
      <title>Author contributions</title>
      <p>The study was conceptualized by D.W., T.E. and B.K. Data acquisition and curation was done by C.B., H.J. and M.E. Analyses were performed by K.N.E., B.K., D.W., T.E., C.B., H.J. and M.E. K.N.E. and B.K. wrote the main manuscript text and prepared the figures. All authors reviewed the manuscript.</p>
    </notes>
    <notes notes-type="funding-information">
      <title>Funding</title>
      <p>Open Access funding enabled and organized by Projekt DEAL.</p>
    </notes>
    <notes notes-type="data-availability">
      <title>Data availability</title>
      <p>The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.</p>
    </notes>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par45">The authors declare no competing interests.</p>
    </notes>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Belin</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>RJ</given-names>
            </name>
            <name>
              <surname>Lafaille</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Ahad</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Pike</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>Voice-selective areas in human auditory cortex</article-title>
          <source>Nature</source>
          <year>2000</year>
          <volume>403</volume>
          <fpage>309</fpage>
          <lpage>312</lpage>
          <pub-id pub-id-type="doi">10.1038/35002078</pub-id>
          <pub-id pub-id-type="pmid">10659849</pub-id>
        </element-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>von Kriegstein</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Giraud</surname>
              <given-names>AL</given-names>
            </name>
          </person-group>
          <article-title>Implicit multisensory associations influence voice recognition</article-title>
          <source>PLoS Biol.</source>
          <year>2006</year>
          <volume>4</volume>
          <fpage>e326</fpage>
          <pub-id pub-id-type="doi">10.1371/journal.pbio.0040326</pub-id>
          <pub-id pub-id-type="pmid">17002519</pub-id>
        </element-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Differential influences of emotion, task, and novelty on brain regions underlying the processing of speech melody</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2009</year>
          <volume>21</volume>
          <fpage>1255</fpage>
          <lpage>1268</lpage>
          <pub-id pub-id-type="doi">10.1162/jocn.2009.21099</pub-id>
          <pub-id pub-id-type="pmid">18752404</pub-id>
        </element-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pernet</surname>
              <given-names>CR</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices</article-title>
          <source>Neuroimage</source>
          <year>2015</year>
          <volume>119</volume>
          <fpage>164</fpage>
          <lpage>174</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.050</pub-id>
          <pub-id pub-id-type="pmid">26116964</pub-id>
        </element-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kanwisher</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>McDermott</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Chun</surname>
              <given-names>MM</given-names>
            </name>
          </person-group>
          <article-title>The fusiform face area: A module in human extrastriate cortex specialized for face perception</article-title>
          <source>J. Neurosci.</source>
          <year>1997</year>
          <volume>17</volume>
          <fpage>4302</fpage>
          <lpage>4311</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id>
          <pub-id pub-id-type="pmid">9151747</pub-id>
        </element-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kanwisher</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Yovel</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>The fusiform face area: A cortical region specialized for the perception of faces</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <year>2006</year>
          <volume>361</volume>
          <fpage>2109</fpage>
          <lpage>2128</lpage>
          <pub-id pub-id-type="doi">10.1098/rstb.2006.1934</pub-id>
          <pub-id pub-id-type="pmid">17118927</pub-id>
        </element-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Posamentier</surname>
              <given-names>MT</given-names>
            </name>
            <name>
              <surname>Abdi</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Processing faces and facial expressions</article-title>
          <source>Neuropsychol. Rev.</source>
          <year>2003</year>
          <volume>13</volume>
          <fpage>113</fpage>
          <lpage>143</lpage>
          <pub-id pub-id-type="doi">10.1023/a:1025519712569</pub-id>
          <pub-id pub-id-type="pmid">14584908</pub-id>
        </element-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Halgren</surname>
              <given-names>E</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Location of human face-selective cortex with respect to retinotopic areas</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1999</year>
          <volume>7</volume>
          <fpage>29</fpage>
          <lpage>37</lpage>
          <pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)7:1&lt;29::AID-HBM3&gt;3.0.CO;2-R</pub-id>
          <pub-id pub-id-type="pmid">9882088</pub-id>
        </element-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Huberle</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Grodd</surname>
              <given-names>W</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Association of trait emotional intelligence and individual fMRI-activation patterns during the perception of social signals from voice and face</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2010</year>
          <volume>31</volume>
          <fpage>979</fpage>
          <lpage>991</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.20913</pub-id>
          <pub-id pub-id-type="pmid">19937724</pub-id>
        </element-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Shiozawa</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Grodd</surname>
              <given-names>W</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Cerebral representation of non-verbal emotional perception: fMRI reveals audiovisual integration area between voice- and face-sensitive regions in the superior temporal sulcus</article-title>
          <source>Neuropsychologia</source>
          <year>2009</year>
          <volume>47</volume>
          <fpage>3059</fpage>
          <lpage>3066</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.07.001</pub-id>
          <pub-id pub-id-type="pmid">19596021</pub-id>
        </element-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Robins</surname>
              <given-names>DL</given-names>
            </name>
            <name>
              <surname>Hunyadi</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Schultz</surname>
              <given-names>RT</given-names>
            </name>
          </person-group>
          <article-title>Superior temporal activation in response to dynamic audio-visual emotional cues</article-title>
          <source>Brain Cogn.</source>
          <year>2009</year>
          <volume>69</volume>
          <fpage>269</fpage>
          <lpage>278</lpage>
          <pub-id pub-id-type="doi">10.1016/j.bandc.2008.08.007</pub-id>
          <pub-id pub-id-type="pmid">18809234</pub-id>
        </element-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Pourtois</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Investigating audiovisual integration of emotional signals in the human brain</article-title>
          <source>Prog. Brain Res.</source>
          <year>2006</year>
          <volume>156</volume>
          <fpage>345</fpage>
          <lpage>361</lpage>
          <pub-id pub-id-type="doi">10.1016/S0079-6123(06)56019-4</pub-id>
          <pub-id pub-id-type="pmid">17015090</pub-id>
        </element-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Klasen</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Kenworthy</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Mathiak</surname>
              <given-names>KA</given-names>
            </name>
            <name>
              <surname>Kircher</surname>
              <given-names>TT</given-names>
            </name>
            <name>
              <surname>Mathiak</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>Supramodal representation of emotions</article-title>
          <source>J. Neurosci.</source>
          <year>2011</year>
          <volume>31</volume>
          <fpage>13635</fpage>
          <lpage>13643</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2833-11.2011</pub-id>
          <pub-id pub-id-type="pmid">21940454</pub-id>
        </element-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Functional responses and structural connections of cortical areas for processing faces and voices in the superior temporal sulcus</article-title>
          <source>Neuroimage</source>
          <year>2013</year>
          <volume>76</volume>
          <fpage>45</fpage>
          <lpage>56</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.02.064</pub-id>
          <pub-id pub-id-type="pmid">23507387</pub-id>
        </element-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ethofer</surname>
              <given-names>T</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Cerebral pathways in processing of affective prosody: A dynamic causal modeling study</article-title>
          <source>Neuroimage</source>
          <year>2006</year>
          <volume>30</volume>
          <fpage>580</fpage>
          <lpage>587</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.09.059</pub-id>
          <pub-id pub-id-type="pmid">16275138</pub-id>
        </element-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Tuned to voices and faces: Cerebral responses linked to social anxiety</article-title>
          <source>Neuroimage</source>
          <year>2019</year>
          <volume>197</volume>
          <fpage>450</fpage>
          <lpage>456</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.018</pub-id>
          <pub-id pub-id-type="pmid">31075391</pub-id>
        </element-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Karle</surname>
              <given-names>KN</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Neurobiological correlates of emotional intelligence in voice and face perception networks</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2018</year>
          <volume>13</volume>
          <fpage>233</fpage>
          <lpage>244</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nsy001</pub-id>
          <pub-id pub-id-type="pmid">29365199</pub-id>
        </element-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smitha</surname>
              <given-names>KA</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Resting state fMRI: A review on methods in resting state connectivity analysis and resting state networks</article-title>
          <source>Neuroradiol. J.</source>
          <year>2017</year>
          <volume>30</volume>
          <fpage>305</fpage>
          <lpage>317</lpage>
          <pub-id pub-id-type="doi">10.1177/1971400917697342</pub-id>
          <pub-id pub-id-type="pmid">28353416</pub-id>
        </element-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hahn</surname>
              <given-names>T</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Reliance on functional resting-state network for stable task control predicts behavioral tendency for cooperation</article-title>
          <source>Neuroimage</source>
          <year>2015</year>
          <volume>118</volume>
          <fpage>231</fpage>
          <lpage>236</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.05.093</pub-id>
          <pub-id pub-id-type="pmid">26070266</pub-id>
        </element-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Inagaki</surname>
              <given-names>TK</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>ML</given-names>
            </name>
          </person-group>
          <article-title>Individual differences in resting-state connectivity and giving social support: Implications for health</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2020</year>
          <volume>15</volume>
          <fpage>1076</fpage>
          <lpage>1085</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nsz052</pub-id>
          <pub-id pub-id-type="pmid">31269205</pub-id>
        </element-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Serafini</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Pardini</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Pompili</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Girardi</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Amore</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Understanding suicidal behavior: The contribution of recent resting-state fMRI techniques</article-title>
          <source>Front. Psychiatry</source>
          <year>2016</year>
          <volume>7</volume>
          <fpage>69</fpage>
          <pub-id pub-id-type="doi">10.3389/fpsyt.2016.00069</pub-id>
          <pub-id pub-id-type="pmid">27148097</pub-id>
        </element-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nostro</surname>
              <given-names>AD</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Predicting personality from network-based resting-state functional connectivity</article-title>
          <source>Brain Struct. Funct.</source>
          <year>2018</year>
          <volume>223</volume>
          <fpage>2699</fpage>
          <lpage>2719</lpage>
          <pub-id pub-id-type="doi">10.1007/s00429-018-1651-z</pub-id>
          <pub-id pub-id-type="pmid">29572625</pub-id>
        </element-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Markett</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Montag</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Reuter</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Network neuroscience and personality</article-title>
          <source>Pers. Neurosci.</source>
          <year>2018</year>
          <volume>1</volume>
          <fpage>e14</fpage>
          <pub-id pub-id-type="doi">10.1017/pen.2018.12</pub-id>
        </element-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Parkes</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Satterthwaite</surname>
              <given-names>TD</given-names>
            </name>
            <name>
              <surname>Bassett</surname>
              <given-names>DS</given-names>
            </name>
          </person-group>
          <article-title>Towards precise resting-state fMRI biomarkers in psychiatry: Synthesizing developments in transdiagnostic research, dimensional models of psychopathology, and normative neurodevelopment</article-title>
          <source>Curr. Opin. Neurobiol.</source>
          <year>2020</year>
          <volume>65</volume>
          <fpage>120</fpage>
          <lpage>128</lpage>
          <pub-id pub-id-type="doi">10.1016/j.conb.2020.10.016</pub-id>
          <pub-id pub-id-type="pmid">33242721</pub-id>
        </element-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Romero-Martinez</surname>
              <given-names>A</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The brain resting-state functional connectivity underlying violence proneness: Is it a reliable marker for neurocriminology? A systematic review</article-title>
          <source>Behav. Sci. (Basel).</source>
          <year>2019</year>
          <pub-id pub-id-type="doi">10.3390/bs9010011</pub-id>
          <pub-id pub-id-type="pmid">31126061</pub-id>
        </element-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van den Heuvel</surname>
              <given-names>MP</given-names>
            </name>
            <name>
              <surname>Hulshoff Pol</surname>
              <given-names>HE</given-names>
            </name>
          </person-group>
          <article-title>Exploring the brain network: A review on resting-state fMRI functional connectivity</article-title>
          <source>Eur. Neuropsychopharmacol.</source>
          <year>2010</year>
          <volume>20</volume>
          <fpage>519</fpage>
          <lpage>534</lpage>
          <pub-id pub-id-type="doi">10.1016/j.euroneuro.2010.03.008</pub-id>
          <pub-id pub-id-type="pmid">20471808</pub-id>
        </element-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>Intrinsically organized network for face perception during the resting state</article-title>
          <source>Neurosci. Lett.</source>
          <year>2009</year>
          <volume>454</volume>
          <fpage>1</fpage>
          <lpage>5</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neulet.2009.02.054</pub-id>
          <pub-id pub-id-type="pmid">19429043</pub-id>
        </element-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>O'Neil</surname>
              <given-names>EB</given-names>
            </name>
            <name>
              <surname>Hutchison</surname>
              <given-names>RM</given-names>
            </name>
            <name>
              <surname>McLean</surname>
              <given-names>DA</given-names>
            </name>
            <name>
              <surname>Kohler</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Resting-state fMRI reveals functional connectivity between face-selective perirhinal cortex and the fusiform face area related to face inversion</article-title>
          <source>Neuroimage</source>
          <year>2014</year>
          <volume>92</volume>
          <fpage>349</fpage>
          <lpage>355</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.02.005</pub-id>
          <pub-id pub-id-type="pmid">24531049</pub-id>
        </element-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kruschwitz</surname>
              <given-names>JD</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Segregation of face sensitive areas within the fusiform gyrus using global signal regression? A study on amygdala resting-state functional connectivity</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2015</year>
          <volume>36</volume>
          <fpage>4089</fpage>
          <lpage>4103</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.22900</pub-id>
          <pub-id pub-id-type="pmid">26178527</pub-id>
        </element-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Correia</surname>
              <given-names>AI</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Resting-state connectivity reveals a role for sensorimotor systems in vocal emotional processing in children</article-title>
          <source>Neuroimage</source>
          <year>2019</year>
          <volume>201</volume>
          <fpage>116052</fpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116052</pub-id>
          <pub-id pub-id-type="pmid">31351162</pub-id>
        </element-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Oldfield</surname>
              <given-names>RC</given-names>
            </name>
          </person-group>
          <article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title>
          <source>Neuropsychologia</source>
          <year>1971</year>
          <volume>9</volume>
          <fpage>97</fpage>
          <lpage>113</lpage>
          <pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>
          <pub-id pub-id-type="pmid">5146491</pub-id>
        </element-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Non-verbal emotion communication training induces specific changes in brain function and structure</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2013</year>
          <volume>7</volume>
          <fpage>648</fpage>
          <pub-id pub-id-type="doi">10.3389/fnhum.2013.00648</pub-id>
          <pub-id pub-id-type="pmid">24146641</pub-id>
        </element-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>The neural correlates of face-voice-integration in social anxiety disorder</article-title>
          <source>Front. Psychiatry</source>
          <year>2020</year>
          <volume>11</volume>
          <fpage>657</fpage>
          <pub-id pub-id-type="doi">10.3389/fpsyt.2020.00657</pub-id>
          <pub-id pub-id-type="pmid">32765311</pub-id>
        </element-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Collins</surname>
              <given-names>DL</given-names>
            </name>
            <name>
              <surname>Neelin</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Peters</surname>
              <given-names>TM</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>AC</given-names>
            </name>
          </person-group>
          <article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title>
          <source>J. Comput. Assist. Tomogr</source>
          <year>1994</year>
          <volume>18</volume>
          <fpage>192</fpage>
          <lpage>205</lpage>
          <pub-id pub-id-type="doi">10.1097/00004728-199403000-00005</pub-id>
          <pub-id pub-id-type="pmid">8126267</pub-id>
        </element-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>KJ</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Classical and Bayesian inference in neuroimaging: Applications</article-title>
          <source>Neuroimage</source>
          <year>2002</year>
          <volume>16</volume>
          <fpage>484</fpage>
          <lpage>512</lpage>
          <pub-id pub-id-type="doi">10.1006/nimg.2002.1091</pub-id>
          <pub-id pub-id-type="pmid">12030833</pub-id>
        </element-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Worsley</surname>
              <given-names>KJ</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>A unified statistical approach for determining significant signals in images of cerebral activation</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>1996</year>
          <volume>4</volume>
          <fpage>58</fpage>
          <lpage>73</lpage>
          <pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1996)4:1&lt;58::AID-HBM4&gt;3.0.CO;2-O</pub-id>
          <pub-id pub-id-type="pmid">20408186</pub-id>
        </element-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Cerebral resting state markers of biased perception in social anxiety</article-title>
          <source>Brain Struct. Funct.</source>
          <year>2019</year>
          <volume>224</volume>
          <fpage>759</fpage>
          <lpage>777</lpage>
          <pub-id pub-id-type="doi">10.1007/s00429-018-1803-1</pub-id>
          <pub-id pub-id-type="pmid">30506458</pub-id>
        </element-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Whitfield-Gabrieli</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Nieto-Castanon</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Conn: A functional connectivity toolbox for correlated and anticorrelated brain networks</article-title>
          <source>Brain Connect</source>
          <year>2012</year>
          <volume>2</volume>
          <fpage>125</fpage>
          <lpage>141</lpage>
          <pub-id pub-id-type="doi">10.1089/brain.2012.0073</pub-id>
          <pub-id pub-id-type="pmid">22642651</pub-id>
        </element-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tzourio-Mazoyer</surname>
              <given-names>N</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>
          <source>Neuroimage</source>
          <year>2002</year>
          <volume>15</volume>
          <fpage>273</fpage>
          <lpage>289</lpage>
          <pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id>
          <pub-id pub-id-type="pmid">11771995</pub-id>
        </element-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <mixed-citation publication-type="other">Whitfield-Gabrieli, S. N.-C. A. <italic>CONN Toolbox Manual</italic>. <ext-link ext-link-type="uri" xlink:href="https://web.conn-toolbox.org/resources/documentation/manual">https://web.conn-toolbox.org/resources/documentation/manual</ext-link> (2017). Accessed 23 Aug 2017.</mixed-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nichols</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Brett</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Andersson</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wager</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Poline</surname>
              <given-names>JB</given-names>
            </name>
          </person-group>
          <article-title>Valid conjunction inference with the minimum statistic</article-title>
          <source>Neuroimage</source>
          <year>2005</year>
          <volume>25</volume>
          <fpage>653</fpage>
          <lpage>660</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.005</pub-id>
          <pub-id pub-id-type="pmid">15808966</pub-id>
        </element-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>Q</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>YL</given-names>
            </name>
            <name>
              <surname>Dilks</surname>
              <given-names>DD</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Resting-state neural activity across face-selective cortical regions is behaviorally relevant</article-title>
          <source>J. Neurosci.</source>
          <year>2011</year>
          <volume>31</volume>
          <fpage>10323</fpage>
          <lpage>10330</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0873-11.2011</pub-id>
          <pub-id pub-id-type="pmid">21753009</pub-id>
        </element-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>YJ</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <article-title>Intrinsic functional connectivity of medial prefrontal cortex predicts the individual moral bias in economic valuation partially through the moral sensitivity trait</article-title>
          <source>Brain Imaging Behav.</source>
          <year>2020</year>
          <volume>14</volume>
          <fpage>2024</fpage>
          <lpage>2036</lpage>
          <pub-id pub-id-type="doi">10.1007/s11682-019-00152-1</pub-id>
          <pub-id pub-id-type="pmid">31250264</pub-id>
        </element-citation>
      </ref>
      <ref id="CR44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Di</surname>
              <given-names>X</given-names>
            </name>
            <name>
              <surname>Gohel</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>EH</given-names>
            </name>
            <name>
              <surname>Biswal</surname>
              <given-names>BB</given-names>
            </name>
          </person-group>
          <article-title>Task vs. rest-different network configurations between the coactivation and the resting-state brain networks</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2013</year>
          <volume>7</volume>
          <fpage>493</fpage>
          <pub-id pub-id-type="doi">10.3389/fnhum.2013.00493</pub-id>
          <pub-id pub-id-type="pmid">24062654</pub-id>
        </element-citation>
      </ref>
      <ref id="CR45">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mwansisya</surname>
              <given-names>TE</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Task and resting-state fMRI studies in first-episode schizophrenia: A systematic review</article-title>
          <source>Schizophr. Res.</source>
          <year>2017</year>
          <volume>189</volume>
          <fpage>9</fpage>
          <lpage>18</lpage>
          <pub-id pub-id-type="doi">10.1016/j.schres.2017.02.026</pub-id>
          <pub-id pub-id-type="pmid">28268041</pub-id>
        </element-citation>
      </ref>
      <ref id="CR46">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Engell</surname>
              <given-names>AD</given-names>
            </name>
            <name>
              <surname>McCarthy</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Probabilistic atlases for face and biological motion perception: An analysis of their reliability and overlap</article-title>
          <source>Neuroimage</source>
          <year>2013</year>
          <volume>74</volume>
          <fpage>140</fpage>
          <lpage>151</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.02.025</pub-id>
          <pub-id pub-id-type="pmid">23435213</pub-id>
        </element-citation>
      </ref>
      <ref id="CR47">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brancucci</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Lucci</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Mazzatenta</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Tommasi</surname>
              <given-names>L</given-names>
            </name>
          </person-group>
          <article-title>Asymmetries of the human social brain in the visual, auditory and chemical modalities</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <year>2009</year>
          <volume>364</volume>
          <fpage>895</fpage>
          <lpage>914</lpage>
          <pub-id pub-id-type="doi">10.1098/rstb.2008.0279</pub-id>
          <pub-id pub-id-type="pmid">19064350</pub-id>
        </element-citation>
      </ref>
      <ref id="CR48">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bonte</surname>
              <given-names>M</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Development from childhood to adulthood increases morphological and functional inter-individual variability in the right superior temporal cortex</article-title>
          <source>Neuroimage</source>
          <year>2013</year>
          <volume>83</volume>
          <fpage>739</fpage>
          <lpage>750</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.07.017</pub-id>
          <pub-id pub-id-type="pmid">23867553</pub-id>
        </element-citation>
      </ref>
      <ref id="CR49">
        <label>49.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarz</surname>
              <given-names>L</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Properties of face localizer activations and their application in functional magnetic resonance imaging (fMRI) fingerprinting</article-title>
          <source>PLoS ONE</source>
          <year>2019</year>
          <volume>14</volume>
          <fpage>e0214997</fpage>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0214997</pub-id>
          <pub-id pub-id-type="pmid">31013276</pub-id>
        </element-citation>
      </ref>
      <ref id="CR50">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Amodio</surname>
              <given-names>DM</given-names>
            </name>
            <name>
              <surname>Frith</surname>
              <given-names>CD</given-names>
            </name>
          </person-group>
          <article-title>Meeting of minds: The medial frontal cortex and social cognition</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2006</year>
          <volume>7</volume>
          <fpage>268</fpage>
          <lpage>277</lpage>
          <pub-id pub-id-type="doi">10.1038/nrn1884</pub-id>
          <pub-id pub-id-type="pmid">16552413</pub-id>
        </element-citation>
      </ref>
      <ref id="CR51">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wildgruber</surname>
              <given-names>D</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Different types of laughter modulate connectivity within distinct parts of the laughter perception network</article-title>
          <source>PLoS ONE</source>
          <year>2013</year>
          <volume>8</volume>
          <fpage>e63441</fpage>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0063441</pub-id>
          <pub-id pub-id-type="pmid">23667619</pub-id>
        </element-citation>
      </ref>
      <ref id="CR52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bruck</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Kreifelts</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Gossling-Arnold</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Wertheimer</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wildgruber</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>'Inner voices': The cerebral representation of emotional voice cues described in literary texts</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2014</year>
          <volume>9</volume>
          <fpage>1819</fpage>
          <lpage>1827</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nst180</pub-id>
          <pub-id pub-id-type="pmid">24396008</pub-id>
        </element-citation>
      </ref>
      <ref id="CR53">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peelen</surname>
              <given-names>MV</given-names>
            </name>
            <name>
              <surname>Atkinson</surname>
              <given-names>AP</given-names>
            </name>
            <name>
              <surname>Vuilleumier</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Supramodal representations of perceived emotions in the human brain</article-title>
          <source>J. Neurosci.</source>
          <year>2010</year>
          <volume>30</volume>
          <fpage>10127</fpage>
          <lpage>10134</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2161-10.2010</pub-id>
          <pub-id pub-id-type="pmid">20668196</pub-id>
        </element-citation>
      </ref>
      <ref id="CR54">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>KJ</given-names>
            </name>
            <name>
              <surname>Penny</surname>
              <given-names>WD</given-names>
            </name>
            <name>
              <surname>Glaser</surname>
              <given-names>DE</given-names>
            </name>
          </person-group>
          <article-title>Conjunction revisited</article-title>
          <source>Neuroimage</source>
          <year>2005</year>
          <volume>25</volume>
          <fpage>661</fpage>
          <lpage>667</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.01.013</pub-id>
          <pub-id pub-id-type="pmid">15808967</pub-id>
        </element-citation>
      </ref>
      <ref id="CR55">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Muller</surname>
              <given-names>VI</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Incongruence effects in crossmodal emotional integration</article-title>
          <source>Neuroimage</source>
          <year>2011</year>
          <volume>54</volume>
          <fpage>2257</fpage>
          <lpage>2266</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.10.047</pub-id>
          <pub-id pub-id-type="pmid">20974266</pub-id>
        </element-citation>
      </ref>
      <ref id="CR56">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Davies-Thompson</surname>
              <given-names>J</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Hierarchical brain network for face and voice integration of emotion expression</article-title>
          <source>Cereb. Cortex</source>
          <year>2019</year>
          <volume>29</volume>
          <fpage>3590</fpage>
          <lpage>3605</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhy240</pub-id>
          <pub-id pub-id-type="pmid">30272134</pub-id>
        </element-citation>
      </ref>
      <ref id="CR57">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Webster</surname>
              <given-names>MA</given-names>
            </name>
            <name>
              <surname>MacLeod</surname>
              <given-names>DI</given-names>
            </name>
          </person-group>
          <article-title>Visual adaptation and face perception</article-title>
          <source>Philos. Trans. R. Soc. Lond. B Biol. Sci.</source>
          <year>2011</year>
          <volume>366</volume>
          <fpage>1702</fpage>
          <lpage>1725</lpage>
          <pub-id pub-id-type="doi">10.1098/rstb.2010.0360</pub-id>
          <pub-id pub-id-type="pmid">21536555</pub-id>
        </element-citation>
      </ref>
      <ref id="CR58">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bestelmeyer</surname>
              <given-names>PE</given-names>
            </name>
            <name>
              <surname>Rouger</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>DeBruine</surname>
              <given-names>LM</given-names>
            </name>
            <name>
              <surname>Belin</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Auditory adaptation in vocal affect perception</article-title>
          <source>Cognition</source>
          <year>2010</year>
          <volume>117</volume>
          <fpage>217</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cognition.2010.08.008</pub-id>
          <pub-id pub-id-type="pmid">20804977</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>
