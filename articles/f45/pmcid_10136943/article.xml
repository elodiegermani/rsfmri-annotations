<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Brain Sci</journal-id>
      <journal-id journal-id-type="iso-abbrev">Brain Sci</journal-id>
      <journal-id journal-id-type="publisher-id">brainsci</journal-id>
      <journal-title-group>
        <journal-title>Brain Sciences</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2076-3425</issn>
      <publisher>
        <publisher-name>MDPI</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">37190601</article-id>
      <article-id pub-id-type="pmc">10136943</article-id>
      <article-id pub-id-type="doi">10.3390/brainsci13040636</article-id>
      <article-id pub-id-type="publisher-id">brainsci-13-00636</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Visual Deprivation Alters Functional Connectivity of Neural Networks for Voice Recognition: A Resting-State fMRI Study</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9243-322X</contrib-id>
          <name>
            <surname>Pang</surname>
            <given-names>Wenbin</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role>
          <xref rid="af1-brainsci-13-00636" ref-type="aff">1</xref>
          <xref rid="af2-brainsci-13-00636" ref-type="aff">2</xref>
          <xref rid="fn1-brainsci-13-00636" ref-type="author-notes">†</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Wei</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
          <xref rid="af3-brainsci-13-00636" ref-type="aff">3</xref>
          <xref rid="fn1-brainsci-13-00636" ref-type="author-notes">†</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Ruan</surname>
            <given-names>Yufang</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
          <xref rid="af4-brainsci-13-00636" ref-type="aff">4</xref>
          <xref rid="af5-brainsci-13-00636" ref-type="aff">5</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Linjun</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
          <xref rid="af6-brainsci-13-00636" ref-type="aff">6</xref>
          <xref rid="c1-brainsci-13-00636" ref-type="corresp">*</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Shu</surname>
            <given-names>Hua</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role>
          <xref rid="af7-brainsci-13-00636" ref-type="aff">7</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6777-3487</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
          <xref rid="af8-brainsci-13-00636" ref-type="aff">8</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Yumei</given-names>
          </name>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources/">Resources</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role>
          <role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
          <xref rid="af2-brainsci-13-00636" ref-type="aff">2</xref>
          <xref rid="af9-brainsci-13-00636" ref-type="aff">9</xref>
          <xref rid="c1-brainsci-13-00636" ref-type="corresp">*</xref>
        </contrib>
      </contrib-group>
      <contrib-group>
        <contrib contrib-type="editor">
          <name>
            <surname>Gainotti</surname>
            <given-names>Guido</given-names>
          </name>
          <role>Academic Editor</role>
        </contrib>
        <contrib contrib-type="editor">
          <name>
            <surname>Pegna</surname>
            <given-names>Alan</given-names>
          </name>
          <role>Academic Editor</role>
        </contrib>
      </contrib-group>
      <aff id="af1-brainsci-13-00636"><label>1</label>Department of Neurology, Beijing Tiantan Hospital, Capital Medical University, Beijing 100070, China; <email>pangwenbin623@gmail.com</email></aff>
      <aff id="af2-brainsci-13-00636"><label>2</label>China National Clinical Research Center for Neurological Diseases, Beijing 100070, China</aff>
      <aff id="af3-brainsci-13-00636"><label>3</label>Beijing Key Lab of Learning and Cognition, School of Psychology, Capital Normal University, Beijing 100048, China</aff>
      <aff id="af4-brainsci-13-00636"><label>4</label>School of Communication Sciences and Disorders, Faculty of Medicine and Health Sciences, McGill University, Montréal, QC H3A 1G1, Canada</aff>
      <aff id="af5-brainsci-13-00636"><label>5</label>Centre for Research on Brain, Language and Music, Montréal, QC H3A 1G1, Canada</aff>
      <aff id="af6-brainsci-13-00636"><label>6</label>School of Chinese as a Second Language, Peking University, Beijing 100871, China</aff>
      <aff id="af7-brainsci-13-00636"><label>7</label>State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, China</aff>
      <aff id="af8-brainsci-13-00636"><label>8</label>Department of Speech-Language-Hearing Sciences and Center for Neurobehavioral Development, The University of Minnesota, Minneapolis, MN 55455, USA</aff>
      <aff id="af9-brainsci-13-00636"><label>9</label>Department of Rehabilitation, Beijing Tiantan Hospital, Capital Medical University, Beijing 100070, China</aff>
      <author-notes>
        <corresp id="c1-brainsci-13-00636"><label>*</label>Correspondence: <email>zhanglinjun75@gmail.com</email> (L.Z.); <email>zhangyumei95@aliyun.com</email> (Y.Z.)</corresp>
        <fn id="fn1-brainsci-13-00636">
          <label>†</label>
          <p>These authors contributed equally to this work.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="epub">
        <day>07</day>
        <month>4</month>
        <year>2023</year>
      </pub-date>
      <pub-date pub-type="collection">
        <month>4</month>
        <year>2023</year>
      </pub-date>
      <volume>13</volume>
      <issue>4</issue>
      <elocation-id>636</elocation-id>
      <history>
        <date date-type="received">
          <day>12</day>
          <month>2</month>
          <year>2023</year>
        </date>
        <date date-type="rev-recd">
          <day>29</day>
          <month>3</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>04</day>
          <month>4</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2023 by the authors.</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Humans recognize one another by identifying their voices and faces. For sighted people, the integration of voice and face signals in corresponding brain networks plays an important role in facilitating the process. However, individuals with vision loss primarily resort to voice cues to recognize a person’s identity. It remains unclear how the neural systems for voice recognition reorganize in the blind. In the present study, we collected behavioral and resting-state fMRI data from 20 early blind (5 females; mean age = 22.6 years) and 22 sighted control (7 females; mean age = 23.7 years) individuals. We aimed to investigate the alterations in the resting-state functional connectivity (FC) among the voice- and face-sensitive areas in blind subjects in comparison with controls. We found that the intranetwork connections among voice-sensitive areas, including amygdala-posterior “temporal voice areas” (TVAp), amygdala-anterior “temporal voice areas” (TVAa), and amygdala-inferior frontal gyrus (IFG) were enhanced in the early blind. The blind group also showed increased FCs of “fusiform face area” (FFA)-IFG and “occipital face area” (OFA)-IFG but decreased FCs between the face-sensitive areas (i.e., FFA and OFA) and TVAa. Moreover, the voice-recognition accuracy was positively related to the strength of TVAp-FFA in the sighted, and the strength of amygdala-FFA in the blind. These findings indicate that visual deprivation shapes functional connectivity by increasing the intranetwork connections among voice-sensitive areas while decreasing the internetwork connections between the voice- and face-sensitive areas. Moreover, the face-sensitive areas are still involved in the voice-recognition process in blind individuals through pathways such as the subcortical-occipital or occipitofrontal connections, which may benefit the visually impaired greatly during voice processing.</p>
      </abstract>
      <kwd-group>
        <kwd>voice recognition</kwd>
        <kwd>face recognition</kwd>
        <kwd>early blind</kwd>
        <kwd>functional connectivity</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>National Key Technology Research and Development Program of China</funding-source>
          <award-id>2020YFC2004102</award-id>
        </award-group>
        <award-group>
          <funding-source>National Natural Science Foundation of China</funding-source>
          <award-id>81972144</award-id>
          <award-id>81972148</award-id>
        </award-group>
        <award-group>
          <funding-source>Disciplinary Construction Project of Peking University</funding-source>
          <award-id>7100603842</award-id>
        </award-group>
        <award-group>
          <funding-source>Collaborative Research Fund of Chinese Institute for Brain Research</funding-source>
          <award-id>2021-NKX-XM-05</award-id>
        </award-group>
        <funding-statement>This research was funded by the National Key Technology Research and Development Program of China (2020YFC2004102), the National Natural Science Foundation of China (NSFC: 81972144, 81972148), the Disciplinary Construction Project of Peking University (7100603842), and the Collaborative Research Fund of Chinese Institute for Brain Research (2021-NKX-XM-05).</funding-statement>
      </funding-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro" id="sec1-brainsci-13-00636">
      <title>1. Introduction</title>
      <p>Humans recognize a person’s identity primarily by their face and voice. Functional magnetic resonance imaging (fMRI) studies in human and nonhuman primates have revealed a set of cortical areas specialized for face processing [<xref rid="B1-brainsci-13-00636" ref-type="bibr">1</xref>,<xref rid="B2-brainsci-13-00636" ref-type="bibr">2</xref>,<xref rid="B3-brainsci-13-00636" ref-type="bibr">3</xref>,<xref rid="B4-brainsci-13-00636" ref-type="bibr">4</xref>,<xref rid="B5-brainsci-13-00636" ref-type="bibr">5</xref>]. Face-sensitive areas of humans are most selectively and reliably located in two regions, namely, the fusiform face area (FFA) [<xref rid="B6-brainsci-13-00636" ref-type="bibr">6</xref>,<xref rid="B7-brainsci-13-00636" ref-type="bibr">7</xref>,<xref rid="B8-brainsci-13-00636" ref-type="bibr">8</xref>], which is predominantly implicated in face-identity recognition, and the occipital face area (OFA), which is primarily involved in sensory information representation [<xref rid="B5-brainsci-13-00636" ref-type="bibr">5</xref>,<xref rid="B9-brainsci-13-00636" ref-type="bibr">9</xref>,<xref rid="B10-brainsci-13-00636" ref-type="bibr">10</xref>]. In analogy to face processing, voice-sensitive regions are localized in the superior temporal gyrus/sulcus (STG/STS), particularly on the right side [<xref rid="B11-brainsci-13-00636" ref-type="bibr">11</xref>,<xref rid="B12-brainsci-13-00636" ref-type="bibr">12</xref>,<xref rid="B13-brainsci-13-00636" ref-type="bibr">13</xref>] with the anterior STG/STS, which is closely related to voice identity perception, and the posterior STG/STS, which is more involved in the processing of acoustic properties of voices [<xref rid="B14-brainsci-13-00636" ref-type="bibr">14</xref>,<xref rid="B15-brainsci-13-00636" ref-type="bibr">15</xref>,<xref rid="B16-brainsci-13-00636" ref-type="bibr">16</xref>]. In addition, extratemporal regions, including the amygdala and prefrontal regions, are also involved in voice recognition [<xref rid="B15-brainsci-13-00636" ref-type="bibr">15</xref>,<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>,<xref rid="B18-brainsci-13-00636" ref-type="bibr">18</xref>].</p>
      <p>In a phone call, the moment we recognize an acquaintance by the voice, we can also recall the face. There is accumulating evidence supporting the crossmodal interaction of facial and vocal information during speaker recognition [<xref rid="B19-brainsci-13-00636" ref-type="bibr">19</xref>,<xref rid="B20-brainsci-13-00636" ref-type="bibr">20</xref>,<xref rid="B21-brainsci-13-00636" ref-type="bibr">21</xref>,<xref rid="B22-brainsci-13-00636" ref-type="bibr">22</xref>]. Specifically, there are direct functional and structural links between the temporal voice areas (TVA) and FFA [<xref rid="B23-brainsci-13-00636" ref-type="bibr">23</xref>,<xref rid="B24-brainsci-13-00636" ref-type="bibr">24</xref>,<xref rid="B25-brainsci-13-00636" ref-type="bibr">25</xref>]. These findings suggest that our brain integrates multisensory information to recognize a person. The two most important mechanisms for determining personal identity, i.e., voice and face recognition, are not isolated from each other [<xref rid="B26-brainsci-13-00636" ref-type="bibr">26</xref>,<xref rid="B27-brainsci-13-00636" ref-type="bibr">27</xref>,<xref rid="B28-brainsci-13-00636" ref-type="bibr">28</xref>,<xref rid="B29-brainsci-13-00636" ref-type="bibr">29</xref>]. Some researchers emphasized the similarities in the cognitive and neural mechanisms underlying face and voice perception (for a review, see [<xref rid="B28-brainsci-13-00636" ref-type="bibr">28</xref>]) based on the “metamodal” principle of brain organization [<xref rid="B30-brainsci-13-00636" ref-type="bibr">30</xref>]. According to the “metamodal” principle, the human brain is organized based on assigned functions or computations regardless of sensory input modality [<xref rid="B31-brainsci-13-00636" ref-type="bibr">31</xref>,<xref rid="B32-brainsci-13-00636" ref-type="bibr">32</xref>]. The distinct voice and face perception systems, therefore, may share similar computational mechanisms in support of identity recognition across auditory and visual modalities.</p>
      <p>People who lost vision in their early life provide researchers with an exceptional opportunity to investigate the “metamodal” hypothesis for identity recognition. For instance, an fMRI study in congenitally blind individuals observed increased activation in STS and FFA for the “vocal versus nonvocal” condition when compared with sighted controls [<xref rid="B33-brainsci-13-00636" ref-type="bibr">33</xref>]. Enhanced activation in FFA was also found specifically as early blind participants responded to normal voices relative to scrambled voices [<xref rid="B34-brainsci-13-00636" ref-type="bibr">34</xref>], and congenitally blind participants responded to person–voice incongruent stimuli relative to person–voice congruent stimuli [<xref rid="B35-brainsci-13-00636" ref-type="bibr">35</xref>]. Neuroimaging studies on affective information perception found that emotions conveyed by voices could be decoded in the face-sensitive areas including right FFA in the blind [<xref rid="B36-brainsci-13-00636" ref-type="bibr">36</xref>]. In addition, increased right amygdala activation to emotional voices was found in congenitally blind individuals compared with sighted participants [<xref rid="B37-brainsci-13-00636" ref-type="bibr">37</xref>]. These task-based neuroimaging findings provide substantial evidence for the plastic changes that occurred in both the voice and face systems during voice processing induced by vision loss, indicating that the involvement of FFA in identity recognition does not necessarily rely on visual input.</p>
      <p>Several interesting issues concerning the reorganization of neural mechanisms of voice recognition in the blind remain to be further clarified. For instance, does visual deprivation reshape the internal links within the intact voice system and disrupted face system? More importantly, do the interactions between the voice and face systems retain or cease to exist in blind individuals? Lifelong blindness presumably involves behavioral adaptations, which may lead to enhanced auditory memory and attention. Several resting-state fMRI (rs-fMRI) studies have demonstrated extensive alterations of brain functional connectivity (FC) after visual deprivation [<xref rid="B38-brainsci-13-00636" ref-type="bibr">38</xref>], including the generally decreased FCs between the occipital visual cortices and temporal multisensory cortices and increased FCs between visual cortex and regions important for memory and cognitive control of attention [<xref rid="B39-brainsci-13-00636" ref-type="bibr">39</xref>,<xref rid="B40-brainsci-13-00636" ref-type="bibr">40</xref>,<xref rid="B41-brainsci-13-00636" ref-type="bibr">41</xref>]. However, as human voice processing depends on a distributed network of interlinked voice-sensitive areas [<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>], there may be different patterns of changes in the pathways between the anatomically separable, functionally specialized voice-sensitive areas (e.g., the anterior/posterior TVA and amygdala) and face-sensitive areas (e.g., FFA and OFA). Furthermore, as blind individuals have lost the ability to process visual input, including facial information, they almost exclusively rely on voices to identify others. Several studies using a “training-recognition” paradigm have revealed compensatory enhancements for voice recognition in congenitally blind individuals. These individuals learn faster in voice-recognition training, recognize learned speakers with higher accuracy, and respond faster than their sighted counterparts [<xref rid="B35-brainsci-13-00636" ref-type="bibr">35</xref>,<xref rid="B42-brainsci-13-00636" ref-type="bibr">42</xref>]. Their superior performance in voice recognition persists even two weeks after training [<xref rid="B43-brainsci-13-00636" ref-type="bibr">43</xref>]. However, it remains unclear whether the heightened voice-recognition ability in blind individuals is due to altered functional connectivity (FC) in the neural substrates for voice processing.</p>
      <p>To investigate the plastic changes in the FC patterns of the voice perception network in blind compared to sighted participants, the present study quantitatively evaluated the internal FCs of different subareas involved in voice processing, the FCs between voice-sensitive and face-sensitive areas, and whether the FC changes among these areas could predict the superior voice-recognition ability in early blind individuals. In light of our recent study revealing a strong language familiarity effect in voice recognition in blind individuals [<xref rid="B44-brainsci-13-00636" ref-type="bibr">44</xref>], we included both Chinese and Japanese materials to verify the significant effect of recognizing voices spoken in a nonnative language. Findings from this study will provide insights into how visual deprivation affects the voice and face neural systems that process identity information in different sensory modalities and how the affected systems cooperate functionally during speaker recognition.</p>
    </sec>
    <sec id="sec2-brainsci-13-00636">
      <title>2. Materials and Methods</title>
      <sec sec-type="subjects" id="sec2dot1-brainsci-13-00636">
        <title>2.1. Participants</title>
        <p>Twenty early blind adults (EB; 5 females; mean age = 22.6 years; age range: 18–35 years) were recruited from the Special Education College at Beijing Union University and local communities in Beijing. All the blind participants had complete vision loss or no more than rudimentary sensitivity for brightness differences with no pattern vision. Four had become completely blind no later than the age of four and the others were congenitally blind (see <xref rid="app1-brainsci-13-00636" ref-type="app">Supplementary Table S1</xref> for a full description). We also recruited 22 sighted control participants (SC; 7 females; mean age = 23.7 years; age range: 20–38 years) who were matched to the blind participants for age, educational level, and musical experience. All participants were right-handed according to Edinburgh Handedness Inventory [<xref rid="B45-brainsci-13-00636" ref-type="bibr">45</xref>]. All the blind and sighted participants reported normal hearing and no history of neurological or psychiatric disorders. All participants were native Chinese speakers with no prior experience of Japanese. The study was approved by the Institutional Review Board of Peking University (approval code: #2015-12-06). Each participant provided written informed consent to their participation in the experiment.</p>
      </sec>
      <sec id="sec2dot2-brainsci-13-00636">
        <title>2.2. Stimuli and Procedure of the Behavioral Experiment</title>
        <p>Stimulus material for the voice-recognition task consisted of 15 Mandarin Chinese sentences and 15 Japanese sentences, which were selected from a corpus used in our previous study [<xref rid="B46-brainsci-13-00636" ref-type="bibr">46</xref>]. The number of syllables across sentences in both languages was kept at 15 on average. The average duration of sentences was 2695 ms (SD = 55 ms) for Chinese (CN) and 2676 ms (SD = 55 ms) for Japanese (JP). Sentences were read naturally by five female native speakers of each language, resulting in a total of 150 stimuli. All the sentences were perceived as having no discernible idiosyncratic talker characteristics (e.g., unusual phonetic or prosodic properties such as creaky voice). The 16-bit digital audio recordings were sampled at 44.1 kHz. The stimulus materials were volume balanced using Praat software (<uri xlink:href="http://www.fon.hum.uva.nl/praat/">http://www.fon.hum.uva.nl/praat/</uri>, accessed on 2 February 2017) and were presented over headphones.</p>
        <p>We adopted a well-established paradigm [<xref rid="B42-brainsci-13-00636" ref-type="bibr">42</xref>,<xref rid="B43-brainsci-13-00636" ref-type="bibr">43</xref>,<xref rid="B47-brainsci-13-00636" ref-type="bibr">47</xref>,<xref rid="B48-brainsci-13-00636" ref-type="bibr">48</xref>] for the speaker recognition experiment to assess the voice-recognition ability of our participants. Each of the blind and sighted participants performed the speaker recognition experiment in both language conditions (CN and JP) and the order of language was counterbalanced across participants. In each language condition, the experiment consisted of four sessions: familiarization phase, practice phase, generalization phase (GP), and delayed memory phase (DP) (<xref rid="brainsci-13-00636-f001" ref-type="fig">Figure 1</xref>). The tests in the first three phases were conducted in order on the same day and the delayed memory phase was set after two weeks. Each participant was tested individually in a quiet room.</p>
        <list list-type="bullet">
          <list-item>
            <p>Familiarization phase</p>
          </list-item>
        </list>
        <p>The familiarization phase was introduced first to help participants associate the speakers with the corresponding voices. In each trial, each participant heard a number designating the speaker (i.e., No. 1–5) followed by 1 of the 5 training sentences read by that speaker. Trials were blocked by sentences. Each sentence was read by all five speakers with two repetitions. Thus, each participant heard 5 sentences × 5 speakers × 2 times = 50 trials in total.</p>
        <list list-type="bullet">
          <list-item>
            <p>Practice phase</p>
          </list-item>
        </list>
        <p>After familiarization, participants were trained to identify the voice of each speaker. The sentence stimuli were the same as those presented in the familiarization phase, but after hearing a sentence, participants were asked to enter the number of the speaker on the keyboard. Correct responses were followed by a cue tone (“Ding”). If the answer was incorrect, the correct number of the speaker was announced to remind the participant. Each participant heard 5 sentences × 5 speakers × 5 times = 125 trials in total.</p>
        <list list-type="bullet">
          <list-item>
            <p>Generalization phase (GP)</p>
          </list-item>
        </list>
        <p>After practicing, each participant was asked to recognize the voices of 10 novel sentences read by the same 5 speakers as in the practice phase without feedback. Each participant heard 10 sentences × 5 speakers × 1 time = 50 trials in total. Their accuracy in this phase was computed to measure their voice-recognition ability.</p>
        <list list-type="bullet">
          <list-item>
            <p>Delayed memory phase (DP)</p>
          </list-item>
        </list>
        <p>Two weeks later, the participants returned to the lab and performed the same task as in the generalization phase. This retention test allowed us to examine the possible difference in voice memory ability between the blind and the sighted groups.</p>
      </sec>
      <sec id="sec2dot3-brainsci-13-00636">
        <title>2.3. rs-fMRI Data Acquisition and Preprocessing</title>
        <p>The participants were scanned on a SIEMENS MAGNETOM Prisma 3-Tesla magnetic resonance imaging scanner with a 20-channel head coil. High-resolution T1-weighted images were obtained using an MPRAGE sequence (192 slices, slice thickness = 1.00 mm, in-plane resolution = 448 × 512, TR = 2530 ms, TE = 2.98 ms, TI = 1100 ms, flip angle = 7°, field of view = 224 × 256 mm, voxel size = 0.5 × 0.5 × 1 mm<sup>3</sup>). We acquired rs-fMRI data using an echo-planar imaging (EPI) sequence with the following parameters: 64 transversal slices, slice thickness = 2.00 mm, in-plane resolution = 112 × 112, TR = 2000 ms, TE = 30 ms, flip angle = 90°, FoV = 224 × 224 mm, and voxel size = 2 × 2 × 2 mm<sup>3</sup>, 240 functional volumes. During the resting-state scanning, all the participants were blindfolded and instructed to keep their eyes closed and stay awake, but not to think actively about a particular idea as much as possible.</p>
        <p>Functional volumes were preprocessed and analyzed using SPM12 (Wellcome Department of Imaging Neuroscience, London, UK) and Data Processing Assistant for Resting-State fMRI pipeline analysis (DPARSF) [<xref rid="B49-brainsci-13-00636" ref-type="bibr">49</xref>] implemented in MATLAB (MathWorks). The initial 10 functional volumes were discarded to allow for signal stabilization and the subject’s adaptation to the environment. The preprocessing of the remaining 230 volumes included: (1) slice timing correction for acquisition timing differences, (2) realignment of the functional images to correct for head motions and coregistration of functional and anatomical data, (3) regressing out nuisance covariates including Friston 24-head motion parameters [<xref rid="B50-brainsci-13-00636" ref-type="bibr">50</xref>], white matter signal, cerebrospinal fluid signal, and linear trends, (4) spatially normalizing the realigned images into the Montreal Neurological Institute (MNI) space by using the parameters from the DARTEL algorithm for anatomical images processing [<xref rid="B51-brainsci-13-00636" ref-type="bibr">51</xref>] and resampled to 2 × 2 × 2 mm<sup>3</sup>, (5) spatial smoothing using a 4 mm FWHM Gaussian kernel, and (6) a band-pass filter (0.01–0.10 Hz) to reduce the effect of low-frequency drift and high-frequency noise.</p>
      </sec>
      <sec id="sec2dot4-brainsci-13-00636">
        <title>2.4. Seed-Based FC Analysis</title>
        <p>To explore the reorganization of the specific FCs between voice- and face-sensitive areas, we performed seed-based FC analyses and compared them across the blind and the sighted groups. As the selectivity of voice recognition is particularly pronounced in the right hemisphere [<xref rid="B52-brainsci-13-00636" ref-type="bibr">52</xref>,<xref rid="B53-brainsci-13-00636" ref-type="bibr">53</xref>], our analyses focused on the voice- and face-sensitive regions in the right hemisphere.</p>
        <p>Drawing from the outcomes of the earlier research that identified the voice-sensitive regions in the human auditory cortex, known as the ‘temporal voice areas’ [<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>], we defined two “voice patches” along the right STS/STG as regions-of-interest (ROIs): the right posterior ‘temporal voice areas’ (TVAp, MNI coordinate: x = 42, y = −35, z = 3) and the right anterior ‘temporal voice areas’ (TVAa, MNI coordinate: x = 55, y = −2, z = −7). Considering both temporal and extra-temporal regions play important roles in performing a voice-recognition task, we selected the right amygdala (MNI coordinate: x = 20, y = -8, z = −12) [<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>] and right inferior frontal gyrus (IFG, to be exact, the posterior triangularis; MNI coordinate: x = 53, y = 26, z = 26) [<xref rid="B9-brainsci-13-00636" ref-type="bibr">9</xref>], both of which show reliable voice sensitivity. The face-sensitive areas were identified based on a quantitative meta-analysis of fMRI studies on sighted participants [<xref rid="B9-brainsci-13-00636" ref-type="bibr">9</xref>], including the right FFA (MNI coordinate: x = 41, y = −53, z = −19) and the right OFA (MNI coordinate: x = 40, y = −81, z = −5) as ROIs. We also ran an exploratory analysis of the voice/face-sensitive areas in the left hemisphere (Please refer to <xref rid="app1-brainsci-13-00636" ref-type="app">Supplementary Table S2</xref> for detailed descriptions of the ROIs).</p>
        <p>In the ROI-to-ROI FC analyses, 6 mm radius spheres were created centering in the coordinates of 6 ROIs, and the time course for each seed was extracted by averaging the time courses of all voxels in the ROI for each participant. Then, the synchrony of the time series between the 6 ROIs was assessed by Pearson’s correlation coefficients, which were transformed into Fisher <italic toggle="yes">z</italic>-scores. Next, we performed two-sample <italic toggle="yes">t</italic>-tests to examine the differences between the transformed correlation coefficients of the two groups. To describe the relationship between the reorganization of FCs and voice-recognition ability, brain–behavior correlational results were obtained with the <italic toggle="yes">p</italic>-values corrected by the False Discovery Rate (FDR) method for multiple comparisons. Comparisons of the correlation coefficients were performed between the two groups according to the method proposed by Diedenhofen &amp; Much (2015) [<xref rid="B54-brainsci-13-00636" ref-type="bibr">54</xref>].</p>
      </sec>
    </sec>
    <sec sec-type="results" id="sec3-brainsci-13-00636">
      <title>3. Results</title>
      <sec id="sec3dot1-brainsci-13-00636">
        <title>3.1. Behavioral Results</title>
        <p>The behavioral accuracy data in the generalization phase and delayed memory phase indicated the two participant groups’ voice-recognition ability across the different conditions (<xref rid="brainsci-13-00636-f002" ref-type="fig">Figure 2</xref>). The group difference was analyzed using a three-way repeated-measures ANOVA with Time (GP, DP) and Language (CN, JP) as within-subject factors, and Group (SC, EB) as a between-subject factor. The ANOVA results revealed a significant main effect of Group (<italic toggle="yes">F</italic> (1, 40) = 5.439, <italic toggle="yes">p</italic> = 0.025, <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.120), indicating that overall, the blind participants performed better than their sighted counterparts. There was also a significant main effect of Language (<italic toggle="yes">F</italic> (1, 40) = 42.472, <italic toggle="yes">p</italic> &lt; 0.001, <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.515) with no Group-by-Language interaction (<italic toggle="yes">F</italic> (1, 40) = 0.280, <italic toggle="yes">p</italic> = 0.600, <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.007), indicating that the two groups were equally more accurate in Chinese voice recognition than in Japanese voice recognition. We also found a significant main effect of Time (<italic toggle="yes">F</italic> (1, 40) = 37.030, <italic toggle="yes">p</italic> &lt; 0.001, <inline-formula><mml:math id="mm4" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.481) and a marginally significant Group-by-Time interaction (<italic toggle="yes">F</italic> (1, 40) = 3.711, <italic toggle="yes">p</italic> = 0.061, <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.085), indicating that both groups were more accurate at GP than at DP with a greater difference of accuracy between the repeated tests in the sighted (<italic toggle="yes">F</italic> (1, 40) = 33.698, <italic toggle="yes">p</italic> &lt; 0.001, <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.457) than in the blind group (<italic toggle="yes">F</italic> (1, 40) = 8.255, <italic toggle="yes">p</italic> = 0.006, <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.171). There was no significant three-way (Group by Time by Language) interaction (<italic toggle="yes">F</italic> (1, 40) = 0.595, <italic toggle="yes">p</italic> = 0.445, <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> = 0.015).</p>
      </sec>
      <sec id="sec3dot2-brainsci-13-00636">
        <title>3.2. Changes in Functional Connectivity among the Voice- and Face-Sensitive Areas in the Early Blind</title>
        <p>To delineate the alterations in the functional connectivity among voice- and face-sensitive areas in the blind subjects, we compared the average FCs of the six seeds between the sighted and early blind groups using two-sample <italic toggle="yes">t</italic>-tests (<xref rid="brainsci-13-00636-f003" ref-type="fig">Figure 3</xref>a,b, <xref rid="app1-brainsci-13-00636" ref-type="app">Supplementary Table S3</xref>).</p>
        <p>Within the voice-recognition network, the early blind group exhibited significantly higher FCs than the sighted group between the amygdala and following regions: TVAp (<italic toggle="yes">t</italic> (40) = −2.947, <italic toggle="yes">p</italic>-FDR = 0.027, <italic toggle="yes">d</italic> = 0.911), TVAa (<italic toggle="yes">t</italic> (40) = −2.565, <italic toggle="yes">p</italic>-FDR = 0.030, <italic toggle="yes">d</italic> = 0.793), and IFG (<italic toggle="yes">t</italic> (40) = −2.794, <italic toggle="yes">p</italic>-FDR = 0.029, <italic toggle="yes">d</italic> = 0.863) (see <xref rid="brainsci-13-00636-f003" ref-type="fig">Figure 3</xref>b,c).</p>
        <p>For the FCs between the voice and face recognition networks, we found significant reductions in the strength of FFA-TVAa (<italic toggle="yes">t</italic> (40) = 2.599, <italic toggle="yes">p</italic>-FDR = 0.030, <italic toggle="yes">d</italic> = 0.803) and OFA -TVAa (<italic toggle="yes">t</italic> (40) = 2.720, <italic toggle="yes">p</italic>-FDR = 0.029, <italic toggle="yes">d</italic> = 0.840) in the early blind group compared with the sighted group (<xref rid="brainsci-13-00636-f003" ref-type="fig">Figure 3</xref>b,c). In contrast, the FC strengths of FFA-IFG and OFA-IFG were significantly enhanced in the early blind group relative to the sighted group (<italic toggle="yes">t</italic> (40) = −3.214, <italic toggle="yes">p</italic>-FDR = 0.019, <italic toggle="yes">d</italic> = 0.993; <italic toggle="yes">t</italic> (40) = −4.702, <italic toggle="yes">p</italic>-FDR &lt; 0.001, <italic toggle="yes">d</italic> = 1.453; see <xref rid="brainsci-13-00636-f003" ref-type="fig">Figure 3</xref>b,c). Similarly, the results of seed-based analyses in the left hemisphere showed the FC strength of L.FFA/OFA-L.IFG was higher in blind individuals relative to sighted individuals (see <xref rid="app1-brainsci-13-00636" ref-type="app">Supplementary Table S4</xref> for a full description).</p>
      </sec>
      <sec id="sec3dot3-brainsci-13-00636">
        <title>3.3. Correlations between Voice-recognition Ability and the Strengths of FC</title>
        <p>We conducted Pearson’s correlations between voice-recognition ability and the strength of functional connectivity for each group. The results showed that a stronger connectivity between TVAp and FFA was associated with better voice recognition only in the sighted participants (CN-GP: <italic toggle="yes">r</italic>-SC = 0.640, <italic toggle="yes">p</italic>-FDR = 0.015; <italic toggle="yes">r</italic>-EB = −0.014, <italic toggle="yes">p</italic>-FDR &gt; 0.05; <italic toggle="yes">r</italic>-SC &gt; <italic toggle="yes">r</italic>-EB, <italic toggle="yes">p</italic> = 0.021). On the other hand, in the blind participants, a stronger connectivity between the amygdala and FFA was associated with better voice-recognition performance (CN-GP: <italic toggle="yes">r</italic>-EB = 0.607, <italic toggle="yes">p</italic>-FDR = 0.075; <italic toggle="yes">r</italic>-SC = −0.315, <italic toggle="yes">p</italic>-FDR &gt; 0.05; <italic toggle="yes">r</italic>-EB &gt; <italic toggle="yes">r</italic>-SC, <italic toggle="yes">p</italic> = 0.002; JP-DP: <italic toggle="yes">r</italic>-EB = 0.765, <italic toggle="yes">p</italic>-FDR &lt; 0.001; <italic toggle="yes">r</italic>-SC = 0.395, <italic toggle="yes">p</italic>-FDR &gt; 0.05; <italic toggle="yes">r</italic>-EB &gt; <italic toggle="yes">r</italic>-SC, <italic toggle="yes">p</italic> = 0.077). Refer to <xref rid="brainsci-13-00636-f004" ref-type="fig">Figure 4</xref> and <xref rid="app1-brainsci-13-00636" ref-type="app">Tables S5–S7</xref> for more details.</p>
        <p>Although the <italic toggle="yes">p</italic>-values were not significant after being corrected for multiple comparisons of correlation analysis (<italic toggle="yes">p</italic> uncorrected &lt; 0.05, <italic toggle="yes">p</italic>-FDR &gt; 0.05), there was a tendency that the stronger FCs of TVAp/TVAa-amygdala and TVAa-FFA/OFA were associated with better performance for voice recognition only in the sighted group, while the stronger FCs of amygdala-OFA and FFA-OFA associated with better voice-recognition performance only in the early blind group (<xref rid="brainsci-13-00636-f004" ref-type="fig">Figure 4</xref>a, <xref rid="app1-brainsci-13-00636" ref-type="app">Supplementary Tables S5–S7</xref>).</p>
      </sec>
    </sec>
    <sec sec-type="discussion" id="sec4-brainsci-13-00636">
      <title>4. Discussion</title>
      <p>In the present study, we investigated how vision loss shaped the neural substrates for voice recognition by resting-state fMRI in early blind individuals and sighted controls. Behavioral results replicated previous findings on the superiority of voice retention memory [<xref rid="B43-brainsci-13-00636" ref-type="bibr">43</xref>] and the significant effect of language familiarity on voice recognition in the blind group [<xref rid="B44-brainsci-13-00636" ref-type="bibr">44</xref>]. ROI-wised functional connectivity analyses evidenced a significant enhancement in the functional coupling between the amygdala and TVAp/TVAa/IFG in the early blind. We also found stronger FCs between FFA/OFA and IFG but weaker FCs between FFA/OFA and TVAa in blind than in sighted participants. Furthermore, we analyzed the correlations between FCs and voice-recognition accuracy in each group. Our results showed that better behavioral performance was associated with stronger FC between TVAp and FFA only in sighted individuals but stronger FC between the amygdala and FFA only in early blind individuals.</p>
      <sec id="sec4dot1-brainsci-13-00636">
        <title>4.1. Enhanced Internal Connections of Voice Perception Network in the Early Blind</title>
        <p>Recognizing a person by voice involves multiple processes. Correspondingly, a large network of distributed brain areas is involved in the processing of voice identity, including not only temporal voice areas as the core parts but also subcortical (such as the amygdala) and prefrontal cortices as the extended regions [<xref rid="B15-brainsci-13-00636" ref-type="bibr">15</xref>,<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>,<xref rid="B18-brainsci-13-00636" ref-type="bibr">18</xref>]. One of the key findings in the present study is that the intranetwork connections among the voice-sensitive areas were enhanced in the blind group, indicating reorganization within the intact voice-recognition system associated with visual impairment. Moreover, our results showed that the alterations in the voice perception network were not confined to TVAs but also included the extended parts of the network, especially the amygdala. The amygdala is involved in the processing of emotional voices in the blind [<xref rid="B37-brainsci-13-00636" ref-type="bibr">37</xref>]. Some evidence has suggested that the amygdala is also associated with the processing of voice and face traits regardless of the affective characteristics [<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>,<xref rid="B55-brainsci-13-00636" ref-type="bibr">55</xref>]. A recent fMRI study in patients with primary visual cortex impairment has confirmed that the amygdala is involved in the processing of socially salient but emotionally neutral facial expressions [<xref rid="B56-brainsci-13-00636" ref-type="bibr">56</xref>]. In the current study, emotionally neutral stimuli were used, and more accurate (GP) and delayed (DP) performances were associated with a stronger connection between the amygdala and TVA, thus providing further support for the role of the amygdala in speaker identity recognition irrespective of emotional valence.</p>
        <p>We also observed that the FC between the amygdala and IFG was enhanced in the blind group. The inferior frontal regions are involved in recognizing learned-familiar persons [<xref rid="B9-brainsci-13-00636" ref-type="bibr">9</xref>,<xref rid="B57-brainsci-13-00636" ref-type="bibr">57</xref>], and extensive evidence has indicated that the basolateral complex of the amygdala projects to plenty of regions (e.g., the prefrontal cortex and hippocampus) associated with learning and memory [<xref rid="B58-brainsci-13-00636" ref-type="bibr">58</xref>,<xref rid="B59-brainsci-13-00636" ref-type="bibr">59</xref>,<xref rid="B60-brainsci-13-00636" ref-type="bibr">60</xref>]. The enhanced pathway between the amygdala and IFG observed in this study, therefore, might be a neural basis for enhanced ability to establish and consolidate the link between voice trait and identity in blind people. This result is corroborated by previous evidence that blind individuals learn faster in voice-recognition training [<xref rid="B35-brainsci-13-00636" ref-type="bibr">35</xref>,<xref rid="B42-brainsci-13-00636" ref-type="bibr">42</xref>,<xref rid="B61-brainsci-13-00636" ref-type="bibr">61</xref>] and are more accurate in delayed voice-identity recognition compared with sighted counterparts [<xref rid="B43-brainsci-13-00636" ref-type="bibr">43</xref>]. Taken together, the strengthened intra-network functional connectivity between the distributed voice-sensitive areas might play a critical role in the voice recognition of the early blind. More specifically, the amygdala appeared to be a key component in the voice perception network.</p>
      </sec>
      <sec id="sec4dot2-brainsci-13-00636">
        <title>4.2. Reorganization of the Internetwork Connections between the Voice- and Face-Sensitive Areas in the Early Blind</title>
        <p>Neuropsychological and neuroimaging studies provide mounting evidence for the multimodal integration of facial and vocal information during identity processing [<xref rid="B26-brainsci-13-00636" ref-type="bibr">26</xref>,<xref rid="B28-brainsci-13-00636" ref-type="bibr">28</xref>]. Voice- and face-sensitive areas are functionally and anatomically connected for transferring the identity information during voice recognition [<xref rid="B19-brainsci-13-00636" ref-type="bibr">19</xref>,<xref rid="B22-brainsci-13-00636" ref-type="bibr">22</xref>,<xref rid="B23-brainsci-13-00636" ref-type="bibr">23</xref>,<xref rid="B24-brainsci-13-00636" ref-type="bibr">24</xref>]. The exchange of information between the two systems facilitates identity processing in sighted people [<xref rid="B27-brainsci-13-00636" ref-type="bibr">27</xref>,<xref rid="B62-brainsci-13-00636" ref-type="bibr">62</xref>]. The findings of the current study in the early blind are consistent with previous work by showing a positive association between the FC of TVAp-FFA and voice-recognition performance.</p>
        <p>More importantly, we found that the strengths of the FC between FFA/OFA and TVAa were reduced in the blind group, indicating the absence of crossmodal integration of facial and vocal information due to visual deprivation. Similarly, it was reported that auditory deprivation would introduce a significant reduction of fractional anisotropy and increment of radial diffusivity in the V2/V3- and FFA-TVA connections [<xref rid="B25-brainsci-13-00636" ref-type="bibr">25</xref>]. We speculate that vision loss in blind individuals disrupts the visual input to FFA, leading to the absence of crossmodal sensory integration in the FFA-TVA pathways and the consequent reduced connectivity in the FFA-TVA pathways. Our speculation is further supported by consistent findings across previous studies that the TVA (particularly the anterior part) as an association area receives identity information (such as gender or age) conveyed both by facial and vocal stimuli [<xref rid="B24-brainsci-13-00636" ref-type="bibr">24</xref>,<xref rid="B26-brainsci-13-00636" ref-type="bibr">26</xref>,<xref rid="B63-brainsci-13-00636" ref-type="bibr">63</xref>,<xref rid="B64-brainsci-13-00636" ref-type="bibr">64</xref>].</p>
        <p>Taken together, our result that the functional connectivity between the voice- and face-sensitive areas promoted voice identity processing is consistent with the well-documented “integrative model” of personal recognition in sighted people [<xref rid="B27-brainsci-13-00636" ref-type="bibr">27</xref>,<xref rid="B62-brainsci-13-00636" ref-type="bibr">62</xref>,<xref rid="B65-brainsci-13-00636" ref-type="bibr">65</xref>], but the absence of crossmodal sensory integration induced by visual deprivation leads to reduced coupling between the voice- and face-sensitive areas in early blind individuals.</p>
      </sec>
      <sec id="sec4dot3-brainsci-13-00636">
        <title>4.3. Neuroplastic Changes of the Face-Sensitive Areas in the Early Blind</title>
        <p>The blind group outperformed the sighted group in the delayed memory phase (but not in the generalization phase) during the voice-recognition task. It is possible that early blindness promoted the long-term memory consolidation of speaker identity. Indeed, we found that blind participants’ performance during the delayed memory phase was positively correlated with the FC between the amygdala and FFA. Previous studies have provided strong evidence for the direct white matter pathway [<xref rid="B66-brainsci-13-00636" ref-type="bibr">66</xref>] and high functional coupling between the amygdala and FFA [<xref rid="B67-brainsci-13-00636" ref-type="bibr">67</xref>,<xref rid="B68-brainsci-13-00636" ref-type="bibr">68</xref>]. A meta-analysis study revealed that the superficial subregion nucleus of the amygdala and FFA were primarily involved in cognitive memory [<xref rid="B69-brainsci-13-00636" ref-type="bibr">69</xref>]. Our data suggest that the efficiency of functional connectivity between the amygdala and FFA may modulate the long-term memory storage for voice through the retained pathways in the early blind.</p>
        <p>Moreover, the FC between FFA and OFA was associated with voice-recognition accuracy only in the blind group. Given that the network of face perception was composed of distributed patches such as FFA and OFA [<xref rid="B10-brainsci-13-00636" ref-type="bibr">10</xref>] and that the FC between FFA and OFA plays a critical role in face perception among sighted people [<xref rid="B70-brainsci-13-00636" ref-type="bibr">70</xref>], our result indicates that face-sensitive areas can retain their functional selectivity in blind people [<xref rid="B30-brainsci-13-00636" ref-type="bibr">30</xref>,<xref rid="B32-brainsci-13-00636" ref-type="bibr">32</xref>,<xref rid="B71-brainsci-13-00636" ref-type="bibr">71</xref>]. This is consistent with previous findings that the FFA could be activated by auditory-only voice recognition without corresponding face training in sighted people [<xref rid="B19-brainsci-13-00636" ref-type="bibr">19</xref>,<xref rid="B23-brainsci-13-00636" ref-type="bibr">23</xref>]. In addition, a recent fMRI study using multivoxel pattern analysis and functional cortical mapping techniques demonstrated that blind individuals could develop category selectivity (face, body, etc.) in the ventral-temporal cortex which was strikingly similar to the sighted controls [<xref rid="B72-brainsci-13-00636" ref-type="bibr">72</xref>]. However, given that visual impairment disrupts cortical processing of facial properties, it remains inconclusive whether the disrupted face system was dedicated to early or late stages of voice processing or both.</p>
        <p>Meanwhile, we observed that FCs between FFA/OFA and IFG were enhanced in the blind group. The inferior frontal areas are considered as extended parts of both the voice perception network [<xref rid="B17-brainsci-13-00636" ref-type="bibr">17</xref>,<xref rid="B18-brainsci-13-00636" ref-type="bibr">18</xref>,<xref rid="B73-brainsci-13-00636" ref-type="bibr">73</xref>] and the face perception network [<xref rid="B10-brainsci-13-00636" ref-type="bibr">10</xref>]. The frontal regions associated with voice recognition are directly adjacent to the regions involved in face recognition [<xref rid="B9-brainsci-13-00636" ref-type="bibr">9</xref>]. Further investigations are needed to clarify the precise role of IFG and its subregions in voice perception.</p>
      </sec>
    </sec>
    <sec sec-type="conclusions" id="sec5-brainsci-13-00636">
      <title>5. Conclusions</title>
      <p>The clear group differences in the current behavioral and resting-state fMRI data reveal plastic changes in the neural substrates for voice recognition associated with early visual deprivation. Specifically, the internal links of the intact voice system were enhanced, while the connections between the core part of the voice system and the disrupted face system were decreased in the early blind. Despite visual deprivation in blind individuals, intrinsic brain activities independent of experimental tasks showed that the face system was not excluded from the processing of personal identity; instead, it was found to be actively involved in voice recognition via the connections between the core face-sensitive areas (e.g., FFA and OFA) and the amygdala/IFG. These findings are in line with the “metamodal” theory that the two systems conduct similar computational operations during face and voice processing during the functional reorganization [<xref rid="B28-brainsci-13-00636" ref-type="bibr">28</xref>], which may facilitate blind individuals’ talker identity recognition and their adaptation to the social environment in daily life.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>We wish to thank National Center for Protein Sciences at Peking University in Beijing, China, for assistance with data acquisition, and Weiwei Men &amp; Gaolang Gong for help with parameter-setting on the MRI scanning sequences. We also thank all participants for their participation.</p>
    </ack>
    <fn-group>
      <fn>
        <p><bold>Disclaimer/Publisher’s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p>
      </fn>
    </fn-group>
    <app-group>
      <app id="app1-brainsci-13-00636">
        <title>Supplementary Materials</title>
        <p>The following supporting information can be downloaded at: <uri xlink:href="https://www.mdpi.com/article/10.3390/brainsci13040636/s1">https://www.mdpi.com/article/10.3390/brainsci13040636/s1</uri>, Table S1: Characteristics of the early blind participants; Table S2: Region coordinates for defined regions of interest in the left hemisphere; Table S3: The results of group comparisons between the sighted and blind participants in the strength of functional connectivity; Table S4: The results of group comparisons between the sighted control and early blind subjects in the strength of functional connectivity in the left hemisphere; Table S5: Correlation of voice-recognition accuracy and the strength of functional connectivity in the sighted; Table S6: Correlation of voice-recognition accuracy and the strength of functional connectivity in the blind; Table S7: The comparisons of correlations between the sighted control and early blind groups.</p>
        <supplementary-material id="brainsci-13-00636-s001" position="float" content-type="local-data">
          <media xlink:href="brainsci-13-00636-s001.zip">
            <caption>
              <p>Click here for additional data file.</p>
            </caption>
          </media>
        </supplementary-material>
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, W.P., L.Z. and Y.Z. (Yumei Zhang); methodology, W.P., W.Z. and Y.Z. (Yang Zhang); formal analysis, W.P. and W.Z.; investigation, W.P., W.Z. and Y.R.; resources, L.Z., H.S. and Y.Z. (Yumei Zhang); writing—original draft preparation: W.P. and W.Z.; writing—review and editing, Y.R., L.Z., Y.Z. (Yang Zhang) and Y.Z. (Yumei Zhang); visualization, W.P.; supervision, L.Z., H.S. and Y.Z. (Yumei Zhang); funding acquisition, L.Z. and Y.Z. (Yumei Zhang). All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Institutional Review Board Statement</title>
      <p>The study was conducted in accordance with the Declaration of Helsinki and approved by the Institutional Review Board of Peking University (approval code: #2015-12-06).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      <p>Informed consent was obtained from all subjects involved in the study and written informed consent has been obtained from the subjects to publish this paper.</p>
    </notes>
    <notes notes-type="data-availability">
      <title>Data Availability Statement</title>
      <p>The data that support the findings of this study are available from the corresponding author, W.P., upon reasonable request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="B1-brainsci-13-00636">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Tsao</surname><given-names>D.Y.</given-names></name>
<name><surname>Freiwald</surname><given-names>W.A.</given-names></name>
<name><surname>Knutsen</surname><given-names>T.A.</given-names></name>
<name><surname>Mandeville</surname><given-names>J.B.</given-names></name>
<name><surname>Tootell</surname><given-names>R.B.</given-names></name>
</person-group>
          <article-title>Faces and objects in macaque cerebral cortex</article-title>
          <source>Nat. Neurosci.</source>
          <year>2003</year>
          <volume>6</volume>
          <fpage>989</fpage>
          <lpage>995</lpage>
          <pub-id pub-id-type="doi">10.1038/nn1111</pub-id>
          <pub-id pub-id-type="pmid">12925854</pub-id>
        </element-citation>
      </ref>
      <ref id="B2-brainsci-13-00636">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Tsao</surname><given-names>D.Y.</given-names></name>
<name><surname>Moeller</surname><given-names>S.</given-names></name>
<name><surname>Freiwald</surname><given-names>W.A.</given-names></name>
</person-group>
          <article-title>Comparing face patch systems in macaques and humans</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2008</year>
          <volume>105</volume>
          <fpage>19514</fpage>
          <lpage>19519</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.0809662105</pub-id>
          <pub-id pub-id-type="pmid">19033466</pub-id>
        </element-citation>
      </ref>
      <ref id="B3-brainsci-13-00636">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Pinsk</surname><given-names>M.A.</given-names></name>
<name><surname>DeSimone</surname><given-names>K.</given-names></name>
<name><surname>Moore</surname><given-names>T.</given-names></name>
<name><surname>Gross</surname><given-names>C.G.</given-names></name>
<name><surname>Kastner</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Representations of faces and body parts in macaque temporal cortex: A functional MRI study</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2005</year>
          <volume>102</volume>
          <fpage>6996</fpage>
          <lpage>7001</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.0502605102</pub-id>
          <pub-id pub-id-type="pmid">15860578</pub-id>
        </element-citation>
      </ref>
      <ref id="B4-brainsci-13-00636">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Gobbini</surname><given-names>M.I.</given-names></name>
<name><surname>Haxby</surname><given-names>J.V.</given-names></name>
</person-group>
          <article-title>Neural systems for recognition of familiar faces</article-title>
          <source>Neuropsychologia</source>
          <year>2007</year>
          <volume>45</volume>
          <fpage>32</fpage>
          <lpage>41</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.04.015</pub-id>
          <pub-id pub-id-type="pmid">16797608</pub-id>
        </element-citation>
      </ref>
      <ref id="B5-brainsci-13-00636">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Freiwald</surname><given-names>W.</given-names></name>
<name><surname>Duchaine</surname><given-names>B.</given-names></name>
<name><surname>Yovel</surname><given-names>G.</given-names></name>
</person-group>
          <article-title>Face Processing Systems: From Neurons to Real-World Social Perception</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <year>2016</year>
          <volume>39</volume>
          <fpage>325</fpage>
          <lpage>346</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013934</pub-id>
          <pub-id pub-id-type="pmid">27442071</pub-id>
        </element-citation>
      </ref>
      <ref id="B6-brainsci-13-00636">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kanwisher</surname><given-names>N.</given-names></name>
<name><surname>McDermott</surname><given-names>J.</given-names></name>
<name><surname>Chun</surname><given-names>M.M.</given-names></name>
</person-group>
          <article-title>The Fusiform Face Area: A Module in Human Extrastriate Cortex Specialized for Face Perception</article-title>
          <source>J. Neurosci.</source>
          <year>1997</year>
          <volume>17</volume>
          <fpage>4302</fpage>
          <lpage>4311</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id>
          <pub-id pub-id-type="pmid">9151747</pub-id>
        </element-citation>
      </ref>
      <ref id="B7-brainsci-13-00636">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kanwisher</surname><given-names>N.</given-names></name>
<name><surname>Stanley</surname><given-names>D.</given-names></name>
<name><surname>Harris</surname><given-names>A.</given-names></name>
</person-group>
          <article-title>The fusiform face area is selective for faces not animals</article-title>
          <source>Neuroreport</source>
          <year>1999</year>
          <volume>10</volume>
          <fpage>183</fpage>
          <lpage>187</lpage>
          <pub-id pub-id-type="doi">10.1097/00001756-199901180-00035</pub-id>
          <pub-id pub-id-type="pmid">10094159</pub-id>
        </element-citation>
      </ref>
      <ref id="B8-brainsci-13-00636">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>McCarthy</surname><given-names>G.</given-names></name>
<name><surname>Puce</surname><given-names>A.</given-names></name>
<name><surname>Gore</surname><given-names>J.C.</given-names></name>
<name><surname>Allison</surname><given-names>T.</given-names></name>
</person-group>
          <article-title>Face-Specific Processing in the Human Fusiform Gyrus</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>1997</year>
          <volume>9</volume>
          <fpage>605</fpage>
          <lpage>610</lpage>
          <pub-id pub-id-type="doi">10.1162/jocn.1997.9.5.605</pub-id>
          <pub-id pub-id-type="pmid">23965119</pub-id>
        </element-citation>
      </ref>
      <ref id="B9-brainsci-13-00636">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Blank</surname><given-names>H.</given-names></name>
<name><surname>Wieland</surname><given-names>N.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Person recognition and the brain: Merging evidence from patients and healthy individuals</article-title>
          <source>Neurosci. Biobehav. Rev.</source>
          <year>2014</year>
          <volume>47</volume>
          <fpage>717</fpage>
          <lpage>734</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.10.022</pub-id>
          <pub-id pub-id-type="pmid">25451765</pub-id>
        </element-citation>
      </ref>
      <ref id="B10-brainsci-13-00636">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Duchaine</surname><given-names>B.</given-names></name>
<name><surname>Yovel</surname><given-names>G.</given-names></name>
</person-group>
          <article-title>A Revised Neural Framework for Face Processing</article-title>
          <source>Annu. Rev. Vis. Sci.</source>
          <year>2015</year>
          <volume>1</volume>
          <fpage>393</fpage>
          <lpage>416</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035518</pub-id>
          <pub-id pub-id-type="pmid">28532371</pub-id>
        </element-citation>
      </ref>
      <ref id="B11-brainsci-13-00636">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>Zatorre</surname><given-names>R.J.</given-names></name>
<name><surname>Lafaille</surname><given-names>P.</given-names></name>
<name><surname>Ahad</surname><given-names>P.</given-names></name>
<name><surname>Pike</surname><given-names>B.</given-names></name>
</person-group>
          <article-title>Voice-selective areas in human auditory cortex</article-title>
          <source>Nature</source>
          <year>2000</year>
          <volume>403</volume>
          <fpage>309</fpage>
          <lpage>312</lpage>
          <pub-id pub-id-type="doi">10.1038/35002078</pub-id>
          <pub-id pub-id-type="pmid">10659849</pub-id>
        </element-citation>
      </ref>
      <ref id="B12-brainsci-13-00636">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>Zatorre</surname><given-names>R.J.</given-names></name>
</person-group>
          <article-title>Adaptation to speaker’s voice in right anterior temporal lobe</article-title>
          <source>Neuroreport</source>
          <year>2003</year>
          <volume>14</volume>
          <fpage>2105</fpage>
          <lpage>2109</lpage>
          <pub-id pub-id-type="doi">10.1097/00001756-200311140-00019</pub-id>
          <pub-id pub-id-type="pmid">14600506</pub-id>
        </element-citation>
      </ref>
      <ref id="B13-brainsci-13-00636">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kriegstein</surname><given-names>K.V.</given-names></name>
<name><surname>Giraud</surname><given-names>A.L.</given-names></name>
</person-group>
          <article-title>Distinct functional substrates along the right superior temporal sulcus for the processing of voices</article-title>
          <source>Neuroimage</source>
          <year>2004</year>
          <volume>22</volume>
          <fpage>948</fpage>
          <lpage>955</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.02.020</pub-id>
          <pub-id pub-id-type="pmid">15193626</pub-id>
        </element-citation>
      </ref>
      <ref id="B14-brainsci-13-00636">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>Fecteau</surname><given-names>S.</given-names></name>
<name><surname>Bedard</surname><given-names>C.</given-names></name>
</person-group>
          <article-title>Thinking the voice: Neural correlates of voice perception</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2004</year>
          <volume>8</volume>
          <fpage>129</fpage>
          <lpage>135</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2004.01.008</pub-id>
          <pub-id pub-id-type="pmid">15301753</pub-id>
        </element-citation>
      </ref>
      <ref id="B15-brainsci-13-00636">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Andics</surname><given-names>A.</given-names></name>
<name><surname>McQueen</surname><given-names>J.M.</given-names></name>
<name><surname>Petersson</surname><given-names>K.M.</given-names></name>
<name><surname>Gal</surname><given-names>V.</given-names></name>
<name><surname>Rudas</surname><given-names>G.</given-names></name>
<name><surname>Vidnyanszky</surname><given-names>Z.</given-names></name>
</person-group>
          <article-title>Neural mechanisms for voice recognition</article-title>
          <source>Neuroimage</source>
          <year>2010</year>
          <volume>52</volume>
          <fpage>1528</fpage>
          <lpage>1540</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.048</pub-id>
          <pub-id pub-id-type="pmid">20553895</pub-id>
        </element-citation>
      </ref>
      <ref id="B16-brainsci-13-00636">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Schall</surname><given-names>S.</given-names></name>
<name><surname>Kiebel</surname><given-names>S.J.</given-names></name>
<name><surname>Maess</surname><given-names>B.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Voice identity recognition: Functional division of the right STS and its behavioral relevance</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2015</year>
          <volume>27</volume>
          <fpage>280</fpage>
          <lpage>291</lpage>
          <pub-id pub-id-type="doi">10.1162/jocn_a_00707</pub-id>
          <pub-id pub-id-type="pmid">25170793</pub-id>
        </element-citation>
      </ref>
      <ref id="B17-brainsci-13-00636">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Pernet</surname><given-names>C.R.</given-names></name>
<name><surname>McAleer</surname><given-names>P.</given-names></name>
<name><surname>Latinus</surname><given-names>M.</given-names></name>
<name><surname>Gorgolewski</surname><given-names>K.J.</given-names></name>
<name><surname>Charest</surname><given-names>I.</given-names></name>
<name><surname>Bestelmeyer</surname><given-names>P.E.</given-names></name>
<name><surname>Watson</surname><given-names>R.H.</given-names></name>
<name><surname>Fleming</surname><given-names>D.</given-names></name>
<name><surname>Crabbe</surname><given-names>F.</given-names></name>
<name><surname>Valdes-Sosa</surname><given-names>M.</given-names></name>
<etal/>
</person-group>
          <article-title>The human voice areas: Spatial organization and inter-individual variability in temporal and extra-temporal cortices</article-title>
          <source>Neuroimage</source>
          <year>2015</year>
          <volume>119</volume>
          <fpage>164</fpage>
          <lpage>174</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.06.050</pub-id>
          <pub-id pub-id-type="pmid">26116964</pub-id>
        </element-citation>
      </ref>
      <ref id="B18-brainsci-13-00636">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Aglieri</surname><given-names>V.</given-names></name>
<name><surname>Cagna</surname><given-names>B.</given-names></name>
<name><surname>Velly</surname><given-names>L.</given-names></name>
<name><surname>Takerkart</surname><given-names>S.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>FMRI-based identity classification accuracy in left temporal and frontal regions predicts speaker recognition performance</article-title>
          <source>Sci. Rep.</source>
          <year>2021</year>
          <volume>11</volume>
          <fpage>489</fpage>
          <pub-id pub-id-type="doi">10.1038/s41598-020-79922-7</pub-id>
          <pub-id pub-id-type="pmid">33436825</pub-id>
        </element-citation>
      </ref>
      <ref id="B19-brainsci-13-00636">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>von Kriegstein</surname><given-names>K.v.</given-names></name>
<name><surname>Kleinschmidt</surname><given-names>A.</given-names></name>
<name><surname>Sterzer</surname><given-names>P.</given-names></name>
<name><surname>Giraud</surname><given-names>A.-L.</given-names></name>
</person-group>
          <article-title>Interaction of Face and Voice Areas during Speaker Recognition</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2005</year>
          <volume>17</volume>
          <fpage>367</fpage>
          <lpage>376</lpage>
          <pub-id pub-id-type="doi">10.1162/0898929053279577</pub-id>
          <pub-id pub-id-type="pmid">15813998</pub-id>
        </element-citation>
      </ref>
      <ref id="B20-brainsci-13-00636">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
<name><surname>Dogan</surname><given-names>Ö.</given-names></name>
<name><surname>Grüter</surname><given-names>M.</given-names></name>
<name><surname>Giraud</surname><given-names>A.-L.</given-names></name>
<name><surname>Kell</surname><given-names>C.A.</given-names></name>
<name><surname>Grüter</surname><given-names>T.</given-names></name>
<name><surname>Kleinschmidt</surname><given-names>A.</given-names></name>
<name><surname>Kiebel</surname><given-names>S.J.</given-names></name>
</person-group>
          <article-title>Simulation of talking faces in the human brain improves auditory speech recognition</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2008</year>
          <volume>105</volume>
          <fpage>6747</fpage>
          <lpage>6752</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.0710826105</pub-id>
          <pub-id pub-id-type="pmid">18436648</pub-id>
        </element-citation>
      </ref>
      <ref id="B21-brainsci-13-00636">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
<name><surname>Giraud</surname><given-names>A.L.</given-names></name>
</person-group>
          <article-title>Implicit multisensory associations influence voice recognition</article-title>
          <source>PLoS Biol.</source>
          <year>2006</year>
          <volume>4</volume>
          <elocation-id>e326</elocation-id>
          <pub-id pub-id-type="doi">10.1371/journal.pbio.0040326</pub-id>
          <pub-id pub-id-type="pmid">17002519</pub-id>
        </element-citation>
      </ref>
      <ref id="B22-brainsci-13-00636">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Schall</surname><given-names>S.</given-names></name>
<name><surname>Kiebel</surname><given-names>S.J.</given-names></name>
<name><surname>Maess</surname><given-names>B.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Early auditory sensory processing of voices is facilitated by visual mechanisms</article-title>
          <source>Neuroimage</source>
          <year>2013</year>
          <volume>77</volume>
          <fpage>237</fpage>
          <lpage>245</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.043</pub-id>
          <pub-id pub-id-type="pmid">23563227</pub-id>
        </element-citation>
      </ref>
      <ref id="B23-brainsci-13-00636">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Blank</surname><given-names>H.</given-names></name>
<name><surname>Anwander</surname><given-names>A.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Direct structural connections between voice- and face-recognition areas</article-title>
          <source>J. Neurosci.</source>
          <year>2011</year>
          <volume>31</volume>
          <fpage>12906</fpage>
          <lpage>12915</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2091-11.2011</pub-id>
          <pub-id pub-id-type="pmid">21900569</pub-id>
        </element-citation>
      </ref>
      <ref id="B24-brainsci-13-00636">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Blank</surname><given-names>H.</given-names></name>
<name><surname>Kiebel</surname><given-names>S.J.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>How the human brain exchanges information across sensory modalities to recognize other people</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2015</year>
          <volume>36</volume>
          <fpage>324</fpage>
          <lpage>339</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.22631</pub-id>
          <pub-id pub-id-type="pmid">25220190</pub-id>
        </element-citation>
      </ref>
      <ref id="B25-brainsci-13-00636">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Benetti</surname><given-names>S.</given-names></name>
<name><surname>Novello</surname><given-names>L.</given-names></name>
<name><surname>Maffei</surname><given-names>C.</given-names></name>
<name><surname>Rabini</surname><given-names>G.</given-names></name>
<name><surname>Jovicich</surname><given-names>J.</given-names></name>
<name><surname>Collignon</surname><given-names>O.</given-names></name>
</person-group>
          <article-title>White matter connectivity between occipital and temporal regions involved in face and voice processing in hearing and early deaf individuals</article-title>
          <source>Neuroimage</source>
          <year>2018</year>
          <volume>179</volume>
          <fpage>263</fpage>
          <lpage>274</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.044</pub-id>
          <pub-id pub-id-type="pmid">29908936</pub-id>
        </element-citation>
      </ref>
      <ref id="B26-brainsci-13-00636">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Campanella</surname><given-names>S.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>Integrating face and voice in person perception</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2007</year>
          <volume>11</volume>
          <fpage>535</fpage>
          <lpage>543</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2007.10.001</pub-id>
          <pub-id pub-id-type="pmid">17997124</pub-id>
        </element-citation>
      </ref>
      <ref id="B27-brainsci-13-00636">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Schweinberger</surname><given-names>S.R.</given-names></name>
<name><surname>Kloth</surname><given-names>N.</given-names></name>
<name><surname>Robertson</surname><given-names>D.M.</given-names></name>
</person-group>
          <article-title>Hearing facial identities: Brain correlates of face--voice integration in person identification</article-title>
          <source>Cortex</source>
          <year>2011</year>
          <volume>47</volume>
          <fpage>1026</fpage>
          <lpage>1037</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cortex.2010.11.011</pub-id>
          <pub-id pub-id-type="pmid">21208611</pub-id>
        </element-citation>
      </ref>
      <ref id="B28-brainsci-13-00636">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Yovel</surname><given-names>G.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>A unified coding strategy for processing faces and voices</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2013</year>
          <volume>17</volume>
          <fpage>263</fpage>
          <lpage>271</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2013.04.004</pub-id>
          <pub-id pub-id-type="pmid">23664703</pub-id>
        </element-citation>
      </ref>
      <ref id="B29-brainsci-13-00636">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Awwad Shiekh Hasan</surname><given-names>B.</given-names></name>
<name><surname>Valdes-Sosa</surname><given-names>M.</given-names></name>
<name><surname>Gross</surname><given-names>J.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>“Hearing faces and seeing voices”: Amodal coding of person identity in the human brain</article-title>
          <source>Sci. Rep.</source>
          <year>2016</year>
          <volume>6</volume>
          <fpage>37494</fpage>
          <pub-id pub-id-type="doi">10.1038/srep37494</pub-id>
          <pub-id pub-id-type="pmid">27881866</pub-id>
        </element-citation>
      </ref>
      <ref id="B30-brainsci-13-00636">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Pascual-Leone</surname><given-names>A.</given-names></name>
<name><surname>Hamilton</surname><given-names>R.</given-names></name>
</person-group>
          <article-title>The metamodal organization of the brain</article-title>
          <source>Prog. Brain Res.</source>
          <year>2001</year>
          <volume>134</volume>
          <fpage>427</fpage>
          <lpage>445</lpage>
          <pub-id pub-id-type="doi">10.1016/s0079-6123(01)34028-1</pub-id>
          <pub-id pub-id-type="pmid">11702559</pub-id>
        </element-citation>
      </ref>
      <ref id="B31-brainsci-13-00636">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Reich</surname><given-names>L.</given-names></name>
<name><surname>Maidenbaum</surname><given-names>S.</given-names></name>
<name><surname>Amedi</surname><given-names>A.</given-names></name>
</person-group>
          <article-title>The brain as a flexible task machine: Implications for visual rehabilitation using noninvasive vs. invasive approaches</article-title>
          <source>Curr. Opin. Neurol.</source>
          <year>2012</year>
          <volume>25</volume>
          <fpage>86</fpage>
          <lpage>95</lpage>
          <pub-id pub-id-type="doi">10.1097/WCO.0b013e32834ed723</pub-id>
          <pub-id pub-id-type="pmid">22157107</pub-id>
        </element-citation>
      </ref>
      <ref id="B32-brainsci-13-00636">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Amedi</surname><given-names>A.</given-names></name>
<name><surname>Hofstetter</surname><given-names>S.</given-names></name>
<name><surname>Maidenbaum</surname><given-names>S.</given-names></name>
<name><surname>Heimler</surname><given-names>B.</given-names></name>
</person-group>
          <article-title>Task Selectivity as a Comprehensive Principle for Brain Organization</article-title>
          <source>Trends Cogn. Sci.</source>
          <year>2017</year>
          <volume>21</volume>
          <fpage>307</fpage>
          <lpage>310</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2017.03.007</pub-id>
          <pub-id pub-id-type="pmid">28385460</pub-id>
        </element-citation>
      </ref>
      <ref id="B33-brainsci-13-00636">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Gougoux</surname><given-names>F.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>Voss</surname><given-names>P.</given-names></name>
<name><surname>Lepore</surname><given-names>F.</given-names></name>
<name><surname>Lassonde</surname><given-names>M.</given-names></name>
<name><surname>Zatorre</surname><given-names>R.J.</given-names></name>
</person-group>
          <article-title>Voice perception in blind persons: A functional magnetic resonance imaging study</article-title>
          <source>Neuropsychologia</source>
          <year>2009</year>
          <volume>47</volume>
          <fpage>2967</fpage>
          <lpage>2974</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.06.027</pub-id>
          <pub-id pub-id-type="pmid">19576235</pub-id>
        </element-citation>
      </ref>
      <ref id="B34-brainsci-13-00636">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Dormal</surname><given-names>G.</given-names></name>
<name><surname>Pelland</surname><given-names>M.</given-names></name>
<name><surname>Rezk</surname><given-names>M.</given-names></name>
<name><surname>Yakobov</surname><given-names>E.</given-names></name>
<name><surname>Lepore</surname><given-names>F.</given-names></name>
<name><surname>Collignon</surname><given-names>O.</given-names></name>
</person-group>
          <article-title>Functional Preference for Object Sounds and Voices in the Brain of Early Blind and Sighted Individuals</article-title>
          <source>J. Cogn. Neurosci.</source>
          <year>2018</year>
          <volume>30</volume>
          <fpage>86</fpage>
          <lpage>106</lpage>
          <pub-id pub-id-type="doi">10.1162/jocn_a_01186</pub-id>
          <pub-id pub-id-type="pmid">28891782</pub-id>
        </element-citation>
      </ref>
      <ref id="B35-brainsci-13-00636">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Holig</surname><given-names>C.</given-names></name>
<name><surname>Focker</surname><given-names>J.</given-names></name>
<name><surname>Best</surname><given-names>A.</given-names></name>
<name><surname>Roder</surname><given-names>B.</given-names></name>
<name><surname>Buchel</surname><given-names>C.</given-names></name>
</person-group>
          <article-title>Brain systems mediating voice identity processing in blind humans</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2014</year>
          <volume>35</volume>
          <fpage>4607</fpage>
          <lpage>4619</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.22498</pub-id>
          <pub-id pub-id-type="pmid">24639401</pub-id>
        </element-citation>
      </ref>
      <ref id="B36-brainsci-13-00636">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Fairhall</surname><given-names>S.L.</given-names></name>
<name><surname>Porter</surname><given-names>K.B.</given-names></name>
<name><surname>Bellucci</surname><given-names>C.</given-names></name>
<name><surname>Mazzetti</surname><given-names>M.</given-names></name>
<name><surname>Cipolli</surname><given-names>C.</given-names></name>
<name><surname>Gobbini</surname><given-names>M.I.</given-names></name>
</person-group>
          <article-title>Plastic reorganization of neural systems for perception of others in the congenitally blind</article-title>
          <source>Neuroimage</source>
          <year>2017</year>
          <volume>158</volume>
          <fpage>126</fpage>
          <lpage>135</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.057</pub-id>
          <pub-id pub-id-type="pmid">28669909</pub-id>
        </element-citation>
      </ref>
      <ref id="B37-brainsci-13-00636">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Klinge</surname><given-names>C.</given-names></name>
<name><surname>Roder</surname><given-names>B.</given-names></name>
<name><surname>Buchel</surname><given-names>C.</given-names></name>
</person-group>
          <article-title>Increased amygdala activation to emotional auditory stimuli in the blind</article-title>
          <source>Brain</source>
          <year>2010</year>
          <volume>133</volume>
          <fpage>1729</fpage>
          <lpage>1736</lpage>
          <pub-id pub-id-type="doi">10.1093/brain/awq102</pub-id>
          <pub-id pub-id-type="pmid">20453040</pub-id>
        </element-citation>
      </ref>
      <ref id="B38-brainsci-13-00636">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Boldt</surname><given-names>R.</given-names></name>
<name><surname>Seppa</surname><given-names>M.</given-names></name>
<name><surname>Malinen</surname><given-names>S.</given-names></name>
<name><surname>Tikka</surname><given-names>P.</given-names></name>
<name><surname>Hari</surname><given-names>R.</given-names></name>
<name><surname>Carlson</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Spatial variability of functional brain networks in early-blind and sighted subjects</article-title>
          <source>Neuroimage</source>
          <year>2014</year>
          <volume>95</volume>
          <fpage>208</fpage>
          <lpage>216</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.058</pub-id>
          <pub-id pub-id-type="pmid">24680867</pub-id>
        </element-citation>
      </ref>
      <ref id="B39-brainsci-13-00636">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Burton</surname><given-names>H.</given-names></name>
<name><surname>Snyder</surname><given-names>A.Z.</given-names></name>
<name><surname>Raichle</surname><given-names>M.E.</given-names></name>
</person-group>
          <article-title>Resting state functional connectivity in early blind humans</article-title>
          <source>Front. Syst. Neurosci.</source>
          <year>2014</year>
          <volume>8</volume>
          <fpage>51</fpage>
          <pub-id pub-id-type="doi">10.3389/fnsys.2014.00051</pub-id>
          <pub-id pub-id-type="pmid">24778608</pub-id>
        </element-citation>
      </ref>
      <ref id="B40-brainsci-13-00636">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Yu</surname><given-names>C.</given-names></name>
<name><surname>Liang</surname><given-names>M.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Tian</surname><given-names>L.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Qin</surname><given-names>W.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
</person-group>
          <article-title>Whole brain functional connectivity in the early blind</article-title>
          <source>Brain</source>
          <year>2007</year>
          <volume>130</volume>
          <fpage>2085</fpage>
          <lpage>2096</lpage>
          <pub-id pub-id-type="doi">10.1093/brain/awm121</pub-id>
          <pub-id pub-id-type="pmid">17533167</pub-id>
        </element-citation>
      </ref>
      <ref id="B41-brainsci-13-00636">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Yu</surname><given-names>C.</given-names></name>
<name><surname>Liu</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>J.</given-names></name>
<name><surname>Zhou</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Tian</surname><given-names>L.</given-names></name>
<name><surname>Qin</surname><given-names>W.</given-names></name>
<name><surname>Jiang</surname><given-names>T.</given-names></name>
<name><surname>Li</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Altered functional connectivity of primary visual cortex in early blindness</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2008</year>
          <volume>29</volume>
          <fpage>533</fpage>
          <lpage>543</lpage>
          <pub-id pub-id-type="doi">10.1002/hbm.20420</pub-id>
          <pub-id pub-id-type="pmid">17525980</pub-id>
        </element-citation>
      </ref>
      <ref id="B42-brainsci-13-00636">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Focker</surname><given-names>J.</given-names></name>
<name><surname>Best</surname><given-names>A.</given-names></name>
<name><surname>Holig</surname><given-names>C.</given-names></name>
<name><surname>Roder</surname><given-names>B.</given-names></name>
</person-group>
          <article-title>The superiority in voice processing of the blind arises from neural plasticity at sensory processing stages</article-title>
          <source>Neuropsychologia</source>
          <year>2012</year>
          <volume>50</volume>
          <fpage>2056</fpage>
          <lpage>2067</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2012.05.006</pub-id>
          <pub-id pub-id-type="pmid">22588063</pub-id>
        </element-citation>
      </ref>
      <ref id="B43-brainsci-13-00636">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Pang</surname><given-names>W.</given-names></name>
<name><surname>Xing</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Shu</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
</person-group>
          <article-title>Superiority of blind over sighted listeners in voice recognition</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2020</year>
          <volume>148</volume>
          <fpage>EL208</fpage>
          <pub-id pub-id-type="doi">10.1121/10.0001804</pub-id>
          <pub-id pub-id-type="pmid">32873006</pub-id>
        </element-citation>
      </ref>
      <ref id="B44-brainsci-13-00636">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Zhou</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>Y.</given-names></name>
<name><surname>Shu</surname><given-names>H.</given-names></name>
</person-group>
          <article-title>Language-familiarity effect on voice recognition by blind listeners</article-title>
          <source>JASA Express Lett.</source>
          <year>2021</year>
          <volume>1</volume>
          <fpage>055201</fpage>
          <pub-id pub-id-type="doi">10.1121/10.0004848</pub-id>
          <pub-id pub-id-type="pmid">36154110</pub-id>
        </element-citation>
      </ref>
      <ref id="B45-brainsci-13-00636">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Oldfield</surname><given-names>R.C.</given-names></name>
</person-group>
          <article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title>
          <source>Neuropsychologia</source>
          <year>1971</year>
          <volume>9</volume>
          <fpage>97</fpage>
          <lpage>113</lpage>
          <pub-id pub-id-type="doi">10.1016/0028-3932(71)90067-4</pub-id>
          <pub-id pub-id-type="pmid">5146491</pub-id>
        </element-citation>
      </ref>
      <ref id="B46-brainsci-13-00636">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Shu</surname><given-names>H.</given-names></name>
<name><surname>Zhang</surname><given-names>L.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Gong</surname><given-names>Q.</given-names></name>
<name><surname>Li</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>Cortical competition during language discrimination</article-title>
          <source>Neuroimage</source>
          <year>2008</year>
          <volume>43</volume>
          <fpage>624</fpage>
          <lpage>633</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.07.025</pub-id>
          <pub-id pub-id-type="pmid">18694836</pub-id>
        </element-citation>
      </ref>
      <ref id="B47-brainsci-13-00636">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Perrachione</surname><given-names>T.K.</given-names></name>
<name><surname>Del Tufo</surname><given-names>S.N.</given-names></name>
<name><surname>Gabrieli</surname><given-names>J.D.E.</given-names></name>
</person-group>
          <article-title>Human Voice Recognition Depends on Language Ability</article-title>
          <source>Science</source>
          <year>2011</year>
          <volume>333</volume>
          <fpage>595</fpage>
          <pub-id pub-id-type="doi">10.1126/science.1207327</pub-id>
          <pub-id pub-id-type="pmid">21798942</pub-id>
        </element-citation>
      </ref>
      <ref id="B48-brainsci-13-00636">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Xie</surname><given-names>X.</given-names></name>
<name><surname>Myers</surname><given-names>E.</given-names></name>
</person-group>
          <article-title>The impact of musical training and tone language experience on talker identification</article-title>
          <source>J. Acoust. Soc. Am.</source>
          <year>2015</year>
          <volume>137</volume>
          <fpage>419</fpage>
          <lpage>432</lpage>
          <pub-id pub-id-type="doi">10.1121/1.4904699</pub-id>
          <pub-id pub-id-type="pmid">25618071</pub-id>
        </element-citation>
      </ref>
      <ref id="B49-brainsci-13-00636">
        <label>49.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Chao-Gan</surname><given-names>Y.</given-names></name>
<name><surname>Yu-Feng</surname><given-names>Z.</given-names></name>
</person-group>
          <article-title>DPARSF: A MATLAB Toolbox for “Pipeline” Data Analysis of Resting-State fMRI</article-title>
          <source>Front. Syst. Neurosci.</source>
          <year>2010</year>
          <volume>4</volume>
          <fpage>13</fpage>
          <pub-id pub-id-type="doi">10.3389/fnsys.2010.00013</pub-id>
          <pub-id pub-id-type="pmid">20577591</pub-id>
        </element-citation>
      </ref>
      <ref id="B50-brainsci-13-00636">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Friston</surname><given-names>K.J.</given-names></name>
<name><surname>Williams</surname><given-names>S.</given-names></name>
<name><surname>Howard</surname><given-names>R.</given-names></name>
<name><surname>Frackowiak</surname><given-names>R.S.J.</given-names></name>
<name><surname>Turner</surname><given-names>R.</given-names></name>
</person-group>
          <article-title>Movement-Related effects in fMRI time-series</article-title>
          <source>Magn. Reson. Med.</source>
          <year>1996</year>
          <volume>35</volume>
          <fpage>346</fpage>
          <lpage>355</lpage>
          <pub-id pub-id-type="doi">10.1002/mrm.1910350312</pub-id>
          <pub-id pub-id-type="pmid">8699946</pub-id>
        </element-citation>
      </ref>
      <ref id="B51-brainsci-13-00636">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Ashburner</surname><given-names>J.</given-names></name>
</person-group>
          <article-title>A fast diffeomorphic image registration algorithm</article-title>
          <source>Neuroimage</source>
          <year>2007</year>
          <volume>38</volume>
          <fpage>95</fpage>
          <lpage>113</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.07.007</pub-id>
          <pub-id pub-id-type="pmid">17761438</pub-id>
        </element-citation>
      </ref>
      <ref id="B52-brainsci-13-00636">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Van Lancker</surname><given-names>D.R.</given-names></name>
<name><surname>Canter</surname><given-names>G.J.</given-names></name>
</person-group>
          <article-title>Impairment of voice and face recognition in patients with hemispheric damage</article-title>
          <source>Brain Cogn.</source>
          <year>1982</year>
          <volume>1</volume>
          <fpage>185</fpage>
          <lpage>195</lpage>
          <pub-id pub-id-type="doi">10.1016/0278-2626(82)90016-1</pub-id>
          <pub-id pub-id-type="pmid">6927560</pub-id>
        </element-citation>
      </ref>
      <ref id="B53-brainsci-13-00636">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Bestelmeyer</surname><given-names>P.E.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>Grosbras</surname><given-names>M.H.</given-names></name>
</person-group>
          <article-title>Right temporal TMS impairs voice detection</article-title>
          <source>Curr. Biol.</source>
          <year>2011</year>
          <volume>21</volume>
          <fpage>R838</fpage>
          <lpage>R839</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cub.2011.08.046</pub-id>
          <pub-id pub-id-type="pmid">22032183</pub-id>
        </element-citation>
      </ref>
      <ref id="B54-brainsci-13-00636">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Diedenhofen</surname><given-names>B.</given-names></name>
<name><surname>Musch</surname><given-names>J.</given-names></name>
</person-group>
          <article-title>cocor: A Comprehensive Solution for the Statistical Comparison of Correlations</article-title>
          <source>PLoS ONE</source>
          <year>2015</year>
          <volume>10</volume>
          <elocation-id>e0121945</elocation-id>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0121945</pub-id>
          <pub-id pub-id-type="pmid">25835001</pub-id>
        </element-citation>
      </ref>
      <ref id="B55-brainsci-13-00636">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Todorov</surname><given-names>A.</given-names></name>
<name><surname>Engell</surname><given-names>A.D.</given-names></name>
</person-group>
          <article-title>The role of the amygdala in implicit evaluation of emotionally neutral faces</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2008</year>
          <volume>3</volume>
          <fpage>303</fpage>
          <lpage>312</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nsn033</pub-id>
          <pub-id pub-id-type="pmid">19015082</pub-id>
        </element-citation>
      </ref>
      <ref id="B56-brainsci-13-00636">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Ajina</surname><given-names>S.</given-names></name>
<name><surname>Pollard</surname><given-names>M.</given-names></name>
<name><surname>Bridge</surname><given-names>H.</given-names></name>
</person-group>
          <article-title>The Superior Colliculus and Amygdala Support Evaluation of Face Trait in Blindsight</article-title>
          <source>Front. Neurol.</source>
          <year>2020</year>
          <volume>11</volume>
          <fpage>769</fpage>
          <pub-id pub-id-type="doi">10.3389/fneur.2020.00769</pub-id>
          <pub-id pub-id-type="pmid">32765417</pub-id>
        </element-citation>
      </ref>
      <ref id="B57-brainsci-13-00636">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Latinus</surname><given-names>M.</given-names></name>
<name><surname>Crabbe</surname><given-names>F.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>Learning-induced changes in the cerebral processing of voice identity</article-title>
          <source>Cereb. Cortex</source>
          <year>2011</year>
          <volume>21</volume>
          <fpage>2820</fpage>
          <lpage>2828</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhr077</pub-id>
          <pub-id pub-id-type="pmid">21531779</pub-id>
        </element-citation>
      </ref>
      <ref id="B58-brainsci-13-00636">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>McGaugh</surname><given-names>J.L.</given-names></name>
<name><surname>Cahill</surname><given-names>L.</given-names></name>
<name><surname>Roozendaal</surname><given-names>B.</given-names></name>
</person-group>
          <article-title>Involvement of the amygdala in memory storage: Interaction with other brain systems</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>1996</year>
          <volume>93</volume>
          <fpage>13508</fpage>
          <lpage>13514</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.93.24.13508</pub-id>
          <pub-id pub-id-type="pmid">8942964</pub-id>
        </element-citation>
      </ref>
      <ref id="B59-brainsci-13-00636">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>McGaugh</surname><given-names>J.L.</given-names></name>
</person-group>
          <article-title>Memory consolidation and the amygdala: A systems perspective</article-title>
          <source>Trends Neurosci.</source>
          <year>2002</year>
          <volume>25</volume>
          <fpage>456</fpage>
          <lpage>461</lpage>
          <pub-id pub-id-type="doi">10.1016/S0166-2236(02)02211-7</pub-id>
          <pub-id pub-id-type="pmid">12183206</pub-id>
        </element-citation>
      </ref>
      <ref id="B60-brainsci-13-00636">
        <label>60.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Roozendaal</surname><given-names>B.</given-names></name>
<name><surname>McEwen</surname><given-names>B.S.</given-names></name>
<name><surname>Chattarji</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Stress, memory and the amygdala</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <year>2009</year>
          <volume>10</volume>
          <fpage>423</fpage>
          <lpage>433</lpage>
          <pub-id pub-id-type="doi">10.1038/nrn2651</pub-id>
          <pub-id pub-id-type="pmid">19469026</pub-id>
        </element-citation>
      </ref>
      <ref id="B61-brainsci-13-00636">
        <label>61.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Focker</surname><given-names>J.</given-names></name>
<name><surname>Holig</surname><given-names>C.</given-names></name>
<name><surname>Best</surname><given-names>A.</given-names></name>
<name><surname>Roder</surname><given-names>B.</given-names></name>
</person-group>
          <article-title>Neural plasticity of voice processing: Evidence from event-related potentials in late-onset blind and sighted individuals</article-title>
          <source>Restor. Neurol. Neurosci.</source>
          <year>2015</year>
          <volume>33</volume>
          <fpage>15</fpage>
          <lpage>30</lpage>
          <pub-id pub-id-type="doi">10.3233/RNN-140406</pub-id>
          <pub-id pub-id-type="pmid">25374347</pub-id>
        </element-citation>
      </ref>
      <ref id="B62-brainsci-13-00636">
        <label>62.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Maguinness</surname><given-names>C.</given-names></name>
<name><surname>Roswandowitz</surname><given-names>C.</given-names></name>
<name><surname>von Kriegstein</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Understanding the mechanisms of familiar voice-identity recognition in the human brain</article-title>
          <source>Neuropsychologia</source>
          <year>2018</year>
          <volume>116</volume>
          <fpage>179</fpage>
          <lpage>193</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.03.039</pub-id>
          <pub-id pub-id-type="pmid">29614253</pub-id>
        </element-citation>
      </ref>
      <ref id="B63-brainsci-13-00636">
        <label>63.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Schweinberger</surname><given-names>S.R.</given-names></name>
<name><surname>Kaufmann</surname><given-names>J.M.</given-names></name>
<name><surname>Moratti</surname><given-names>S.</given-names></name>
<name><surname>Keil</surname><given-names>A.</given-names></name>
<name><surname>Burton</surname><given-names>A.M.</given-names></name>
</person-group>
          <article-title>Brain responses to repetitions of human and animal faces, inverted faces, and objects: An MEG study</article-title>
          <source>Brain Res.</source>
          <year>2007</year>
          <volume>1184</volume>
          <fpage>226</fpage>
          <lpage>233</lpage>
          <pub-id pub-id-type="doi">10.1016/j.brainres.2007.09.079</pub-id>
          <pub-id pub-id-type="pmid">17976538</pub-id>
        </element-citation>
      </ref>
      <ref id="B64-brainsci-13-00636">
        <label>64.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Watson</surname><given-names>R.</given-names></name>
<name><surname>Latinus</surname><given-names>M.</given-names></name>
<name><surname>Charest</surname><given-names>I.</given-names></name>
<name><surname>Crabbe</surname><given-names>F.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>People-selectivity, audiovisual integration and heteromodality in the superior temporal sulcus</article-title>
          <source>Cortex</source>
          <year>2014</year>
          <volume>50</volume>
          <fpage>125</fpage>
          <lpage>136</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cortex.2013.07.011</pub-id>
          <pub-id pub-id-type="pmid">23988132</pub-id>
        </element-citation>
      </ref>
      <ref id="B65-brainsci-13-00636">
        <label>65.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Smith</surname><given-names>E.L.</given-names></name>
<name><surname>Grabowecky</surname><given-names>M.</given-names></name>
<name><surname>Suzuki</surname><given-names>S.</given-names></name>
</person-group>
          <article-title>Auditory-visual crossmodal integration in perception of face gender</article-title>
          <source>Curr. Biol.</source>
          <year>2007</year>
          <volume>17</volume>
          <fpage>1680</fpage>
          <lpage>1685</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cub.2007.08.043</pub-id>
          <pub-id pub-id-type="pmid">17825561</pub-id>
        </element-citation>
      </ref>
      <ref id="B66-brainsci-13-00636">
        <label>66.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Catani</surname><given-names>M.</given-names></name>
<name><surname>Jones</surname><given-names>D.K.</given-names></name>
<name><surname>Donato</surname><given-names>R.</given-names></name>
<name><surname>Ffytche</surname><given-names>D.H.</given-names></name>
</person-group>
          <article-title>Occipito-temporal connections in the human brain</article-title>
          <source>Brain</source>
          <year>2003</year>
          <volume>126</volume>
          <fpage>2093</fpage>
          <lpage>2107</lpage>
          <pub-id pub-id-type="doi">10.1093/brain/awg203</pub-id>
          <pub-id pub-id-type="pmid">12821517</pub-id>
        </element-citation>
      </ref>
      <ref id="B67-brainsci-13-00636">
        <label>67.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Herrington</surname><given-names>J.D.</given-names></name>
<name><surname>Taylor</surname><given-names>J.M.</given-names></name>
<name><surname>Grupe</surname><given-names>D.W.</given-names></name>
<name><surname>Curby</surname><given-names>K.M.</given-names></name>
<name><surname>Schultz</surname><given-names>R.T.</given-names></name>
</person-group>
          <article-title>Bidirectional communication between amygdala and fusiform gyrus during facial recognition</article-title>
          <source>Neuroimage</source>
          <year>2011</year>
          <volume>56</volume>
          <fpage>2348</fpage>
          <lpage>2355</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.03.072</pub-id>
          <pub-id pub-id-type="pmid">21497657</pub-id>
        </element-citation>
      </ref>
      <ref id="B68-brainsci-13-00636">
        <label>68.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Kann</surname><given-names>S.J.</given-names></name>
<name><surname>O’Rawe</surname><given-names>J.F.</given-names></name>
<name><surname>Huang</surname><given-names>A.S.</given-names></name>
<name><surname>Klein</surname><given-names>D.N.</given-names></name>
<name><surname>Leung</surname><given-names>H.C.</given-names></name>
</person-group>
          <article-title>Preschool negative emotionality predicts activity and connectivity of the fusiform face area and amygdala in later childhood</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2017</year>
          <volume>12</volume>
          <fpage>1511</fpage>
          <lpage>1519</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nsx079</pub-id>
          <pub-id pub-id-type="pmid">28992271</pub-id>
        </element-citation>
      </ref>
      <ref id="B69-brainsci-13-00636">
        <label>69.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Wang</surname><given-names>J.</given-names></name>
<name><surname>Wei</surname><given-names>Q.</given-names></name>
<name><surname>Bai</surname><given-names>T.</given-names></name>
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>H.</given-names></name>
<name><surname>Becker</surname><given-names>B.</given-names></name>
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Wang</surname><given-names>K.</given-names></name>
<name><surname>Kendrick</surname><given-names>K.</given-names></name>
</person-group>
          <article-title>Electroconvulsive therapy selectively enhanced feedforward connectivity from fusiform face area to amygdala in major depressive disorder</article-title>
          <source>Soc. Cogn. Affect. Neurosci.</source>
          <year>2017</year>
          <volume>12</volume>
          <fpage>1983</fpage>
          <lpage>1992</lpage>
          <pub-id pub-id-type="doi">10.1093/scan/nsx100</pub-id>
          <pub-id pub-id-type="pmid">28981882</pub-id>
        </element-citation>
      </ref>
      <ref id="B70-brainsci-13-00636">
        <label>70.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Davies-Thompson</surname><given-names>J.</given-names></name>
<name><surname>Andrews</surname><given-names>T.J.</given-names></name>
</person-group>
          <article-title>Intra- and interhemispheric connectivity between face-selective regions in the human brain</article-title>
          <source>J. Neurophysiol.</source>
          <year>2012</year>
          <volume>108</volume>
          <fpage>3087</fpage>
          <lpage>3095</lpage>
          <pub-id pub-id-type="doi">10.1152/jn.01171.2011</pub-id>
          <pub-id pub-id-type="pmid">22972952</pub-id>
        </element-citation>
      </ref>
      <ref id="B71-brainsci-13-00636">
        <label>71.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Pascual-Leone</surname><given-names>A.</given-names></name>
<name><surname>Amedi</surname><given-names>A.</given-names></name>
<name><surname>Fregni</surname><given-names>F.</given-names></name>
<name><surname>Merabet</surname><given-names>L.B.</given-names></name>
</person-group>
          <article-title>The plastic human brain cortex</article-title>
          <source>Annu. Rev. Neurosci.</source>
          <year>2005</year>
          <volume>28</volume>
          <fpage>377</fpage>
          <lpage>401</lpage>
          <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144216</pub-id>
          <pub-id pub-id-type="pmid">16022601</pub-id>
        </element-citation>
      </ref>
      <ref id="B72-brainsci-13-00636">
        <label>72.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>van den Hurk</surname><given-names>J.</given-names></name>
<name><surname>Van Baelen</surname><given-names>M.</given-names></name>
<name><surname>Op de Beeck</surname><given-names>H.P.</given-names></name>
</person-group>
          <article-title>Development of visual category selectivity in ventral visual cortex does not require visual experience</article-title>
          <source>Proc. Natl. Acad. Sci. USA</source>
          <year>2017</year>
          <volume>114</volume>
          <fpage>E4501</fpage>
          <lpage>E4510</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.1612862114</pub-id>
          <pub-id pub-id-type="pmid">28507127</pub-id>
        </element-citation>
      </ref>
      <ref id="B73-brainsci-13-00636">
        <label>73.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
<name><surname>Aglieri</surname><given-names>V.</given-names></name>
<name><surname>Chaminade</surname><given-names>T.</given-names></name>
<name><surname>Takerkart</surname><given-names>S.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
</person-group>
          <article-title>Functional connectivity within the voice perception network and its behavioural relevance</article-title>
          <source>Neuroimage</source>
          <year>2018</year>
          <volume>183</volume>
          <fpage>356</fpage>
          <lpage>365</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.011</pub-id>
          <pub-id pub-id-type="pmid">30099078</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
  <floats-group>
    <fig position="float" id="brainsci-13-00636-f001">
      <label>Figure 1</label>
      <caption>
        <p>Illustration of behavioral experimental design. (<bold>a</bold>) Familiarization phase: participants heard a number designating the speaker followed by a training sentence read by that speaker. Then participants pressed the key to begin the next trial. (<bold>b</bold>) Practice phase: after hearing a sentence, participants were asked to type in the number of the speaker. Correct responses were followed by a cue tone (“Ding”). Incorrect responses were followed by a cue tone (“DiDi”), then the correct number of the speaker was presented. (<bold>c</bold>) Generalization phase (GP): after hearing a sentence, participants were asked to enter the number of the speaker on the keyboard without feedback. (<bold>d</bold>) Delayed memory phase (DP): the stimuli and procedure were the same as in the generalization phase.</p>
      </caption>
      <graphic xlink:href="brainsci-13-00636-g001" position="float"/>
    </fig>
    <fig position="float" id="brainsci-13-00636-f002">
      <label>Figure 2</label>
      <caption>
        <p>Mean voice-recognition performance of the sighted and early blind participants in each condition. Abbreviations: CN, Chinese condition; JP, Japanese condition; GP, Generalization phase; DP, Delayed memory phase; SC, Sighted control; EB, Early blind. Significance indicated by * (<italic toggle="yes">p</italic> &lt; 0.05), ** (<italic toggle="yes">p</italic> &lt; 0.01), and *** (<italic toggle="yes">p</italic> &lt; 0.001). Error bars represent the standard error of the mean.</p>
      </caption>
      <graphic xlink:href="brainsci-13-00636-g002" position="float"/>
    </fig>
    <fig position="float" id="brainsci-13-00636-f003">
      <label>Figure 3</label>
      <caption>
        <p>The between-group difference in the seed-based FCs. (<bold>a</bold>) Connectivity matrix reporting average FC between 6 ROIs of the sighted (SC) and blind (EB) participants. The color bar represents the Fisher <italic toggle="yes">z</italic>-scores of FCs. (<bold>b</bold>) The color bar represents the <italic toggle="yes">t</italic>-values for the contrast (SC vs. EB). Significance indicated by * (<italic toggle="yes">p</italic>-FDR &lt; 0.05) and *** (<italic toggle="yes">p</italic>-FDR &lt; 0.001). (<bold>c</bold>) The sagittal views show the significant differences in ROI-to-ROI FCs between the sighted and blind participants. Left panel: Changes of FCs within voice-sensitive areas; Right panel: Changes of FCs between the voice- and face-sensitive areas. Abbreviations: FFA, fusiform face area; OFA, occipital face area; TVAp, posterior “temporal voice areas”; AMY, amygdala; TVAa, anterior “temporal voice areas”; IFG, inferior frontal gyrus. Red lines represent enhanced connectivity in the blind (EB &gt; SC); Green lines represent decreased connectivity in the blind (SC &gt; EB).</p>
      </caption>
      <graphic xlink:href="brainsci-13-00636-g003" position="float"/>
    </fig>
    <fig position="float" id="brainsci-13-00636-f004">
      <label>Figure 4</label>
      <caption>
        <p>FC-behavior correlations. (<bold>a</bold>) Correlation analyses between the mean FC strength and mean voice-recognition accuracy of the sighted (SC) and blind (EB) participants. Abbreviations: CN-GP, accuracy in the Chinese condition in the Generalization phase; CN-GP, accuracy in the Chinese condition in the Delayed memory phase; JP-GP, accuracy in the Japanese condition in the Generalization phase; JP-GP, accuracy in the Japanese condition in the Delayed memory phase. The color bar represents Pearson’s <italic toggle="yes">r</italic>. Significance indicated by † (<italic toggle="yes">p</italic>-FDR = 0.075), * (<italic toggle="yes">p</italic>-FDR &lt; 0.05) and *** (<italic toggle="yes">p</italic>-FDR &lt; 0.001). (<bold>b</bold>) Scatterplots show the significant correlations (after FDR corrected) between the ROI-to-ROI FCs and the voice-recognition performance accuracy.</p>
      </caption>
      <graphic xlink:href="brainsci-13-00636-g004" position="float"/>
    </fig>
  </floats-group>
</article>
