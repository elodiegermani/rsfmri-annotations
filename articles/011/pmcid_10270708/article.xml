<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Netw Neurosci</journal-id>
      <journal-id journal-id-type="iso-abbrev">Netw Neurosci</journal-id>
      <journal-id journal-id-type="publisher-id">netn</journal-id>
      <journal-title-group>
        <journal-title>Network Neuroscience</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2472-1751</issn>
      <publisher>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA journals-info@mit.edu</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">37334006</article-id>
      <article-id pub-id-type="pmc">10270708</article-id>
      <article-id pub-id-type="publisher-id">netn_a_00281</article-id>
      <article-id pub-id-type="doi">10.1162/netn_a_00281</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A transformer model for learning spatiotemporal contextual representation in fMRI data</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5102-6927</contrib-id>
          <name>
            <surname>Asadi</surname>
            <given-names>Nima</given-names>
          </name>
          <xref rid="cor1" ref-type="corresp">*</xref>
          <role content-type="https://credit.niso.org/contributor-roles/conceptualization"/>
          <role content-type="https://credit.niso.org/contributor-roles/data-curation"/>
          <role content-type="https://credit.niso.org/contributor-roles/formal-analysis"/>
          <role content-type="https://credit.niso.org/contributor-roles/investigation"/>
          <role content-type="https://credit.niso.org/contributor-roles/methodology"/>
          <role content-type="https://credit.niso.org/contributor-roles/software"/>
          <role content-type="https://credit.niso.org/contributor-roles/visualization"/>
          <role content-type="https://credit.niso.org/contributor-roles/writing-original-draft"/>
          <role content-type="https://credit.niso.org/contributor-roles/writing-review-editing"/>
          <xref rid="aff1" ref-type="aff"/>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8947-6432</contrib-id>
          <name>
            <surname>Olson</surname>
            <given-names>Ingrid R.</given-names>
          </name>
          <role content-type="https://credit.niso.org/contributor-roles/investigation"/>
          <role content-type="https://credit.niso.org/contributor-roles/supervision"/>
          <role content-type="https://credit.niso.org/contributor-roles/validation"/>
          <xref rid="aff2" ref-type="aff"/>
          <xref rid="aff3" ref-type="aff"/>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2051-0142</contrib-id>
          <name>
            <surname>Obradovic</surname>
            <given-names>Zoran</given-names>
          </name>
          <role content-type="https://credit.niso.org/contributor-roles/formal-analysis"/>
          <role content-type="https://credit.niso.org/contributor-roles/supervision"/>
          <role content-type="https://credit.niso.org/contributor-roles/validation"/>
          <xref rid="aff1" ref-type="aff"/>
        </contrib>
      </contrib-group>
      <aff id="aff1">Department of Computer and Information Sciences, College of Science and Technology, Temple University, Philadelphia, PA, USA</aff>
      <aff id="aff2">Department of Psychology and Neuroscience, College of Liberal Arts, Temple University, Philadelphia, PA, USA</aff>
      <aff id="aff3">Decision Neuroscience, College of Liberal Arts, Temple University, Philadelphia, PA, USA</aff>
      <author-notes>
        <fn fn-type="com">
          <p>Competing Interests: The authors have declared that no competing interest exist.</p>
        </fn>
        <corresp id="cor1">* Corresponding Author: <email xlink:href="mailto:nima.asadi@temple.edu">nima.asadi@temple.edu</email></corresp>
        <fn>
          <p>Handling Editor: Vince Calhoun</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <year>2023</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>01</day>
        <month>1</month>
        <year>2023</year>
      </pub-date>
      <volume>7</volume>
      <issue>1</issue>
      <fpage>22</fpage>
      <lpage>47</lpage>
      <history>
<date date-type="received"><day>04</day><month>5</month><year>2022</year></date>
<date date-type="accepted"><day>26</day><month>9</month><year>2022</year></date>
</history>
      <permissions>
        <copyright-statement>Â© 2022 Massachusetts Institute of Technology</copyright-statement>
        <copyright-year>2022</copyright-year>
        <copyright-holder>Massachusetts Institute of Technology</copyright-holder>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
        </license>
      </permissions>
      <self-uri xlink:href="netn-7-1-22.pdf"/>
      <abstract>
        <title>Abstract</title>
        <p>Representation learning is a core component in data-driven modeling of various complex phenomena. Learning a contextually informative representation can especially benefit the analysis of fMRI data because of the complexities and dynamic dependencies present in such datasets. In this work, we propose a framework based on transformer models to learn an embedding of the fMRI data by taking the spatiotemporal contextual information in the data into account. This approach takes the multivariate BOLD time series of the regions of the brain as well as their functional connectivity network simultaneously as the input to create a set of meaningful features that can in turn be used in various downstream tasks such as classification, feature extraction, and statistical analysis. The proposed spatiotemporal framework uses the attention mechanism as well as the graph convolution neural network to jointly inject the contextual information regarding the dynamics in time series data and their connectivity into the representation. We demonstrate the benefits of this framework by applying it to two resting-state fMRI datasets, and provide further discussion on various aspects and advantages of it over a number of other commonly adopted architectures.</p>
      </abstract>
      <kwd-group>
        <kwd>Dynamic functional connectivity</kwd>
        <kwd>Transformer models</kwd>
        <kwd>Attention mechanism</kwd>
        <kwd>Graph convolution networks</kwd>
        <kwd>Feature learning</kwd>
        <kwd>Deep learning</kwd>
      </kwd-group>
      <funding-group specific-use="FundRef">
        <award-group award-type="grant">
          <funding-source>
<institution-wrap><institution>National Institutes of Health</institution></institution-wrap>
</funding-source>
          <award-id>2R56MH091113-11</award-id>
          <principal-award-recipient id="recipient1">
<name><surname>Olson</surname><given-names>Ingrid R.</given-names></name>
<contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8947-6432</contrib-id>
</principal-award-recipient>
        </award-group>
      </funding-group>
      <funding-group specific-use="FundRef">
        <award-group award-type="grant">
          <funding-source>
<institution-wrap><institution>National Institutes of Health</institution></institution-wrap>
</funding-source>
          <award-id>R21HD098509</award-id>
          <principal-award-recipient id="recipient2">
<name><surname>Olson</surname><given-names>Ingrid R.</given-names></name>
<contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8947-6432</contrib-id>
</principal-award-recipient>
        </award-group>
      </funding-group>
      <funding-group specific-use="FundRef">
        <award-group award-type="grant">
          <funding-source>
<institution-wrap><institution>National Institutes of Health</institution></institution-wrap>
</funding-source>
          <award-id>R01HD099165</award-id>
          <principal-award-recipient id="recipient3">
<name><surname>Olson</surname><given-names>Ingrid R.</given-names></name>
<contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8947-6432</contrib-id>
</principal-award-recipient>
        </award-group>
      </funding-group>
      <counts>
        <page-count count="26"/>
      </counts>
      <custom-meta-group>
        <custom-meta>
          <meta-name>citation</meta-name>
          <meta-value>Asadi, N., Olson, I. R., &amp; Obradovic, Z. (2023). A transformer model for learning spatiotemporal contextual representation in fMRI data. <italic toggle="yes">Network Neuroscience</italic>, <italic toggle="yes">7</italic>(1), 22â47. <ext-link xlink:href="https://doi.org/10.1162/netn_a_00281" ext-link-type="uri">https://doi.org/10.1162/netn_a_00281</ext-link></meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>INTRODUCTION</title>
      <p>Analysis and modeling of brainâs blood oxygen levelâdependent (BOLD) activity and functional connectivity (FC) through functional magnetic resonance imaging (fMRI) have led to utilization of an expanding array of methodological tools such as graph theory, machine learning, and statistical tests (<xref rid="bib3" ref-type="bibr">Bastos &amp; Schoffelen, 2016</xref>; <xref rid="bib19" ref-type="bibr">Y. He &amp; Evans, 2010</xref>; <xref rid="bib40" ref-type="bibr">Rogers, Morgan, Newton, &amp; Gore, 2007</xref>). A powerful class of machine learning approaches for building predictive models is the deep architectures of artificial neural networks, also known as deep learning models (<xref rid="bib11" ref-type="bibr">Deng &amp; Yu, 2014</xref>; <xref rid="bib31" ref-type="bibr">LeCun, Bengio, &amp; Hinton, 2015</xref>). Deep learning models are able to capture higher level nonlinearities and to learn informative representations in order to facilitate training a multitude of modeling tasks with little to no requirement for <xref rid="def1" ref-type="def">feature</xref> selection (<xref rid="bib31" ref-type="bibr">LeCun et al., 2015</xref>). This family of predictive models has proven to be a powerful tool for a diverse set of analytical tasks, including feature selection, pattern discovery, feature learning, and predictive modeling (<xref rid="bib41" ref-type="bibr">Sarraf &amp; Tofighi, 2016a</xref>; <xref rid="bib54" ref-type="bibr">Wen et al., 2018</xref>; <xref rid="bib59" ref-type="bibr">Yin, Li, &amp; Wu, 2022</xref>).</p>
      <p>Several deep learning architectures have been utilized recently to analyze fMRI data in areas such as predictive modeling, representation learning, and adversarial data augmentation and synthesis (<xref rid="bib9" ref-type="bibr">Dado et al., 2022</xref>; <xref rid="bib14" ref-type="bibr">Dong et al., 2020</xref>; <xref rid="bib15" ref-type="bibr">Frolov, Maksimenko, LÃ¼ttjohann, Koronovskii, &amp; Hramov, 2019</xref>; <xref rid="bib26" ref-type="bibr">Kawahara et al., 2017</xref>; <xref rid="bib27" ref-type="bibr">J.-H. Kim et al., 2021</xref>; <xref rid="bib32" ref-type="bibr">Li, Satterthwaite, &amp; Fan, 2018</xref>; <xref rid="bib39" ref-type="bibr">Riaz, Asad, Alonso, &amp; Slabaugh, 2020</xref>; <xref rid="bib42" ref-type="bibr">Sarraf &amp; Tofighi, 2016b</xref>; <xref rid="bib44" ref-type="bibr">Suk, Wee, Lee, &amp; Shen, 2016</xref>; <xref rid="bib64" ref-type="bibr">Zhuang, Schwing, &amp; Koyejo, 2019</xref>).</p>
      <p>An important factor in deep learningâs superior performance is its ability in learning an effective representation from the data to facilitate the task of predictive modeling. One of the main objectives of representation learning (also known as feature learning) is informative encoding of the input data; this encoding embeds hidden dependencies and patterns of the data into the learned features to serve several downstream tasks such as regression, classification, imputation, and forecasting (<xref rid="bib33" ref-type="bibr">Liu et al., 2015</xref>; <xref rid="bib62" ref-type="bibr">Zerveas, Jayaraman, Patel, Bhamidipaty, &amp; Eickhoff, 2021</xref>). Encoding has gained significant attention in recent years for disentangling latent characteristics in data in various applications with limited supervision. A representationâs advantage relies on its power in capturing the information from a broad set of characteristics and contextual knowledge in the data (<xref rid="bib4" ref-type="bibr">Bengio, Courville, &amp; Vincent, 2013</xref>). Therefore, in the field of fMRI data analysis, learning a conclusive representation requires obtaining not only the contextual information regarding spatial dependencies but also the variations in connectivity topology through the course of the fMRI experiment. Dynamic functional connectivity (dFC) of the brain is generally highly volatile because of variables such as cognitive tasks and states, as well as spontaneous fluctuations in resting-state BOLD signal, either in normal conditions or during sleep and different levels of anesthesia (<xref rid="bib7" ref-type="bibr">Chen, Nomi, Uddin, Duan, &amp; Chen, 2017</xref>; <xref rid="bib8" ref-type="bibr">Chou et al., 2017</xref>; <xref rid="bib35" ref-type="bibr">Mantini, Perrucci, Del Gratta, Romani, &amp; Corbetta, 2007</xref>). Static functional connectivity analysis fails to capture such dynamics that characterize the interactions and contexts between the activities of different regions of the brain. Therefore, true modeling of functional connectivity requires dynamically capturing time-dependent aspects of spatial dependencies. Popular architectures such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTMs) have been employed for the modeling fMRI data. However, these architectures suffer from certain shortcomings when dealing with large-scale evolutionary graphs (<xref rid="bib43" ref-type="bibr">Scarselli, Gori, Tsoi, Hagenbuchner, &amp; Monfardini, 2008</xref>; <xref rid="bib50" ref-type="bibr">Wan et al., 2019</xref>). These disadvantages include, but are not limited to, lack of true contextual modeling and adaptability with graphâs flexible topology, the inability in preserving information over longer graph âwalks,â and inefficient training time. These shortcomings are addressed by a recently popular architecture called the transformer (<xref rid="bib48" ref-type="bibr">Vaswani et al., 2017</xref>). The transformer is a powerful deep learning model that confers the context for any position in the input sequence by adopting an <xref rid="def2" ref-type="def">attention</xref> mechanism while facilitating efficient parallel training (<xref rid="bib48" ref-type="bibr">Vaswani et al., 2017</xref>; <xref rid="bib55" ref-type="bibr">Wolf et al., 2020</xref>; <xref rid="bib62" ref-type="bibr">Zerveas et al., 2021</xref>). Because of these capabilities, this class of deep learning models has rapidly become the dominant architecture in many complex machine learning tasks and has proven to be adaptable to various structures such as graphs and time series to learn spatial, temporal, and positional context in the data (<xref rid="bib28" ref-type="bibr">T. H. Kim, Sajjadi, Hirsch, &amp; SchÃ¶lkopf, 2018</xref>; <xref rid="bib37" ref-type="bibr">Plizzari, Cannici, &amp; Matteucci, 2021</xref>; <xref rid="bib56" ref-type="bibr">M. Xu et al., 2020</xref>; <xref rid="bib61" ref-type="bibr">C. Yu, Ma, Ren, Zhao, &amp; Yi, 2020</xref>). The attention mechanism is one of the main frontiers in representation learning, which was developed to enhance the encoder-decoder performance on long input sequences. The core idea behind attention on sequence data is that instead of relying merely on the context vector, the decoder also uses the past states and time steps of the encoder. The attention weights are therefore introduced as trainable parameters that assign different importance levels to the different elements of the input sequence. The advantages of attention is its capability in identifying the information in an input element that is most pertinent to carrying out a prediction task with high accuracy (<xref rid="bib48" ref-type="bibr">Vaswani et al., 2017</xref>; <xref rid="bib55" ref-type="bibr">Wolf et al., 2020</xref>).</p>
      <p>Inspired by the proposed <xref rid="def3" ref-type="def">transformer models</xref> for various applications in recent years, in this work we adopt a framework for jointly learning the embedding of spatiotemporal contextual information within fMRI data based on a transformer architecture that utilizes the concepts of attention mechanism as well as <xref rid="def4" ref-type="def">graph convolution network</xref>. The objective of the proposed framework is to learn a set of embedded features that capture a holistic representation regarding the dynamics and dependencies within the fMRI data. For this purpose, the proposed model leverages both the multivariate BOLD time series and the dFC networks simultaneously to learn a representation that takes into account the spatial and temporal contextual relations within both of the mentioned input data components. The extracted representation can then be used in several applications such as classification between cohorts of data, anomaly detection in activation patterns, and feature selection. In this work, the derived contextual representations are utilized for classification tasks and are compared with several commonly used baseline models for assessment. For this purpose, we put forward two binary classification tasks where the model is trained to predict subjects diagnosed with autism spectrum disorder (ASD) from healthy subjects in one task, and the sex of the subjects in the second task.</p>
      <p>In the next section we discuss the different building blocks of the proposed framework, followed by experimental results. We then discuss the advantages and shortcomings of the proposed approach in the discussions.</p>
    </sec>
    <sec id="sec2">
      <title>METHODOLOGY</title>
      <p>In this section, we describe the proposed spatiotemporal transformer framework for representation learning and modeling of activity and dFC of brainâs regions. We first lay out the task of modeling dFC as a classification problem, and then explain the overall architecture of the transformer framework. Afterwards, we describe each building block of the proposed approach in detail. The definitions of the terminologies used in this section are provided in the margin.</p>
      <sec id="sec3">
        <title>Problem Formulation</title>
        <p>Dynamic functional connectivity of the brain can be represented as an evolving graph characterized by varying intensity of interactions between its regions. The dFC network is composed of separate regions of the brain as the nodes, and their coactivation over a temporal window as the weight of the links connecting them. We express this graph as <italic toggle="yes">G</italic> = (<italic toggle="yes">V</italic>, <italic toggle="yes">E</italic>, <italic toggle="yes">T</italic>), where <italic toggle="yes">V</italic> = {<italic toggle="yes">v</italic><sub>1</sub>, <italic toggle="yes">v</italic><sub>2</sub>, â¦, <italic toggle="yes">v</italic><sub><italic toggle="yes">N</italic></sub>} is the set of <italic toggle="yes">N</italic> vertices, <italic toggle="yes">E</italic> is the set of edges, and <italic toggle="yes">T</italic> = {<italic toggle="yes">t</italic><sub>1</sub>, <italic toggle="yes">t</italic><sub>2</sub>, â¦, <italic toggle="yes">t</italic><sub><italic toggle="yes">Ï</italic></sub>} is the set of <italic toggle="yes">Ï</italic> time steps of the experiment during which the dFC graph <italic toggle="yes">G</italic> evolves. To learn the higher order spatiotemporal representation of dependencies in the dFC network, we formulate the modelâs training process as a classification problem with the objective of distinguishing between cohorts of subjects. Through the training process, the weights within the different blocks of the transformer are learned, and the trained model generates the representation of spatiotemporal dependencies <italic toggle="yes">S</italic><sub><italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub></sub> as a vector of features for each node <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> at time step <italic toggle="yes">t</italic>. To learn this new set of features, the transformer leverages the BOLD time series of the brain regions as well as their dFC networks within each temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> simultaneously. The time series are utilized by an attention mechanism to extract the spatial and temporal context for each node <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> at time step <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>, and the functional connectivity network within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> is adopted by a graph convolution network (GCN) to inject the topological information of connectivity into the newly generated features (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>; <xref rid="bib29" ref-type="bibr">Kipf &amp; Welling, 2016</xref>; <xref rid="bib52" ref-type="bibr">Wang, Li, &amp; Hu, 2021</xref>). The output of the two embedding units are then fused together to provide a rich set of features with spatiotemporal contextual knowledge of the data, which can in turn facilitate analysis and prediction tasks. This model can be applied on different spatial resolutions including training on specific regions of interest (ROIs) where the nodes are the voxels within the regions, or at a lower resolution setup where each ROI constitutes a node.</p>
        <p>In the following sections we explain in detail the architecture for spatiotemporal representation learning based on the time series and the dFC network. We then lay out the details of the experimental setup in the <xref rid="sec13" ref-type="sec">Results</xref> section.</p>
      </sec>
      <sec id="sec4">
        <title>Overall Architecture</title>
        <p>To learn the higher order representation of dynamic spatiotemporal dependencies, we develop a two-tier architecture that includes a spatial transformer followed by a temporal transformer. The general schema of this approach is provided in <xref rid="F1" ref-type="fig">Figure 1</xref>, where the spatial and temporal components are placed sequentially within each spatiotemporal (ST) block (the blocks in gray). The ST blocks are also positioned sequentially, meaning that the output of the temporal component of each ST block is used as the input to the spatial transformer of the next ST block, except the final ST block, where the output of the temporal component is supplied to the prediction layer. The input to the first ST block (i.e., the spatial component of the first ST block) is a <xref rid="def5" ref-type="def">positional embedding</xref> of the time series data within the temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> based on a 1 Ã 1 convolution layer, as well as the dFC network constructed based on the coactivations of the BOLD time series within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. As depicted in <xref rid="F1" ref-type="fig">Figure 1</xref>, the input of the next spatial blocks include the embedding of the features that are the output of the previous temporal block, aggregated with the input to the previous block, as well as the dFC network constructed based on the coactivations within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. The input-output aggregation, also known as residual connection, is widely adopted in deep learning architectures because of its advantage in providing a stable training and enhanced representation in each block (<xref rid="bib18" ref-type="bibr">K. He, Zhang, Ren, &amp; Sun, 2016</xref>; <xref rid="bib25" ref-type="bibr">Jastrzebski et al., 2017</xref>). We also adopt the residual connections within each spatial and temporal transformer because of the same advantages.</p>
        <fig position="float" id="F1">
          <label><bold>Figure 1.</bold>â</label>
          <caption>
            <p>Overall architecture of the transformer model and input batch data preparation. Each block in gray color is a spatiotemporal (ST) block containing one spatial and one temporal transformer.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g001" position="float"/>
        </fig>
        <p>The sequential training process is performed for every batch of time series data until the model converges based on the assigned error metric. In the next step, we describe the batch data preparation process for training our transformer model.</p>
      </sec>
      <sec id="sec5">
        <title>Batch Data Preparation</title>
        <p>After preprocessing the BOLD time series and generating the dFC networks within each temporal window, batch data preparation is needed in order to facilitate the training process by the transformer model. This is because large models such as transformers require large input data for robust training, as otherwise the weights and hidden features remain underdetermined. In order to create batches of input data, the time series for each region of interest are sliced according to a fixed window size <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> with temporal overlap <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub>. In other words, instead of using the entire time series <inline-formula><mml:math id="m1"><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula> of each voxel <inline-formula><mml:math id="m2"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula> for each subject <italic toggle="yes">S</italic> as the input data (i.e., <italic toggle="yes">S</italic> input data points for <italic toggle="yes">S</italic> subjects), <italic toggle="yes">M</italic> segments of each time series are used as the inputs, resulting in an adequately larger dataset (<italic toggle="yes">S</italic> Ã <italic toggle="yes">M</italic> input data points for <italic toggle="yes">S</italic> subjects) and robust training of the transformer model. This process is depicted in the left side of <xref rid="F1" ref-type="fig">Figure 1</xref>. For this study, we selected the window size <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 25, and temporal overlap <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 5 as the default setup of our analysis on the first dataset, and <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 50, and temporal overlap <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 10 for the second experimental dataset. This preparatory step resulted in 15,000 time series slices for each voxel <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub> for the first dataset and 31,680 segments for each voxel for the second dataset. The details of the datasets used in this study will be discussed in the <xref rid="sec13" ref-type="sec">Results</xref> section along with an analysis of the effect of temporal window size on the classification performance.</p>
        <p>We also set the size of each <xref rid="def6" ref-type="def">input batch</xref> to 50 entries, where each entry is composed of two components: the multivariate time series segments of the temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> for the <italic toggle="yes">N</italic> voxels within the ROI, as well as the FC adjacency matrix based on the coactivations of the same time series segments. The prepared input batches are then supplied to the first ST block to begin the process of training.</p>
      </sec>
      <sec id="sec6">
        <title>Spatial Transformer</title>
        <p>The spatial transformer consists of a spatial positional embedding layer that provides the encoding for the attention mechanism, a dynamic graph attention layer to inject the spatial context of each nodeâs BOLD activation level into the newly generated features, and a GCN to embed the topological properties of the FC network within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. The building blocks of the spatial transformer are depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>, where the output of positional embedding is supplied to the attention and GCN blocks simultaneously. We explain each block of the spatial transformer in the following sections.</p>
        <fig position="float" id="F2">
          <label><bold>Figure 2.</bold>â</label>
          <caption>
            <p>Block-level architecture of the transformer model. Left: The architecture of the spatial transformer component, where <italic toggle="yes">T</italic><sub><italic toggle="yes">w</italic></sub> is a temporal window (time series segment) within which the input data are derived, and <italic toggle="yes">y</italic><sup><italic toggle="yes">s</italic></sup> is the output of this transformer. The output of the positional embedding is supplied to the graph convolution network and the attention in parallel. The output of these two components is then fused through a gate mechanism to generate the features. Right: The architecture of the temporal transformer block. The input to this block is the output of the spatial block combined with the input to the spatial block by a residual connection (also see <xref rid="F1" ref-type="fig">Figure 1</xref>).</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g002" position="float"/>
        </fig>
        <sec id="sec7">
          <title>Positional embedding.</title>
          <p>An embedding of the time series data is needed to introduce the positional information of each node to the attention block. For this purpose, a 1 Ã 1 convolution layer is adopted to encode the positional features into a <italic toggle="yes">d</italic>-dimensional vector for each node at each time step, where <italic toggle="yes">d</italic> is the embedding size. For spatial positional embedding, we adopt the approach proposed by <xref rid="bib52" ref-type="bibr">Wang et al. (2021)</xref>, in which functional connectomic neighborhoods are used as the topological input through the adjacency matrix of the functional connectivity network. For temporal positional encoding, trigonometry-based feature transformation was performed by calculating the sine and cosine values of each time step and using them as the temporal embedding of each time series value. The benefit of this approach over one-hot encoding of temporal features is that it avoids generating a high-dimensional and unbalanced vector of positional encoding features. The 1-D depthwise convolution is then used to convert the positional information into a feature vector of appropriate size for each node at each time step <italic toggle="yes">t</italic> (<xref rid="bib34" ref-type="bibr">Mandal &amp; Mahto, 2019</xref>; <xref rid="bib49" ref-type="bibr">Vosoughi, Vijayaraghavan, &amp; Roy, 2016</xref>). This block outputs a vector for each node at each time step <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>, containing the spatial and temporal information that is in turn used by both the GCN and the dynamic graph attention blocks in parallel, as depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
        </sec>
        <sec id="sec8">
          <title>Graph convolution block.</title>
          <p>Graph convolution network is a variant of convolutional neural networks (CNN); it learns a representation of graphs by leveraging their structure and aggregate node information from its neighborhood in a convolutional fashion. To learn the structure-aware node features based on the connectivity topology, a convolution approximated by Chebyshev polynomials is employed (<xref rid="bib10" ref-type="bibr">Defferrard, Bresson, &amp; Vandergheynst, 2016</xref>). A GCN setup for classification task on resting-state fMRI was suggested by <xref rid="bib52" ref-type="bibr">Wang et al. (2021)</xref> in which the functional connectivity network is used instead of the network of Euclidean distances as the topological input to facilitate an encoding that is appropriate for the organization of the brain. We build upon this approach by using the FC network for the GCN in parallel with the attention mechanism within the spatial component. However, a difference between our proposed setup and the setup proposed by Wang et al. is that they adopt the time series of the nodes as input features, whereas we utilize the embedded features of the nodes (from the previous block) within each time <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> as the input features to GCN, as depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>. Therefore, the input to the GCN includes the embedding of the time series segments from the previous block as the vector of features for each node at each time <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>, as well as the functional connectivity of the same time series segments as the network input. The GCN mechanism first aggregates all the features of the neighbors of every node, including itself, through an aggregate function. The aggregated feature sets are then passed through a nonlinear neural network layer to output a vector of features for each node at every time point. This vector is finally fused together with the results of the dynamic attention layer via a <xref rid="def7" ref-type="def">gate mechanism</xref> to create the output of the spatial block, as depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
        </sec>
        <sec id="sec9">
          <title>Dynamic attention block.</title>
          <p>To capture the contextual time-evolving functional dependencies between the nodes, we adopt a dynamical graph attention mechanism that maps the embedded features of each node <inline-formula><mml:math id="m3"><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover></mml:math></inline-formula> from the positional embedding block to high-dimensional latent subspaces. Attention mechanism consists of three main components: query, key, and value (<xref rid="bib48" ref-type="bibr">Vaswani et al., 2017</xref>). The set of input vectors that we aim to calculate the attention for is called a query, and the set of vectors to calculate attention against is called the key. For each query, the similarity between the query and the keys is calculated, which provides a score for each key-query pair. In this study a dot product attention is adopted, meaning that it calculates the inner product between the query and a key vector to provide the similarity score between them (<xref rid="bib48" ref-type="bibr">Vaswani et al., 2017</xref>). This process can be performed for multiple key, query, and value vectors at once; therefore, packing together sets of queries, keys, and values, we have the <italic toggle="yes">Q</italic><sup><italic toggle="yes">S</italic></sup>, <italic toggle="yes">K</italic><sup><italic toggle="yes">S</italic></sup>, and <italic toggle="yes">V</italic><sup><italic toggle="yes">S</italic></sup>, such that<disp-formula id="E1"><mml:math id="m4"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>Q</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mi>S</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>S</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(1)</label></disp-formula>where <inline-formula><mml:math id="m5"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m6"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="m7"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula> are the projection matrices that are used to generate the subspace representations of the query, key, and value matrices. Each row of <italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, and <italic toggle="yes">V</italic> represents an entity, therefore the dot product attention takes a weighted sum of the entity values in <italic toggle="yes">V</italic> where the weights are given by the interactions of query-key pairs. This process is depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>, where the dynamic spatial dependencies calculated from the query-key dot product is then supplied to a softmax function for scaling, and then multiplied with the value matrix <italic toggle="yes">V</italic><sup><italic toggle="yes">S</italic></sup> to update the node features.<disp-formula id="E2"><mml:math id="m8"><mml:mtext mathvariant="italic">Attn</mml:mtext><mml:mfenced open="(" close=")" separators=",,"><mml:mi>Q</mml:mi><mml:mi>K</mml:mi><mml:mi>V</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">softmax</mml:mtext><mml:mfenced open="(" close=")"><mml:mstyle displaystyle="true"><mml:mfrac><mml:msup><mml:mrow><mml:mi>Q</mml:mi><mml:mi>K</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msup><mml:msqrt><mml:msup><mml:mi>d</mml:mi><mml:mi>S</mml:mi></mml:msup></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:msup><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math><label>(2)</label></disp-formula>where <italic toggle="yes">d</italic> is the feature dimension. As the next step of the spatial component, a three-layer feed-forward neural network with nonlinear activation is applied on each nodeâs weighted sum contextual features to capture the interactions between the features, as in <xref rid="bib48" ref-type="bibr">Vaswani et al. (2017)</xref>.<disp-formula id="E3"><mml:math id="m9"><mml:msup><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">ReLU</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mtext mathvariant="italic">ReLU</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mtext mathvariant="italic">Attn</mml:mtext><mml:mfenced open="(" close=")" separators=",,"><mml:mi>Q</mml:mi><mml:mi>K</mml:mi><mml:mi>V</mml:mi></mml:mfenced><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>S</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>S</mml:mi></mml:msubsup></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>W</mml:mi><mml:mn>3</mml:mn><mml:mi>S</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:math><label>(3)</label></disp-formula>where <inline-formula><mml:math id="m10"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>S</mml:mi></mml:msubsup></mml:math></inline-formula> is the weight matrix for the <italic toggle="yes">i</italic>th layer and ReLU stands for rectified linear unit.</p>
          <p>This process is illustrated in <xref rid="F3" ref-type="fig">Figure 3</xref>, where four example nodes (voxels or regions of interest, depending on spatial precision) constitute the functional connectivity network. The query node in this figure is voxel <italic toggle="yes">V</italic>1, and each node is assigned a feature vector, which is the output of the positional encoding on the time series prior to the attention block. As this figure demonstrates, the similarity between the query node and every other node (keys) is obtained through the dot product of its encoded features, which divided by a scaling factor (see <xref rid="E7" ref-type="disp-formula">Equation 7</xref>) provides the attention weights for the nodes. The attention weights emphasize parts of the FC network while diminishing other parts based on their contextual importance for the prediction task. For voxel <italic toggle="yes">v</italic><sub>1</sub> as the query, the output vector <italic toggle="yes">Y</italic><sub>1</sub> is derived by<disp-formula id="E4"><mml:math id="m11"><mml:msub><mml:mi>W</mml:mi><mml:mn>11</mml:mn></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>12</mml:mn></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mn>2</mml:mn><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>13</mml:mn></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mn>3</mml:mn><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mo>â¦</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mi>N</mml:mi><mml:mi>f</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:math><label>(4)</label></disp-formula>where <inline-formula><mml:math id="m12"><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:msubsup></mml:math></inline-formula> are the input feature vectors for voxels <italic toggle="yes">v</italic><sub><italic toggle="yes">i</italic></sub>, and <italic toggle="yes">W</italic><sub>1<italic toggle="yes">i</italic></sub> correspond to the attention weights based on similarity of features between voxel <italic toggle="yes">v</italic><sub>1</sub> and every other voxel. This process is performed for every node in the network, meaning that each node plays the role of the query separately. Thus, for each node at time step <italic toggle="yes">t</italic>, the input to the attention mechanism is a vector of its features, and the output consists of a vector with contextual information. The weights of query, key, and value layers are then updated through back-propagation during training. Therefore, through the spatial attention process, the context of the nodes (voxel/ROI) with regard to the other nodes within the FC network at time <italic toggle="yes">t</italic> is extracted to be combined with the output of the GCN block to form the spatial representations.</p>
          <fig position="float" id="F3">
            <label><bold>Figure 3.</bold>â</label>
            <caption>
              <p>The attention mechanism within the spatial block. Similarity between the features of each voxel (query) and other voxels (keys) within the FC network is calculated through the dot product process and is reweighted during the training process to create the attention weights for the input sequence.</p>
            </caption>
            <graphic xlink:href="netn-7-1-22-g003" position="float"/>
          </fig>
          <p>The last step of the spatial component is the gate mechanism, which is applied to fuse the spatial features learned from the GCN and the dynamic attention block. The steps of the gate mechanism include aggregating the features from GCN and attention block, calculating the sigmoid of this aggregation, and then using the sigmoid output to create a weighted sum of the output of GCN and attention block such that<disp-formula id="E5"><mml:math id="m13"><mml:msup><mml:mi>Y</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>Î±</mml:mi><mml:msup><mml:mi>U</mml:mi><mml:mi>S</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Î±</mml:mi></mml:mrow></mml:mfenced><mml:msup><mml:mi>U</mml:mi><mml:mi>G</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:math><label>(5)</label></disp-formula></p>
          <p>The output of this operation is a vector of features for each node at each time step <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. Therefore, for <italic toggle="yes">N</italic><sub><italic toggle="yes">f</italic></sub> number of features, <italic toggle="yes">t</italic> time steps within the temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>, <italic toggle="yes">N</italic> nodes, and a batch size <italic toggle="yes">N</italic><sub><italic toggle="yes">b</italic></sub>, the output of the spatial block is a tensor of size <italic toggle="yes">N</italic><sub><italic toggle="yes">b</italic></sub> Ã <italic toggle="yes">t</italic> Ã <italic toggle="yes">N</italic> Ã <italic toggle="yes">N</italic><sub><italic toggle="yes">f</italic></sub>. This output is then supplied to the temporal transformer component of the ST block, as illustrated in <xref rid="F2" ref-type="fig">Figure 2</xref>. In the next part, we explain the building blocks of the temporal transformer.</p>
        </sec>
      </sec>
      <sec id="sec10">
        <title>Temporal Transformer</title>
        <p>Left-to-right architectures of temporal dependencies such as RNN models are limited to consider temporal dependencies based on preceding time steps, and fail to consider contextual dependencies. Therefore, for the temporal transformer we also adopt a attention mechanism to incorporate the temporal information, similar to the spatial transformer. The input to the temporal component is the embedded features, which are obtained by passing the concatenation of the input features <italic toggle="yes">X</italic><sup><italic toggle="yes">s</italic></sup> aggregated with the temporal embedding <italic toggle="yes">X</italic><sup><italic toggle="yes">T</italic></sup> (i.e., the output of the previous spatial block and its input as the residual connection). Similar to the spatial transformer, this input is passed to a 1 Ã 1 convolution positional embedding layer:<disp-formula id="E6"><mml:math id="m14"><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mtext mathvariant="italic">Conv</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:msup><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mfenced><mml:mo>,</mml:mo></mml:math><label>(6)</label></disp-formula>where <italic toggle="yes">X</italic><sup><italic toggle="yes">T</italic></sup> = <italic toggle="yes">X</italic><sup><italic toggle="yes">S</italic></sup> + <italic toggle="yes">Y</italic><sup><italic toggle="yes">S</italic></sup> is calculated from the outputs of the spatial transformer block, and <italic toggle="yes">D</italic><sup><italic toggle="yes">T</italic></sup> is the temporal embedding. Therefore, we obtain an embedding of features as a vector for each node at each time step <italic toggle="yes">t</italic> within the temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. Similar to the spatial transformer, we have<disp-formula id="E7"><mml:math id="m15"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">Ë</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><label>(7)</label></disp-formula>where <inline-formula><mml:math id="m16"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m17"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="m18"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula> are the learned liner mappings. Here we also adopt the scaled dot product function to consider bidirectional temporal dependencies.<disp-formula id="E8"><mml:math id="m19"><mml:mtext mathvariant="italic">Attn</mml:mtext><mml:mfenced open="(" close=")" separators=",,"><mml:mi>Q</mml:mi><mml:mi>K</mml:mi><mml:mi>V</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">softmax</mml:mtext><mml:mfenced open="(" close=")"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msup><mml:mi>Q</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mfrac></mml:mstyle></mml:mfenced><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:math><label>(8)</label></disp-formula>Then, to explore the interactions among <xref rid="def8" ref-type="def">latent features</xref>, a shared three-layer feed-forward neural network is developed whose output is aggregated with the output of positional embedding unit as a residual connection to create the vector of features for each node for time step <italic toggle="yes">t</italic> within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>, as depicted in <xref rid="F2" ref-type="fig">Figure 2</xref>. Unless the temporal transformer belongs to the final ST block, the aggregation of its output <italic toggle="yes">Y</italic><sup><italic toggle="yes">t</italic></sup> with its input <italic toggle="yes">X</italic><sup><italic toggle="yes">t</italic></sup> is supplied to the spatial block of the next ST block. However, if the temporal transformer is a part of the final ST block, its output is supplied to the prediction layer. This procedure is depicted in <xref rid="F4" ref-type="fig">Figure 4</xref>, where the dot product is calculated between the feature vector for each query node at time step <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> with the features of the same node at other time steps. Aside from this difference between the temporal and spatial attention, the rest of the process for capturing the contextual vector for each node is similar. Thus, the output vector for voxel <italic toggle="yes">v</italic> at time point <italic toggle="yes">t</italic> is obtained from the following equation:<disp-formula id="E9"><mml:math id="m20"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>f</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mo>â¦</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>Ï</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math><label>(9)</label></disp-formula>where <inline-formula><mml:math id="m21"><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msubsup></mml:math></inline-formula> are the input feature vectors for voxel <italic toggle="yes">v</italic> at time step <italic toggle="yes">i</italic>, and <italic toggle="yes">W</italic><sub><italic toggle="yes">ti</italic></sub> corresponds to the attention weights based on similarity of features between voxel <italic toggle="yes">v</italic><sub>1</sub> at time <italic toggle="yes">t</italic> and its features at time <italic toggle="yes">i</italic>. Therefore, in the temporal attention block, the attention weights enhance parts of the time series sequence while diminishing other parts based on their contextual importance for the prediction task. In the next section, we discuss the prediction layer as a unit outside of the ST blocks.</p>
        <fig position="float" id="F4">
          <label><bold>Figure 4.</bold>â</label>
          <caption>
            <p>The attention mechanism within the temporal block. Similarity between the features of each voxel at time <italic toggle="yes">t</italic> (query) and its own values on other time steps (keys) is calculated through the dot product process and is reweighted during the training process to create the attention weights for the input sequence.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g004" position="float"/>
        </fig>
      </sec>
      <sec id="sec11">
        <title>Prediction Layer</title>
        <p>The prediction layer consists of two fully connected convolution layers with a ReLU activation function in between, which is similar to the feed-forward network used in <xref rid="bib48" ref-type="bibr">Vaswani et al. (2017)</xref>, followed by a softmax activation function for classification. This architecture for prediction layer has been commonly adopted to introduce nonlinearity that assists the model with learning complex mappings between the inputs and target variables (<xref rid="bib1" ref-type="bibr">Agarap, 2018</xref>; <xref rid="bib24" ref-type="bibr">Ide &amp; Kurita, 2017</xref>).</p>
        <p>The encoder component of the transformer generates a set of embedded features for each node at each time step. Consequently, the input to the prediction layer is a batch of size <italic toggle="yes">N</italic><sub><italic toggle="yes">b</italic></sub> of three-dimensional tensor of <italic toggle="yes">N</italic><sub><italic toggle="yes">f</italic></sub> spatiotemporal features yielded from the final ST block for each node <italic toggle="yes">N</italic> at each time point <italic toggle="yes">t</italic> â <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. The output of this layer is a prediction depending on the downstream task. For classification tasks, the AUC was measured through cross-entropy between the predicted labels and the true labels. In the next section we provide the experimental results based on the discussed transformer architecture.</p>
      </sec>
      <sec id="sec12">
        <title>Training Setup</title>
        <p>In this section we provide the details and parameters of data preparation and the experimental setup. The implementation code for the methodology is available in Python via <ext-link xlink:href="https://github.com/ThisIsNima/Spatio-Temporal-Transformer" ext-link-type="uri">https://github.com/ThisIsNima/Spatio-Temporal-Transformer</ext-link> (<xref rid="bib2" ref-type="bibr">Asadi, 2022</xref>). All the experiments were performed on an Intel Core i7-3370 CPU, 3.40 GHz with 32 GB of RAM, and the implementation code was written in Python programming language. The average training time of the spatiotemporal transformer model for the ROI-level analysis on the Autism Brain Imaging Data Exchange (ABIDE) dataset was 22 min and 16 s, and for the HCP data it was 28 min and 32 s.</p>
        <p>The segmentation process was performed on the preprocessed time series data with the window length <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 25 and temporal overlap <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 5 for the first dataset, and <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 50, and overlap length <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> = 10 for the second dataset. Batch size was set to <italic toggle="yes">N</italic><sub><italic toggle="yes">b</italic></sub> = 50 for both datasets. For training, validation, and testing, the data were selected randomly from this data subset for each ROI, and then the training group was partitioned into batches of 50 items. The FC networks were then generated for the time series of each data entry within each window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. Therefore, each of the 50 entries within each input batch for a region of interest included the time series segments for its <italic toggle="yes">N</italic> voxels as well as their FC network. A positional embedding of the two data components is then derived through a 1 Ã 1 convolution on the spatial and temporal encodings of the time series data to output a vector of features for each node at time point <italic toggle="yes">t</italic> within <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>. Therefore, the output of the positional embedding is a 4D tensor of size <italic toggle="yes">N</italic><sub><italic toggle="yes">b</italic></sub> Ã <italic toggle="yes">N</italic> Ã <italic toggle="yes">t</italic><sub><italic toggle="yes">Ï</italic></sub> Ã <italic toggle="yes">N</italic><sub><italic toggle="yes">f</italic></sub>, where <italic toggle="yes">N</italic><sub><italic toggle="yes">f</italic></sub> is the embedding feature size, which was set to 64 for this experiment. The vector of embedded features is then supplied to the dynamic attention unit, and the pair of time series embedding output and FC adjacency matrix are supplied to the GCN unit of the spatial transformer. The spatial and temporal components are placed sequentially to form a spatiotemporal block. Three spatiotemporal blocks with 2-head dot product attention mechanisms were adopted for this analysis. Also, the initial leaning rate is set to 10<sup>â4</sup> with a decay at a rate of 0.5.</p>
        <p>Two resting-state fMRI datasets were used as the case studies in this work. The first dataset for this study is composed of 600 subjects from the ABIDE database, including 300 subjects diagnosed with ASD and 300 control subjects (<xref rid="bib13" ref-type="bibr">Di Martino et al., 2014</xref>). This dataset was preprocessed by the Configurable Pipeline for the Analysis of Connectomes (C-PAC) pipeline and was slice time and motion corrected (<xref rid="bib36" ref-type="bibr"><italic toggle="yes">MS Windows NT kernel description</italic>, n.d.</xref>). Also, the voxel intensities were normalized through global signal regression. The automated anatomical labeling (AAL) atlas was then adopted for parcellation of regions of interest (<xref rid="bib46" ref-type="bibr">Tzourio-Mazoyer et al., 2002</xref>). The BOLD time series were then segmented using the sliding-window approach, and Pearsonâs correlation between the time series within each temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> was calculated to generate the weight of the links between the nodes. The second dataset was constructed from data provided by the Human Connectome Project (HCP S1200) release comprising 440 subjects (age range: 22â37, mean age: 28.7 years; 220 males), where male and female subjects were matched for age (<xref rid="bib47" ref-type="bibr">Van Essen et al., 2013</xref>). The resting-state BOLD data comprised 1,200 functional volumes per subject, and the AAL atlas was also used for parcellation of regions of interest. The demographic characteristics of the two datasets are provided in Table 3 in the <xref rid="sec25" ref-type="sec">Supporting Information</xref>.</p>
        <p>Two classification tasks were set up to evaluate the performance of the model based on the features generated on the two experimental datasets. The objective of the classification tasks was to assess the quality of the generated features for distinguishing between cohorts of subjects based on fMRI data. In other words, the aim of this analysis was to evaluate how well the generated features characterize the BOLD activation pattern of each region within the context of global spatiotemporal dynamics of the brainâs regions by taking the spatiotemporal context of its BOLD activation dynamics as well as the dFC networks into consideration. After training the transformer model, it is supplied with test data to distinguish between the ASD and control subjects for the first dataset (ABIDE), and predict the sex of the subjects for the second dataset (HCP). For both classification tasks, 70% of the dataset was used for training, 15% for <xref rid="def9" ref-type="def">cross-validation</xref>, and 15% for testing.</p>
        <p>An analysis of the effects of various architectural configurations on modelâs performance is provided in Figure 1 in the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. In this analysis, we investigated the combination of three different values for the number of attention heads, the embedding feature size, and the number of ST blocks against the modelâs average classification AUC on 10 trials for both datasets. This analysis was the basis for our configuration setup. Furthermore, the effect of various temporal window sizes on the modelâs performance is explored in the next section.</p>
        <p>The experiments were performed on two spatial resolution levels including voxel-level analysis, and ROI-level analysis. In voxel-level analysis, a model is trained for each region, and the voxels within the ROI represent the nodes of the graph, whereas in the ROI-level analysis, a model is trained on the entire brain, where the regions of interest play the roles of graph nodes. For the ROI-level analysis, the times series of the voxels within each region are averaged to create one time course per ROI.</p>
        <p>In our voxel-level experiments, we trained the model for each region separately in parallel, and then used an ensemble majority voting criteria for the prediction step. This setup has the benefits of more localized representation learning by considering the biological properties of the regions independently, as well as significantly enhancing the computational efficiency. Moreover, quite similar to the general principle of bagging ensemble training approach, these criteria can reduce the variance of the model. Therefore, during test, the model trained on each region predicts the class label of the test data from the same region, and a simple majority voting among the regions is used to determine the final classification of the subject from the test data.</p>
        <p>Two comparative experiments are designed to compare the predictive power of learned representations for each of the two experimental case studies. For the first set of experiments, we adopted a standalone GCN model that takes the time series positional embedding as well as the FC network as the input, a standalone attention block (SA) as the second baseline, and a feed-forward convolution neural network (FF-CNN) as the third baseline, where the latter two baselines use the spatiotemporal embedding of the time series data within each temporal window as the input. The reason for adopting the first two baselines was to compare how well each of the two blocks of our model performs as popular standalone architectures. To compare the performances, the area under the classification ROC curve (AUC) were compared on unseen test data. In the next section, we first provide example visualizations and preliminary analysis of the results, and then offer the results of the classification tasks. For the second comparative analysis, three deep learningâbased models that are used for fMRI classification were used as baselines. These three models include spatiotemporal graph convolutional networks (ST-GCN), deep-fMRI, and the multiscale RNN (MsRNN; <xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>; <xref rid="bib30" ref-type="bibr">Kong et al., 2021</xref>; <xref rid="bib39" ref-type="bibr">Riaz et al., 2020</xref>; <xref rid="bib58" ref-type="bibr">Yan et al., 2019</xref>; <xref rid="bib60" ref-type="bibr">B. Yu, Yin, &amp; Zhu, 2017</xref>).</p>
      </sec>
    </sec>
    <sec id="sec13">
      <title>RESULTS</title>
      <p>In this section, we discuss the experimental results based on the proposed architecture on two sets of resting-state fMRI datasets discussed in the <xref rid="sec2" ref-type="sec">Methodology</xref> section. We first provide a preliminary analysis of the representations, including visualizations of the attention maps of number of brain regions, and then provide the classification results. For region-specific voxel-level analysis, we provide the visualizations for four regions, namely left and right amygdalas and hippocampus in this section, and the results for other regions in Table 4 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. The importance of the four mentioned regions in understanding memory and analysis of ASD and other neurological conditions according to related literature is the factor in choosing these regions for the visualizations (<xref rid="bib5" ref-type="bibr">Burgess, Maguire, &amp; OâKeefe, 2002</xref>; <xref rid="bib17" ref-type="bibr">Guo et al., 2016</xref>; <xref rid="bib45" ref-type="bibr">Treves &amp; Rolls, 1994</xref>; <xref rid="bib57" ref-type="bibr">Q. Xu, Zuo, Liao, Long, &amp; Wang, 2020</xref>). Furthermore, we provide the visualization for the the ROI-level full-brain analysis in this section.</p>
      <sec id="sec14">
        <title>Analysis of the Representations</title>
        <p>A visualization of the outputs of the ST blocks for the left amygdala of one healthy subject from the ABIDE dataset is provided in <xref rid="F5" ref-type="fig">Figure 5</xref>. This visualization corresponds to temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic>=1</sub>, and the nodes of the network represent the voxels within the left amygdala. As that figure demonstrates, the output of the two attention heads for each ST block is sequentially fed into the next ST block, and the output of the last block is supplied to the prediction block. The final convolution layer of the prediction block generates the predictions <italic toggle="yes">y</italic><sub><italic toggle="yes">pred</italic></sub>, which is a matrix of size <italic toggle="yes">N</italic> Ã <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> where <italic toggle="yes">N</italic> is the number of nodes (voxels/regions) and <italic toggle="yes">T</italic><sub><italic toggle="yes">Ï</italic></sub> is the temporal window size. This procedure is applied to every entry within each batch for the model to be trained for each region. (In this case, the model is trained for the left amygdala.) Note that the transformer model can be trained on different spatial resolution levels. In our voxel-level experiments, we trained the model for each region separately in parallel, and then used an ensemble voting criteria for the prediction step. This setup has the benefits of more localized representation learning by considering the biological properties of the regions independently, as well as enhancing the training efficiency.</p>
        <fig position="float" id="F5">
          <label><bold>Figure 5.</bold>â</label>
          <caption>
            <p>A visualization of the attention maps based on each transformer head and prediction block for the left amygdala (region 41 per AAL atlas) of one subject for the first temporal window, where the window size is 25 time steps, and the embedding feature size is 64.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g005" position="float"/>
        </fig>
        <p>Further visualizations are provided in <xref rid="F6" ref-type="fig">Figure 6</xref>, which shows the attention results of the left amygdala for four control subjects from the ABIDE dataset within the first batch of data for temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic>=2</sub>. Such representations can assist interpretable analysis of the underlying contextual information in the data.</p>
        <fig position="float" id="F6">
          <label><bold>Figure 6.</bold>â</label>
          <caption>
            <p>The attention output of the final spatiotemporal (ST) block for the left amygdala of four subjects at temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> = 2, with 64 voxels and 64 embedding features.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g006" position="float"/>
        </fig>
        <p>Furthermore, the effect of the length of temporal window and the size of the overlap between the windows on classification AUC is provided in <xref rid="F7" ref-type="fig">Figure 7</xref> for both datasets, where training and testing were performed 10 times on each window-overlap size, and their average AUCs were measured. We can observe that the highest AUCs were achieved on temporal window length and overlap of around 20 and 5, respectively, for the ABIDE dataset, and about 50 and 10 for the HCP dataset. Therefore those window-overlap sizes were adopted for this study. In order to examine and compare the performance of the models with temporal window size, we performed this classification with various lengths of the windows. This analysis is provided in Figure 5 in the <xref rid="sec25" ref-type="sec">Supporting Information</xref>, which demonstrates that despite the decline in the AUC, the ST model outperforms the baselines. The decline in AUCs for small window size can be explained by statistically weak and inconsistent functional connectivity information as the length of the time series segments is decreased. On the other hand, the weaker prediction power for large window sizes can be explained by the decrease in the number of time series segments generated as input data, which results in under-training of the model due to small input data size. To further analyze the consistency of attention weights with variations of the temporal window size, we can measure the similarity between the attention matrices. The results of this analysis is provided in Figure 4 in the <xref rid="sec25" ref-type="sec">Supporting Information</xref>, where the values of the matrix cells correspond to the similarity between the attention maps measured by mean percentage error (MPE) of the voxel-wise difference (between the values of corresponding matrix cells). Note that the dimensions of attention maps depend on the number of voxels within the regions in voxel-level analysis, therefore they differ from one region to another. We can observe that the attention maps show a strong similarity along the diagonals, meaning that experiments with close temporal window sizes provide similar attention maps, with a slow decline in similarity with the increase in the gap between temporal window sizes across experiments.</p>
        <fig position="float" id="F7">
          <label><bold>Figure 7.</bold>â</label>
          <caption>
            <p>Effect of temporal parameters on AUC. (A) The effect of the length of temporal windows as well as their temporal overlap on average classification AUC for the ABIDE dataset. The values of the cells corresponds to the average classification AUC. Note that the lower triangle does not have any values, as the length of overlap does not exceed the length of the window. (B) The results of the same analysis for the HCP dataset.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g007" position="float"/>
        </fig>
        <p>For the ROI-level analysis, a visualization of the output of each attention head of the last two ST blocks is illustrated for <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic>=1</sub> in <xref rid="F8" ref-type="fig">Figure 8</xref>, and the attention outputs for four subjects from the ABIDE dataset are provided in <xref rid="F9" ref-type="fig">Figure 9</xref>. As discussed previously, in ROI-level analysis the nodes of the network correspond to the regions of interest whose fMRI signal is averaged. Also, a visualization of averaged attention weights for 300 healthy subjects based on the ABIDE dataset for the left and right amygdalas and hippocampus is provided in <xref rid="F10" ref-type="fig">Figure 10</xref>. As can be seen in that figure, for the mentioned four regions, we can observe higher overall attention weights for the temporal lobe, and a consistent level of overall attention on the frontal lobe. A similar visualization is provided in the <xref rid="sec25" ref-type="sec">Supporting Information</xref> for average attention scores for the second dataset, which demonstrates relatively similar attention patterns. Moreover, visualizations for the attention weight based on four cerebellum regions as the query node are provided in Figure 4 in the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. For ease of presentation, we provide the higher attention weights that exceed the top half score cutoff threshold. In that figure we can observe contextual interaction between the cerebellar regions and other cerebellar regions, the amygdalas, and motor and visual cortices. These results can demonstrate the contextual functional interactions between the regions through the framework of attention mechanism. The spatiotemporal attention weights inject this contextual information into the learned representation (features) to assist the prediction tasks.</p>
        <fig position="float" id="F8">
          <label><bold>Figure 8.</bold>â</label>
          <caption>
            <p>A visualization of the attention map output of each head of the final spatiotemporal (ST) block and prediction block for the full brain setup (116 regions per AAL atlas) of one subject for the first temporal window, where the window size is 25 time steps, and the embedding feature size is 64.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g008" position="float"/>
        </fig>
        <fig position="float" id="F9">
          <label><bold>Figure 9.</bold>â</label>
          <caption>
            <p>The attention map output of the final spatiotemporal (ST) block for the entire brain of four subjects at temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub> = 2 with 116 ROIs and 64 embedding features.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g009" position="float"/>
        </fig>
        <fig position="float" id="F10">
          <label><bold>Figure 10.</bold>â</label>
          <caption>
            <p>The attention weights of various areas of the brain with regards to the left and right amygdalas and hippocampus, averaged across all healthy subjects in the ABIDE dataset.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g010" position="float"/>
        </fig>
      </sec>
      <sec id="sec15">
        <title>Classification Results</title>
        <p>The classification results on voxel-level resolution for both datasets is provided in <xref rid="F11" ref-type="fig">Figure 11</xref>, along with the classification confusion matrix in <xref rid="T1" ref-type="table">Table 1</xref>. As mentioned in the <xref rid="sec2" ref-type="sec">Methodology</xref> section, for this analysis a model is trained for each region, and during test a majority voting is performed to provide the final classification. As demonstrated in these results, the spatiotemporal contextual features derived by the ST transformer offer an enhanced pattern extraction compared with the baseline models. In order to provide a more clear analysis of the difference between the AUC values, DeLongâs test for assessing the difference between the AUC values was performed; the null hypothesis is that the true performance of two models are equal. The results of this test are provided in Table 1 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. As can be seen in that table, the null hypothesis is rejected between the ST method and the baseline methods. This can be explained by the broader information that the features generated by the ST model retain through exploiting the spatiotemporal contexts of BOLD activations as well as the functional connectivity network of the regions during the experiment. In order to evaluate the consistency of classification votes of each region, the percentage of subjects classified as healthy for the ABIDE dataset and the percentage of subjects classified as female for the HCP dataset for every region are provided in Figures 6 and 7 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. Note that these percentages include false and true positive/negative classifications.</p>
        <fig position="float" id="F11">
          <label><bold>Figure 11.</bold>â</label>
          <caption>
            <p>Voxel-level classification results. (A) The voxel-level classification AUC of the ST transformer, graph convolution network (GCN), transformer with only self attention (SA) block, and feed-forward convolution neural network (FF-CNN) for the ABIDE dataset. (B) The classification performance of the same models on the HCP dataset.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g011" position="float"/>
        </fig>
        <table-wrap position="float" id="T1">
          <label><bold>Table 1.</bold>â</label>
          <caption>
            <p>The confusion matrix for the voxel-level classification based on the spatiotemporal transfromer model based on the ABIDE (left) and HCP (right) datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr valign="bottom">
                <th rowspan="2" colspan="1">Â </th>
                <th rowspan="2" colspan="1">Â </th>
                <th align="center" colspan="2" rowspan="1">Predicted ASD</th>
                <th align="center" rowspan="2" colspan="1">Total</th>
                <th rowspan="2" colspan="1">Â </th>
                <th rowspan="2" colspan="1">Â </th>
                <th align="center" colspan="2" rowspan="1">Predicted sex</th>
                <th align="center" rowspan="2" colspan="1">Total</th>
              </tr>
              <tr valign="bottom">
                <th align="center" rowspan="1" colspan="1">Positive</th>
                <th align="center" rowspan="1" colspan="1">Negative</th>
                <th align="center" rowspan="1" colspan="1">Female</th>
                <th align="center" rowspan="1" colspan="1">Male</th>
              </tr>
            </thead>
            <tbody>
              <tr valign="top">
                <td align="left" rowspan="3" colspan="1">True label</td>
                <td align="left" rowspan="1" colspan="1">Positive</td>
                <td align="center" rowspan="1" colspan="1">33</td>
                <td align="center" rowspan="1" colspan="1">12</td>
                <td align="center" rowspan="1" colspan="1">45</td>
                <td align="left" rowspan="3" colspan="1">True label</td>
                <td align="left" rowspan="1" colspan="1">Female</td>
                <td align="center" rowspan="1" colspan="1">24</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">33</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Negative</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">34</td>
                <td align="center" rowspan="1" colspan="1">45</td>
                <td align="left" rowspan="1" colspan="1">Male</td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">26</td>
                <td align="center" rowspan="1" colspan="1">33</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Total</td>
                <td align="center" rowspan="1" colspan="1">44</td>
                <td align="center" rowspan="1" colspan="1">46</td>
                <td align="center" rowspan="1" colspan="1">90</td>
                <td align="left" rowspan="1" colspan="1">Total</td>
                <td align="center" rowspan="1" colspan="1">31</td>
                <td align="center" rowspan="1" colspan="1">35</td>
                <td align="center" rowspan="1" colspan="1">66</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Moreover, the classification power of separate regions of interest can be examined by training the model on an ROI and calculating the prediction AUC on data of the same region from test subjects. Since the dataset is balanced, we also provide the accuracy for all regions in Table 4 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>. <xref rid="F12" ref-type="fig">Figure 12</xref> demonstrates the classification performance of the same models on four regions of interest, including the left and right amygdalas and hippocampus from the ABIDE dataset, where the voxels within each ROI constitute the nodes of the FC network. The results of this analysis for the second dataset are provided in Figure 2 in <xref rid="sec25" ref-type="sec">Supporting Information</xref>. We can note a decrease in classification performance for training the model on only one region compared with all regions, which was carried out in the previous analysis.</p>
        <fig position="float" id="F12">
          <label><bold>Figure 12.</bold>â</label>
          <caption>
            <p>The classification AUC of the ST transformer, graph convolution network (GCN), transformer with only self attention (SA) block, and feed-forward convolution neural network (FF-CNN) for four regions of interest of 600 subjects from the ABIDE dataset.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g012" position="float"/>
        </fig>
        <p>As the last step of our analysis, we set up two ROI-level classification tasks. In order to prepare the input batches for this analysis, we derived the average time series of each region of interest and performed the same segmentation approach as the voxel-level analysis. Therefore, regions of interest were set as the nodes of the FC networks instead of the voxels within the regions, and one training task was performed instead of parallel training on separate regions. Through this process, a dataset size of 15,000 segments was generated for the ABIDE dataset, and 31,680 segments for the HCP sample. The dFC networks were also generated for each temporal window, where the nodes represented regions of interest, and the weights of the links between them were calculated based on the correlation between the average ROI time series within each temporal window <italic toggle="yes">t</italic><sub><italic toggle="yes">w</italic></sub>.</p>
        <p>The results of classification tasks based on both datasets are provided in <xref rid="F13" ref-type="fig">Figure 13</xref> along with the confusion matrix in <xref rid="T2" ref-type="table">Table 2</xref>, where the same baseline methods as the voxel-level analysis were adopted. As <xref rid="F13" ref-type="fig">Figure 13</xref> demonstrates, the ST approach provides a more informative representation of the fMRI data compared with the baseline methods. DeLongâs test results are also provided for the ROI classification setup in Table 2 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>, which shows that the null hypothesis is rejected between the ST method and the baseline approaches. However, a drop in the overall classification performances is noticeable compared with the voxel-level analysis in <xref rid="F13" ref-type="fig">Figure 13</xref>. The difference between the results of the voxel-level and ROI-level setups can be explained by the loss of information due to the lower spatial resolution of the input data, which also affects the topology and weights of the dynamic connectivity networks.</p>
        <fig position="float" id="F13">
          <label><bold>Figure 13.</bold>â</label>
          <caption>
            <p>(A) The autism spectrum disorder (ASD) classification AUC of the ST transformer, graph convolution network (GCN), transformer with only the self attention (SA) block, and feed-forward convolution neural network (FF-CNN) on ROI-level setup for 600 subjects from the ABIDE dataset. (B) The classification AUC for sex classification on ROI-level setup for the HCP dataset.</p>
          </caption>
          <graphic xlink:href="netn-7-1-22-g013" position="float"/>
        </fig>
        <table-wrap position="float" id="T2">
          <label><bold>Table 2.</bold>â</label>
          <caption>
            <p>The confusion matrix for the ROI-level classification based on the spatiotemporal transfromer model based on the ABIDE (left) and HCP (right) datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr valign="bottom">
                <th rowspan="2" colspan="1">Â </th>
                <th rowspan="2" colspan="1">Â </th>
                <th align="center" colspan="2" rowspan="1">Predicted ASD</th>
                <th align="center" rowspan="2" colspan="1">Total</th>
                <th rowspan="2" colspan="1">Â </th>
                <th rowspan="2" colspan="1">Â </th>
                <th align="center" colspan="2" rowspan="1">Predicted sex</th>
                <th align="center" rowspan="2" colspan="1">Total</th>
              </tr>
              <tr valign="bottom">
                <th align="center" rowspan="1" colspan="1">Positive</th>
                <th align="center" rowspan="1" colspan="1">Negative</th>
                <th align="center" rowspan="1" colspan="1">Female</th>
                <th align="center" rowspan="1" colspan="1">Male</th>
              </tr>
            </thead>
            <tbody>
              <tr valign="top">
                <td align="left" rowspan="3" colspan="1">True label</td>
                <td align="left" rowspan="1" colspan="1">Positive</td>
                <td align="center" rowspan="1" colspan="1">33</td>
                <td align="center" rowspan="1" colspan="1">12</td>
                <td align="center" rowspan="1" colspan="1">45</td>
                <td align="left" rowspan="3" colspan="1">True label</td>
                <td align="left" rowspan="1" colspan="1">Female</td>
                <td align="center" rowspan="1" colspan="1">22</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">33</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Negative</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">31</td>
                <td align="center" rowspan="1" colspan="1">45</td>
                <td align="left" rowspan="1" colspan="1">Male</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">25</td>
                <td align="center" rowspan="1" colspan="1">33</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Total</td>
                <td align="center" rowspan="1" colspan="1">47</td>
                <td align="center" rowspan="1" colspan="1">43</td>
                <td align="center" rowspan="1" colspan="1">90</td>
                <td align="left" rowspan="1" colspan="1">Total</td>
                <td align="center" rowspan="1" colspan="1">30</td>
                <td align="center" rowspan="1" colspan="1">36</td>
                <td align="center" rowspan="1" colspan="1">66</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec16">
        <title>Ablation Analysis</title>
        <p><xref rid="T3" ref-type="table">Table 3</xref> shows an ablation study to assess the significance of different architectural blocks on the classification performance of the ST model. For this purpose, we designed two experiments. In the first experiment we excluded three subcomponents of the model, including the positional encoding, the attention block, and the GCN block one at a time. In the second experiment, the entire spatial and temporal blocks were removed separately to assess the modelâs performance in their absence. A first observation of the results in <xref rid="T3" ref-type="table">Table 3</xref> indicates a level of degradation in the modelâs performance with removal of each of its components. This deterioration is more prominent in the second experiment, where one of the spatial or temporal blocks is entirely removed. Also, as we can observe from this analysis, removal of the attention block affected the modelâs performance relatively more severely compared with removal of the GCN block. A conclusion one can derive from these two observations is the emphasis on the significance of the process of enhancing the relevant nodes (removal of attention mechanism in the spatial transformer) and time points (removal of temporal transformer that contains the temporal attention) for the classification task while diminishing other regions and time points through the self-attention mechanism. However, including the GCN block in the model provides a superior performance compared with the model with ablated components.</p>
        <table-wrap position="float" id="T3">
          <label><bold>Table 3.</bold>â</label>
          <caption>
            <p>Ablation analysis. Left: Average ROI-level classification AUC for ablation analysis of the ST transformer over 10 trials. Right: Average ROI-level classification AUC for four deep learningâbased models over 10 trials</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr valign="bottom">
                <th align="left" rowspan="1" colspan="1">Model</th>
                <th align="center" rowspan="1" colspan="1">ABIDE</th>
                <th align="center" rowspan="1" colspan="1">HCP</th>
                <th align="center" rowspan="1" colspan="1">Approach</th>
                <th align="center" rowspan="1" colspan="1">ABIDE</th>
                <th align="center" rowspan="1" colspan="1">HCP</th>
              </tr>
            </thead>
            <tbody>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Without attention</td>
                <td align="center" rowspan="1" colspan="1">0.626</td>
                <td align="center" rowspan="1" colspan="1">0.618</td>
                <td align="left" rowspan="1" colspan="1">ST-GCN</td>
                <td align="center" rowspan="1" colspan="1">0.677</td>
                <td align="center" rowspan="1" colspan="1">0.651</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Without GCN</td>
                <td align="center" rowspan="1" colspan="1">0.650</td>
                <td align="center" rowspan="1" colspan="1">0.663</td>
                <td align="left" rowspan="1" colspan="1">Deep-fMRI</td>
                <td align="center" rowspan="1" colspan="1">0.649</td>
                <td align="center" rowspan="1" colspan="1">0.640</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Without spatial</td>
                <td align="center" rowspan="1" colspan="1">0.581</td>
                <td align="center" rowspan="1" colspan="1">0.592</td>
                <td align="left" rowspan="1" colspan="1">MsRNN</td>
                <td align="center" rowspan="1" colspan="1">0.668</td>
                <td align="center" rowspan="1" colspan="1">0.654</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Without temporal</td>
                <td align="center" rowspan="1" colspan="1">0.619</td>
                <td align="center" rowspan="1" colspan="1">0.634</td>
                <td align="left" rowspan="1" colspan="1">ST</td>
                <td align="center" rowspan="1" colspan="1">0.711</td>
                <td align="center" rowspan="1" colspan="1">0.704</td>
              </tr>
              <tr valign="top">
                <td align="left" rowspan="1" colspan="1">Full model</td>
                <td align="center" rowspan="1" colspan="1">0.711</td>
                <td align="center" rowspan="1" colspan="1">0.704</td>
                <td rowspan="1" colspan="1">Â </td>
                <td rowspan="1" colspan="1">Â </td>
                <td rowspan="1" colspan="1">Â </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec17">
        <title>Comparison With Deep LearningâBased Models</title>
        <p>In order to gain further insight about the performance and characteristics of the spatiotemporal transformer model, we compare it with a number of state-of-the art deep learning approaches that are used in fMRI data modeling. Specifically, convolution, graph convolution, and RNN-based approaches have gained significant attention during recent years in the computational neuroscience community because of their robust performance and flexibility in analysis of images, time series data, and graph structured data (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>; <xref rid="bib21" ref-type="bibr">Hjelm, Plis, &amp; Calhoun, 2016</xref>; <xref rid="bib38" ref-type="bibr">Qu, Hu, Xiao, &amp; Wang, 2020</xref>; <xref rid="bib51" ref-type="bibr">Wang, Li, Chen, &amp; Hu, 2019</xref>; <xref rid="bib63" ref-type="bibr">Zhao et al., 2018</xref>). The general schema of many of such approaches includes a convolution network for obtaining correlations between brain regions and another deep network for the prediction task (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>; <xref rid="bib22" ref-type="bibr">Hosseini, Tran, Pompili, Elisevich, &amp; Soltanian-Zadeh, 2020</xref>; <xref rid="bib23" ref-type="bibr">Huang et al., 2018</xref>; <xref rid="bib41" ref-type="bibr">Sarraf &amp; Tofighi, 2016a</xref>). For this analysis, three baselines are selected, including the spatiotemporal graph convolutional networks (ST-GCN), deep-fMRI, and the multiscale RNN (MsRNN) (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>; <xref rid="bib30" ref-type="bibr">Kong et al., 2021</xref>; <xref rid="bib39" ref-type="bibr">Riaz et al., 2020</xref>; <xref rid="bib58" ref-type="bibr">Yan et al., 2019</xref>; <xref rid="bib60" ref-type="bibr">B. Yu et al., 2017</xref>).</p>
        <p>ST-GCN is a model for learning from graph-structured time series data (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>). In this baseline, the fMRI data are parcellated and normalized and the average ROI signals are supplied into the model as one-channel spatiotemporal features. These data are processed by three layers of spatiotemporal graph convolution that learn the importance of spatial graph edges for the prediction task and supply this information to the prediction layer for classification (<xref rid="bib16" ref-type="bibr">Gadgil et al., 2020</xref>). Deep-fMRI is an end-to-end deep learning framework that was developed for classification of fMRI data. The inputs to this model are parcellated BOLD signals (<xref rid="bib39" ref-type="bibr">Riaz et al., 2020</xref>). A convolution network is then used to extract features as a vector for each brain region. Next, a multilayer perceptron (MLP) regression layer operates on each pair of regions to predict a correlation matrix. Finally, the generated matrix is used by an MLP classification layer to produce a prediction for the subject (<xref rid="bib39" ref-type="bibr">Riaz et al., 2020</xref>). MsRNN is another deep learningâbased approach, which mainly consists of two components: a CNN block that is used as an encoder for obtaining correlations between the brain regions, and an RNN block that is utilized for sequence classification. In RNNs the output of a layer is used as input for the layer itself, thus forming a feedback loop. This property allows the RNN to consider a history of the data sequence that can be used for prediction of the next sequence elements.</p>
        <p>A comparison of the ST transformer approach and the three mentioned baselines for the ROI-level classification tasks is provided in <xref rid="T3" ref-type="table">Table 3</xref>. The enhanced performance of the the ST transformer compared with the baseline approaches, as can be observed in <xref rid="T3" ref-type="table">Table 3</xref>, can be explained by certain advantages of the attention-based spatiotemporal features compared with CNN-based features. An advantage of attention mechanism compared with convolution-based approaches is that in contrast to the CNN where the receptive field is a neighborhood window of the filter, the receptive field for spatial attention is the entire graph, and for temporal attention is the entire time series. This property provides longer range contextual information for each node (and time point) by considering the global information within the data. Another major difference between the attention mechanism and convolution is that once learned, the temporal or spatial CNN kernels are static. In contrast, instead of calculating the dot product of the input region with a set of fixed kernels, the attention query and key matrices are used to dynamically calculate a new set of kernels for each position in the data sequence. The above-mentioned properties can provide new insight about dynamic codependencies not only between regions of the brain but also between the activation patterns of the same region over time. Moreover, because of their capability in determining the most relevant parts of the input sequence for a certain output, transformer architectures can offer a new point of view regarding the importance of certain interactions between regions of the brain and their temporal behavior in performing various tasks.</p>
        <p>In principle, the spatiotemporal transformer builds upon the core concepts of convolution and sequence modeling by combining a graph convolution network (in the spatial block) and the attention mechanism as described in the <xref rid="sec2" ref-type="sec">Methodology</xref> section. The flexibility and modularity of this architecture also allows for explorations in design of other architectures based on concepts of deep learning to enhance the modeling of neurological conditions or different tasks.</p>
      </sec>
    </sec>
    <sec id="sec18">
      <title>DISCUSSION</title>
      <p>In this paper, we proposed a framework to extract an spatiotemporal representation of the fMRI data by embedding the context of dynamic variations in multivariate BOLD time series and the characteristics of the dFC networks. This framework adopts attention mechanism for learning the contextual dynamic features and graph convolution network to inject the functional connectivity networkâbased information in the representation learning task. The spatial and temporal units are then used as the building blocks of a sequential spatiotemporal transformer model with residual connections that supply the encoded features to the prediction unit. In order to prepare the input data, a sliding-window segmentation process is applied to generate batches of time series segments as well as functional connectivity networks within each window. Therefore, for each region of interest (or voxel) a set of features are extracted at each time point after the training process, and these features are then used as the inputs to the prediction layer.</p>
      <p>By training the model on each region of interest separately on a voxel level, we examined the prediction power of the regions individually. For the ABIDE dataset, we can notice the importance of the amygdalas, insula, hippocampus, inferior frontal gyrus, and cerebellar regions for predicting ASD. Moreover, for the sex classification task for the HCP dataset, the left cingulum posterior (denoted as Cingulum_Post_L in Table 4 of the <xref rid="sec25" ref-type="sec">Supporting Information</xref>), right anterior cingulate cortex (Cingulum_Ant_R), left insula, middle temporal gyrus, cerebellum, and hippocampus exhibit a stronger feature importance. These findings are in line with several studies on ASD as well as sex prediction (<xref rid="bib6" ref-type="bibr">Chaddad, Desrosiers, Hassan, &amp; Tanougast, 2017</xref>; <xref rid="bib12" ref-type="bibr">Dhamala, Jamison, Sabuncu, &amp; Kuceyeski, 2020</xref>; <xref rid="bib20" ref-type="bibr">Heinsfeld, Franco, Craddock, Buchweitz, &amp; Meneguzzi, 2018</xref>; <xref rid="bib53" ref-type="bibr">Weis et al., 2020</xref>; <xref rid="bib57" ref-type="bibr">Q. Xu et al., 2020</xref>). Moreover, the classification results exhibit a superior performance from the classifier based on the learned features of the proposed framework compared with the baseline approaches. Several other studies used machine learning methods for predicting ASD and sex based on similar or different datasets. The input features used in many of such studies consist of the characteristics of functional connectivity networks or statistical attributes of BOLD time series. Learning contextual representations by jointly leveraging information within the FC network and time series data can offer a set of informative features that enhance our understanding of interactions within (voxel level) and between (ROI level) the regions and modelâs prediction power. The proposed approach benefits from several analytical advantages that we discuss in this section, followed by a discussion regarding its limitations, and suggestions for methodological improvements and future work.</p>
      <sec id="sec19">
        <title>Joint Learning Framework Provides Superior Pattern Separation</title>
        <p>Combining the embedding of the information regarding time series dynamics and dFC provides a more powerful set of features for pattern separation tasks compared with adopting only one of the two input structures. Therefore, the two major sources of information in analysis of fMRI data provide a more precise characterization of the higher order dynamics and contexts of the data when embedded jointly.</p>
      </sec>
      <sec id="sec20">
        <title>Dynamics of the Functional Connectivity Are Included in the Learned Representation</title>
        <p>As explained in the <xref rid="sec2" ref-type="sec">Methodology</xref> section, the input batch preparation step includes generating the functional connectivity graphs of each entry of each batch to be utilized by the GCN unit of the spatial component. The FC graphs are created for the time series within each temporal window, similar to the commonly performed dFC network creation based on sliding-window segmentation. Therefore, the variations in the functional connectivity weights of the entire dataset are included in the training and feature encoding process (for <italic toggle="yes">N</italic> subjects and <italic toggle="yes">M</italic> time series segments, <italic toggle="yes">N</italic> Ã <italic toggle="yes">M</italic> connectivity networks are generated). Consequently, the proposed setup takes advantage of the dynamics in the FC network weights as an important source of information regarding functional dependencies during the course of the fMRI experiment.</p>
      </sec>
      <sec id="sec21">
        <title>Spatial Precision Analysis</title>
        <p>The proposed framework displayed enhanced performance in voxel-level experiment compared with the ROI-level setup. While the ROI-level setup provides a significantly more efficient training, it is limited due to loss of information regarding spatial and functional connectivity context. Therefore, for a transformer encoding block, in which the breadth of inferred information is a determining factor in its performance quality, it is favorable to increase the spatial precision of the analysis. Moreover, large models such as transformer architectures commonly show an improved performance with datasets with a high level of granularity, even in the presence of noise confounds, which is an advantageous factor with voxel-level fMRI data analysis.</p>
      </sec>
      <sec id="sec22">
        <title>Architecture Flexibility and Transfer Learning</title>
        <p>The experimental setup for the classification task included using the encoded features as the input to the convolution-based classifier. The set of features created after training the transformer model can be utilized by various classifier models for comparison and exploratory analysis. This is viable because of the flexibility of the transformer framework in being coupled with other models as decoder and prediction or other analytical blocks through the transfer learning paradigm.</p>
      </sec>
      <sec id="sec23">
        <title>Limitations</title>
        <p>Despite the advantageous aspects of the transformer framework, it bears a number of limitations, which we discuss in this section.</p>
        <p>The data preparation process involves performing a segmentation to create the batches of data suitable for large models, such as transformers. Therefore, instead of using the entire time series for each region, a fraction of it is provided for each entry of the batch, which can result in loss of information regarding longer term variations and trends. However, as fMRI data become available to the scale of tens of thousands of subjects, this problem can be amended and the entire time series of each region within the region of analysis (an ROI or the entire brain) can be used for each data entry to train complex models.</p>
        <p>Positional embedding is an essential step for attention-based models. Extraction of complex temporal dependencies can benefit from prior knowledge during preprocessing to play the role of inductive bias. In this work, we injected the spatial positional embedding using the functional connectivity matrices, and the temporal positional embedding by calculating the trigonometry-based values of the time steps. Exploring other positional embedding approaches can enhance the training of attention weights, and in turn the prediction performance of the model.</p>
        <p>Large models such as transformers with attention mechanisms are restricted by large input dataset and memory. Moreover, despite the advantage of transformers over sequential models such as RNN and LSTM due to their ability in parallel training, sequential architecture of the ST blocks coupled with the GCN units within the spatial components decrease the efficiency in the inference step.</p>
        <p>As future work, we would like to explore extraction and comparison of the representations with various brain atlases, as well as analysis of the attention-based context maps across functional networks and different datasets.</p>
      </sec>
    </sec>
    <sec id="sec24">
      <title>ACKNOWLEDGMENTS</title>
      <p>This work was supported in part by National Institutes of Health. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p>
    </sec>
    <sec id="sec25">
      <title>SUPPORTING INFORMATION</title>
      <p>Supporting information for this article is available at <ext-link xlink:href="https://doi.org/10.1162/netn_a_00281" ext-link-type="uri">https://doi.org/10.1162/netn_a_00281</ext-link>.</p>
    </sec>
    <sec id="sec26">
      <title>AUTHOR CONTRIBUTIONS</title>
      <p>Nima Asadi: Conceptualization; Data curation; Formal analysis; Investigation; Methodology; Software; Visualization; Writing â original draft; Writing â review &amp; editing. Ingrid R. Olson: Investigation; Supervision; Validation. Zoran Obradovic: Formal analysis; Supervision; Validation.</p>
    </sec>
    <sec id="sec27">
      <title>FUNDING INFORMATION</title>
      <p>Ingrid R. Olson, National Institutes of Health, Award ID: 2R56MH091113-11. Ingrid R. Olson, National Institutes of Health, Award ID: R21HD098509. Ingrid R. Olson, National Institutes of Health, Award ID: R01HD099165.</p>
    </sec>
    <sec sec-type="supplementary-material">
      <title>Supplementary Material</title>
      <supplementary-material id="SMS1" position="float" content-type="local-data">
        <media xlink:href="netn-7-1-22-s001.pdf">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <glossary>
      <title>TECHNICAL TERMS</title>
      <def-list>
        <def-item id="def1">
          <term>Data feature:</term>
          <def>
            <p>Features are measured properties or characteristics of a phenomenon. The objective of predictive learning such as classification and forecasting is to learn the connection between patterns in these properties with the outcome variables.</p>
          </def>
        </def-item>
        <def-item id="def2">
          <term>Attention:</term>
          <def>
            <p>Attention is a mechanism that calculates the weight of each part of the input data to dynamically highlight relevant features. This process allows the model to focus on the more significant part of the data.</p>
          </def>
        </def-item>
        <def-item id="def3">
          <term>Transformer models:</term>
          <def>
            <p>A transformer is a deep learning model that uses the mechanism of attention at its core to create an encoder-decoder structure for prediction and modeling tasks.</p>
          </def>
        </def-item>
        <def-item id="def4">
          <term>Graph convolution network:</term>
          <def>
            <p>The graph convolution network is a deep learning model for representation learning and prediction on graph-structured data. It is based on the concept of convolution on the neighborhood of each node of the graph.</p>
          </def>
        </def-item>
        <def-item id="def5">
          <term>Positional embedding:</term>
          <def>
            <p>Positional embedding (or encoding) injects the positional context into the input data that are then used by the attention layer for extracting the contextual information.</p>
          </def>
        </def-item>
        <def-item id="def6">
          <term>Input batch:</term>
          <def>
            <p>Batches are groups of training data (commonly with a fixed size) on which the deep learning model trains. At the end of training on each batch, the predicted values are compared with the expected output variables to calculate an error. From the error, the weight parameters are updated to improve its predictive performance. This process takes place until all training batches are trained on.</p>
          </def>
        </def-item>
        <def-item id="def7">
          <term>Gate mechanism:</term>
          <def>
            <p>The gate mechanism is a block of deep learning architecture that is used to fuse the output of multiple blocks together.</p>
          </def>
        </def-item>
        <def-item id="def8">
          <term>Latent features:</term>
          <def>
            <p>As opposed to observable features, latent features are the result of more complex dependencies within the data that can be extracted via the encoder block of the transformer models.</p>
          </def>
        </def-item>
        <def-item id="def9">
          <term>Cross-validation:</term>
          <def>
            <p>Cross-validation is the process of using a subset of data, outside the training dataset, to obtain an indication of how well the trained model will generalize on unseen data. This step is carried out before prediction on test data.</p>
          </def>
        </def-item>
      </def-list>
    </glossary>
    <ref-list>
      <title>REFERENCES</title>
      <ref id="bib1">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Agarap</surname>, <given-names>A. F.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Deep learning using rectified linear units (ReLU)</article-title>. <source>arXiv:1803.08375</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1803.08375</pub-id></mixed-citation>
      </ref>
      <ref id="bib2">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Asadi</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Spatio-temporal-transformer, GitHub</article-title>, <ext-link xlink:href="https://github.com/ThisIsNima/Spatio-Temporal-Transformer" ext-link-type="uri">https://github.com/ThisIsNima/Spatio-Temporal-Transformer</ext-link></mixed-citation>
      </ref>
      <ref id="bib3">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bastos</surname>, <given-names>A. M.</given-names></string-name>, &amp; <string-name><surname>Schoffelen</surname>, <given-names>J.-M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>A tutorial review of functional connectivity analysis methods and their interpretational pitfalls</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>9</volume>, <fpage>175</fpage>. <pub-id pub-id-type="doi">10.3389/fnsys.2015.00175</pub-id>, <pub-id pub-id-type="pmid">26778976</pub-id></mixed-citation>
      </ref>
      <ref id="bib4">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Courville</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Vincent</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Representation learning: A review and new perspectives</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>35</volume>(<issue>8</issue>), <fpage>1798</fpage>â<lpage>1828</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id>, <pub-id pub-id-type="pmid">23787338</pub-id></mixed-citation>
      </ref>
      <ref id="bib5">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burgess</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Maguire</surname>, <given-names>E. A.</given-names></string-name>, &amp; <string-name><surname>OâKeefe</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2002</year>). <article-title>The human hippocampus and spatial and episodic memory</article-title>. <source>Neuron</source>, <volume>35</volume>(<issue>4</issue>), <fpage>625</fpage>â<lpage>641</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00830-9</pub-id>, <pub-id pub-id-type="pmid">12194864</pub-id></mixed-citation>
      </ref>
      <ref id="bib6">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chaddad</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Desrosiers</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Hassan</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Tanougast</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Hippocampus and amygdala radiomic biomarkers for the study of autism spectrum disorder</article-title>. <source>BMC Neuroscience</source>, <volume>18</volume>(<issue>1</issue>), <fpage>52</fpage>. <pub-id pub-id-type="doi">10.1186/s12868-017-0373-0</pub-id>, <pub-id pub-id-type="pmid">28821235</pub-id></mixed-citation>
      </ref>
      <ref id="bib7">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Nomi</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Uddin</surname>, <given-names>L. Q.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Intrinsic functional connectivity variance and state-specific under-connectivity in autism</article-title>. <source>Human Brain Mapping</source>, <volume>38</volume>(<issue>11</issue>), <fpage>5740</fpage>â<lpage>5755</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23764</pub-id>, <pub-id pub-id-type="pmid">28792117</pub-id></mixed-citation>
      </ref>
      <ref id="bib8">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chou</surname>, <given-names>Y.-H.</given-names></string-name>, <string-name><surname>Sundman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Whitson</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Gaur</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Chu</surname>, <given-names>M.-L.</given-names></string-name>, <string-name><surname>Weingarten</surname>, <given-names>C. P.</given-names></string-name>, â¦ <string-name><surname>Chen</surname>, <given-names>N.-K.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Maintenance and representation of mind wandering during resting-state fMRI</article-title>. <source>Scientific Reports</source>, <volume>7</volume>(<issue>1</issue>), <fpage>40722</fpage>. <pub-id pub-id-type="doi">10.1038/srep40722</pub-id>, <pub-id pub-id-type="pmid">28079189</pub-id></mixed-citation>
      </ref>
      <ref id="bib9">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dado</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>GÃ¼Ã§lÃ¼tÃ¼rk</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Ambrogioni</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ras</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Bosch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>van Gerven</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>GÃ¼Ã§lÃ¼</surname>, <given-names>U.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Hyperrealistic neural decoding for reconstructing faces from fMRI activations via the GAN latent space</article-title>. <source>Scientific Reports</source>, <volume>12</volume>(<issue>1</issue>), <fpage>141</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-03938-w</pub-id>, <pub-id pub-id-type="pmid">34997012</pub-id></mixed-citation>
      </ref>
      <ref id="bib10">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Defferrard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bresson</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Vandergheynst</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Convolutional neural networks on graphs with fast localized spectral filtering</article-title>. In <source>Proceedings of the 30th international conference on neural information processing systems</source> (pp. <fpage>3844</fpage>â<lpage>3852</lpage>).</mixed-citation>
      </ref>
      <ref id="bib11">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Deep learning: Methods and applications</article-title>. <source>Foundations and Trends in Signal Processing</source>, <volume>7</volume>(<issue>3â4</issue>), <fpage>197</fpage>â<lpage>387</lpage>. <pub-id pub-id-type="doi">10.1561/2000000039</pub-id></mixed-citation>
      </ref>
      <ref id="bib12">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhamala</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Jamison</surname>, <given-names>K. W.</given-names></string-name>, <string-name><surname>Sabuncu</surname>, <given-names>M. R.</given-names></string-name>, &amp; <string-name><surname>Kuceyeski</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Sex classification using long-range temporal dependence of resting-state functional MRI time series</article-title>. <source>Human Brain Mapping</source>, <volume>41</volume>(<issue>13</issue>), <fpage>3567</fpage>â<lpage>3579</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25030</pub-id>, <pub-id pub-id-type="pmid">32627300</pub-id></mixed-citation>
      </ref>
      <ref id="bib13">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Martino</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>C.-G.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Denio</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Castellanos</surname>, <given-names>F. X.</given-names></string-name>, <string-name><surname>Alaerts</surname>, <given-names>K.</given-names></string-name>, â¦ <string-name><surname>Milham</surname>, <given-names>M. P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The Autism Brain Imaging Data Exchange: Towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>. <source>Molecular Psychiatry</source>, <volume>19</volume>(<issue>6</issue>), <fpage>659</fpage>â<lpage>667</lpage>. <pub-id pub-id-type="doi">10.1038/mp.2013.78</pub-id>, <pub-id pub-id-type="pmid">23774715</pub-id></mixed-citation>
      </ref>
      <ref id="bib14">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dong</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Qiang</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lv</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Dong</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Li</surname>, <given-names>Q.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A novel fMRI representation learning framework with GAN</article-title>. In <source>International workshop on machine learning in medical imaging</source> (pp. <fpage>21</fpage>â<lpage>29</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-59861-7_3</pub-id></mixed-citation>
      </ref>
      <ref id="bib15">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Frolov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Maksimenko</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>LÃ¼ttjohann</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Koronovskii</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Hramov</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Feed-forward artificial neural network provides data-driven inference of functional connectivity</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>, <volume>29</volume>(<issue>9</issue>), <fpage>091101</fpage>. <pub-id pub-id-type="doi">10.1063/1.5117263</pub-id>, <pub-id pub-id-type="pmid">31575143</pub-id></mixed-citation>
      </ref>
      <ref id="bib16">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gadgil</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Pfefferbaum</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>E. V.</given-names></string-name>, <string-name><surname>Adeli</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Pohl</surname>, <given-names>K. M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Spatio-temporal graph convolution for resting-state fMRI analysis</article-title>. In <source>International conference on medical image computing and computer-assisted intervention</source> (pp. <fpage>528</fpage>â<lpage>538</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-59728-3_52</pub-id>, </mixed-citation>
      </ref>
      <ref id="bib17">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Duan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>J.</given-names></string-name>, â¦ <string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Decreased amygdala functional connectivity in adolescents with autism: A resting-state fMRI study</article-title>. <source>Psychiatry Research: Neuroimaging</source>, <volume>257</volume>, <fpage>47</fpage>â<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1016/j.pscychresns.2016.10.005</pub-id>, <pub-id pub-id-type="pmid">27969061</pub-id></mixed-citation>
      </ref>
      <ref id="bib18">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>. In <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source> (pp. <fpage>770</fpage>â<lpage>778</lpage>). <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation>
      </ref>
      <ref id="bib19">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Evans</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Graph theoretical modeling of brain connectivity</article-title>. <source>Current Opinion in Neurology</source>, <volume>23</volume>(<issue>4</issue>), <fpage>341</fpage>â<lpage>350</lpage>. <pub-id pub-id-type="doi">10.1097/WCO.0b013e32833aa567</pub-id>, <pub-id pub-id-type="pmid">20581686</pub-id></mixed-citation>
      </ref>
      <ref id="bib20">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinsfeld</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Craddock</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Buchweitz</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Meneguzzi</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Identification of autism spectrum disorder using deep learning and the ABIDE dataset</article-title>. <source>NeuroImage: Clinical</source>, <volume>17</volume>, <fpage>16</fpage>â<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2017.08.017</pub-id>, <pub-id pub-id-type="pmid">29034163</pub-id></mixed-citation>
      </ref>
      <ref id="bib21">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hjelm</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Plis</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Calhoun</surname>, <given-names>V.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Recurrent neural networks for spatiotemporal dynamics of intrinsic networks from fMRI data</article-title>. <source>NIPS: Brains and Bits</source>.</mixed-citation>
      </ref>
      <ref id="bib22">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hosseini</surname>, <given-names>M.-P.</given-names></string-name>, <string-name><surname>Tran</surname>, <given-names>T. X.</given-names></string-name>, <string-name><surname>Pompili</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Elisevich</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Soltanian-Zadeh</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Multimodal data analysis of epileptic EEG and rs-fMRI via deep learning and edge computing</article-title>. <source>Artificial Intelligence in Medicine</source>, <volume>104</volume>, <fpage>101813</fpage>. <pub-id pub-id-type="doi">10.1016/j.artmed.2020.101813</pub-id>, <pub-id pub-id-type="pmid">32498996</pub-id></mixed-citation>
      </ref>
      <ref id="bib23">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Makkie</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dong</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>S.</given-names></string-name>, â¦ <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Modeling task fMRI data via deep convolutional autoencoder</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>37</volume>(<issue>7</issue>), <fpage>1551</fpage>â<lpage>1561</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2017.2715285</pub-id>, <pub-id pub-id-type="pmid">28641247</pub-id></mixed-citation>
      </ref>
      <ref id="bib24">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ide</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Kurita</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Improvement of learning for CNN with ReLU activation by sparse regularization</article-title>. In <source>2017 international joint conference on neural networks (IJCNN)</source> (pp. <fpage>2684</fpage>â<lpage>2691</lpage>). <pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966185</pub-id></mixed-citation>
      </ref>
      <ref id="bib25">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jastrzebski</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Arpit</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ballas</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Verma</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Che</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Residual connections encourage iterative inference</article-title>. <source>arXiv:1710.04773</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1710.04773</pub-id></mixed-citation>
      </ref>
      <ref id="bib26">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kawahara</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Brown</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>S. P.</given-names></string-name>, <string-name><surname>Booth</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Chau</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Grunau</surname>, <given-names>R. E.</given-names></string-name>, â¦ <string-name><surname>Hamarneh</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>BrainNetCNN: Convolutional neural networks for brain networks; towards predicting neurodevelopment</article-title>. <source>NeuroImage</source>, <volume>146</volume>, <fpage>1038</fpage>â<lpage>1049</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.046</pub-id>, <pub-id pub-id-type="pmid">27693612</pub-id></mixed-citation>
      </ref>
      <ref id="bib27">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>J.-H.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Choi</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Representation learning of resting state fMRI with variational autoencoder</article-title>. <source>NeuroImage</source>, <volume>241</volume>, <fpage>118423</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118423</pub-id>, <pub-id pub-id-type="pmid">34303794</pub-id></mixed-citation>
      </ref>
      <ref id="bib28">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kim</surname>, <given-names>T. H.</given-names></string-name>, <string-name><surname>Sajjadi</surname>, <given-names>M. S. M.</given-names></string-name>, <string-name><surname>Hirsch</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>SchÃ¶lkopf</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Spatio-temporal transformer network for video restoration</article-title>. In <source>Proceedings of the European conference on computer vision (ECCV)</source> (pp. <fpage>111</fpage>â<lpage>127</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-01219-9_7</pub-id></mixed-citation>
      </ref>
      <ref id="bib29">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kipf</surname>, <given-names>T. N.</given-names></string-name>, &amp; <string-name><surname>Welling</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Semi-supervised classification with graph convolutional networks</article-title>. <source>arXiv:1609.02907</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id></mixed-citation>
      </ref>
      <ref id="bib30">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kong</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Yue</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hou</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Shu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Xie</surname>, <given-names>C.</given-names></string-name>, â¦ <string-name><surname>Yuan</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Spatio-temporal graph convolutional network for diagnosis and treatment response prediction of major depressive disorder from functional connectivity</article-title>. <source>Human Brain Mapping</source>, <volume>42</volume>(<issue>12</issue>), <fpage>3922</fpage>â<lpage>3933</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25529</pub-id>, <pub-id pub-id-type="pmid">33969930</pub-id></mixed-citation>
      </ref>
      <ref id="bib31">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>(<issue>7553</issue>), <fpage>436</fpage>â<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>, <pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
      </ref>
      <ref id="bib32">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Li</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Satterthwaite</surname>, <given-names>T. D.</given-names></string-name>, &amp; <string-name><surname>Fan</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Brain age prediction based on resting-state functional connectivity patterns using convolutional neural networks</article-title>. In <source>2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)</source> (pp. <fpage>101</fpage>â<lpage>104</lpage>). <pub-id pub-id-type="doi">10.1109/ISBI.2018.8363532</pub-id></mixed-citation>
      </ref>
      <ref id="bib33">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Deng</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Duh</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>Y.-Y.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Representation learning using multi-task deep neural networks for semantic classification and information retrieval</article-title>. In <source>Proceedings of the 2015 conference of the North American chapter of the association for computational linguistics: Human language technologies</source> (pp. <fpage>912</fpage>â<lpage>921</lpage>). <pub-id pub-id-type="doi">10.3115/v1/N15-1092</pub-id></mixed-citation>
      </ref>
      <ref id="bib34">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Mandal</surname>, <given-names>P. K.</given-names></string-name>, &amp; <string-name><surname>Mahto</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Deep CNN-LSTM with word embeddings for news headline sarcasm detection</article-title>. In <source>16th international conference on information technologyânew generations (ITNG 2019)</source> (pp. <fpage>495</fpage>â<lpage>498</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-14070-0_69</pub-id></mixed-citation>
      </ref>
      <ref id="bib35">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mantini</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Perrucci</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Del Gratta</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Romani</surname>, <given-names>G. L.</given-names></string-name>, &amp; <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Electrophysiological signatures of resting state networks in the human brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>104</volume>(<issue>32</issue>), <fpage>13170</fpage>â<lpage>13175</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0700668104</pub-id>, <pub-id pub-id-type="pmid">17670949</pub-id></mixed-citation>
      </ref>
      <ref id="bib36">
        <mixed-citation publication-type="journal"><collab collab-type="author"><italic toggle="yes">MS Windows NT kernel description</italic></collab>. (<year>n.d.</year>). <ext-link xlink:href="http://web.archive.org/web/20080207010024/https://www.808multimedia.com/winnt/kernel.htm" ext-link-type="uri">https://web.archive.org/web/20080207010024/https://www.808multimedia.com/winnt/kernel.htm</ext-link> (Accessed September 30, 2010).</mixed-citation>
      </ref>
      <ref id="bib37">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Plizzari</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cannici</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Matteucci</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Spatial temporal transformer network for skeleton-based action recognition</article-title>. In <source>International conference on pattern recognition</source> (pp. <fpage>694</fpage>â<lpage>701</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-68796-0_50</pub-id></mixed-citation>
      </ref>
      <ref id="bib38">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qu</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Xiao</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>Y.-P.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A graph deep learning model for the classification of groups with different IQ using resting state fMRI</article-title>. In <source>Medical imaging 2020: Biomedical applications in molecular, structural, and functional imaging</source> (<volume>Vol. 11317</volume>, pp. <fpage>52</fpage>â<lpage>57</lpage>). <pub-id pub-id-type="doi">10.1117/12.2549274</pub-id></mixed-citation>
      </ref>
      <ref id="bib39">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Riaz</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Asad</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Alonso</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Slabaugh</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2020</year>). <article-title>DeepFMRI: End-to-end deep learning for functional connectivity and classification of ADHD using fMRI</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>335</volume>, <fpage>108506</fpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108506</pub-id>, <pub-id pub-id-type="pmid">32001294</pub-id></mixed-citation>
      </ref>
      <ref id="bib40">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rogers</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>V. L.</given-names></string-name>, <string-name><surname>Newton</surname>, <given-names>A. T.</given-names></string-name>, &amp; <string-name><surname>Gore</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Assessing functional connectivity in the human brain by fMRI</article-title>. <source>Magnetic Resonance Imaging</source>, <volume>25</volume>(<issue>10</issue>), <fpage>1347</fpage>â<lpage>1357</lpage>. <pub-id pub-id-type="doi">10.1016/j.mri.2007.03.007</pub-id>, <pub-id pub-id-type="pmid">17499467</pub-id></mixed-citation>
      </ref>
      <ref id="bib41">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sarraf</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Tofighi</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2016a</year>). <article-title>Classification of Alzheimerâs disease using fMRI data and deep learning convolutional neural networks</article-title>. <source>arXiv:1603.08631</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1603.08631</pub-id></mixed-citation>
      </ref>
      <ref id="bib42">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sarraf</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Tofighi</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2016b</year>). <article-title>Deep learning-based pipeline to recognize Alzheimerâs disease using fMRI data</article-title>. In <source>2016 future technologies conference (FTC)</source> (pp. <fpage>816</fpage>â<lpage>820</lpage>). <pub-id pub-id-type="doi">10.1109/FTC.2016.7821697</pub-id></mixed-citation>
      </ref>
      <ref id="bib43">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scarselli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gori</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Tsoi</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Hagenbuchner</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Monfardini</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>The graph neural network model</article-title>. <source>IEEE Transactions on Neural Networks</source>, <volume>20</volume>(<issue>1</issue>), <fpage>61</fpage>â<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id>, <pub-id pub-id-type="pmid">19068426</pub-id></mixed-citation>
      </ref>
      <ref id="bib44">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suk</surname>, <given-names>H.-I.</given-names></string-name>, <string-name><surname>Wee</surname>, <given-names>C.-Y.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>S.-W.</given-names></string-name>, &amp; <string-name><surname>Shen</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>State-space model with deep learning for functional dynamics estimation in resting-state fMRI</article-title>. <source>NeuroImage</source>, <volume>129</volume>, <fpage>292</fpage>â<lpage>307</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.01.005</pub-id>, <pub-id pub-id-type="pmid">26774612</pub-id></mixed-citation>
      </ref>
      <ref id="bib45">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Treves</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Rolls</surname>, <given-names>E. T.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Computational analysis of the role of the hippocampus in memory</article-title>. <source>Hippocampus</source>, <volume>4</volume>(<issue>3</issue>), <fpage>374</fpage>â<lpage>391</lpage>. <pub-id pub-id-type="doi">10.1002/hipo.450040319</pub-id>, <pub-id pub-id-type="pmid">7842058</pub-id></mixed-citation>
      </ref>
      <ref id="bib46">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tzourio-Mazoyer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Landeau</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Papathanassiou</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Crivello</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Etard</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Delcroix</surname>, <given-names>N.</given-names></string-name>, â¦ <string-name><surname>Joliot</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2002</year>). <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>. <source>NeuroImage</source>, <volume>15</volume>(<issue>1</issue>), <fpage>273</fpage>â<lpage>289</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2001.0978</pub-id>, <pub-id pub-id-type="pmid">11771995</pub-id></mixed-citation>
      </ref>
      <ref id="bib47">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T. E. J.</given-names></string-name>, <string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name></person-group>, &amp; <collab collab-type="author">WU-Minn HCP Consortium</collab>. (<year>2013</year>). <article-title>The WU-Minn Human Connectome Project: An overview</article-title>
<source>NeuroImage</source>, <volume>80</volume>, <fpage>62</fpage>â<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id>, <pub-id pub-id-type="pmid">23684880</pub-id></mixed-citation>
      </ref>
      <ref id="bib48">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vaswani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname>, <given-names>A. N.</given-names></string-name>, â¦ <string-name><surname>Polosukhin</surname>, <given-names>I.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Attention is all you need</article-title>. In <source>Proceedings of the 31st international conference on neural information processing systems</source> (pp. <fpage>5998</fpage>â<lpage>6008</lpage>).</mixed-citation>
      </ref>
      <ref id="bib49">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vosoughi</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vijayaraghavan</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Roy</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Tweet2Vec: Learning tweet embeddings using character-level CNN-LSTM encoder-decoder</article-title>. In <source>Proceedings of the 39th international ACM SIGIR conference on research and development in information retrieval</source> (pp. <fpage>1041</fpage>â<lpage>1044</lpage>). <pub-id pub-id-type="doi">10.1145/2911451.2914762</pub-id></mixed-citation>
      </ref>
      <ref id="bib50">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wan</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Zhong</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Du</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Multiscale dynamic graph convolutional network for hyperspectral image classification</article-title>. <source>IEEE Transactions on Geoscience and Remote Sensing</source>, <volume>58</volume>(<issue>5</issue>), <fpage>3162</fpage>â<lpage>3177</lpage>. <pub-id pub-id-type="doi">10.1109/TGRS.2019.2949180</pub-id></mixed-citation>
      </ref>
      <ref id="bib51">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Hu</surname>, <given-names>X. P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Application of convolutional recurrent neural network for individual recognition based on resting state fMRI data</article-title>. <source>Frontiers in Neuroscience</source>, <volume>13</volume>, <fpage>434</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2019.00434</pub-id>, <pub-id pub-id-type="pmid">31118882</pub-id></mixed-citation>
      </ref>
      <ref id="bib52">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Hu</surname>, <given-names>X. P.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Graph convolutional network for fMRI analysis based on connectivity neighborhood</article-title>. <source>Network Neuroscience</source>, <volume>5</volume>(<issue>1</issue>), <fpage>83</fpage>â<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00171</pub-id>, <pub-id pub-id-type="pmid">33688607</pub-id></mixed-citation>
      </ref>
      <ref id="bib53">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weis</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Patil</surname>, <given-names>K. R.</given-names></string-name>, <string-name><surname>Hoffstaedter</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nostro</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name>, &amp; <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Sex classification by resting state brain connectivity</article-title>. <source>Cerebral Cortex</source>, <volume>30</volume>(<issue>2</issue>), <fpage>824</fpage>â<lpage>835</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhz129</pub-id>, <pub-id pub-id-type="pmid">31251328</pub-id></mixed-citation>
      </ref>
      <ref id="bib54">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wen</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Wei</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Han</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Deep learning methods to process fMRI data and their application in the diagnosis of cognitive impairment: A brief overview and our opinion</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>12</volume>, <fpage>23</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2018.00023</pub-id>, <pub-id pub-id-type="pmid">29755334</pub-id></mixed-citation>
      </ref>
      <ref id="bib55">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wolf</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Debut</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Sanh</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Chaumond</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Delangue</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Moi</surname>, <given-names>A.</given-names></string-name>, â¦ <string-name><surname>Rush</surname>, <given-names>A.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Transformers: State-of-the-art natural language processing</article-title>. In <source>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</source> (pp. <fpage>38</fpage>â<lpage>45</lpage>). <pub-id pub-id-type="doi">10.18653/v1/2020.emnlp-demos.6</pub-id></mixed-citation>
      </ref>
      <ref id="bib56">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dai</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Qi</surname>, <given-names>G.-J.</given-names></string-name>, &amp; <string-name><surname>Xiong</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Spatial-temporal transformer networks for traffic flow forecasting</article-title>. <source>arXiv:2001.02908</source>. <pub-id pub-id-type="doi">10.48550/arXiv.2001.02908</pub-id></mixed-citation>
      </ref>
      <ref id="bib57">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Zuo</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Liao</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Abnormal development pattern of the amygdala and hippocampus from childhood to adulthood with autism</article-title>. <source>Journal of Clinical Neuroscience</source>, <volume>78</volume>, <fpage>327</fpage>â<lpage>332</lpage>. <pub-id pub-id-type="doi">10.1016/j.jocn.2020.03.049</pub-id>, <pub-id pub-id-type="pmid">32593622</pub-id></mixed-citation>
      </ref>
      <ref id="bib58">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Calhoun</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Song</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cui</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>S.</given-names></string-name>, â¦ <string-name><surname>Sui</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Discriminating schizophrenia using recurrent neural network applied on time courses of multi-site fMRI data</article-title>. <source>eBioMedicine</source>, <volume>47</volume>, <fpage>543</fpage>â<lpage>552</lpage>. <pub-id pub-id-type="doi">10.1016/j.ebiom.2019.08.023</pub-id>, <pub-id pub-id-type="pmid">31420302</pub-id></mixed-citation>
      </ref>
      <ref id="bib59">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yin</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Wu</surname>, <given-names>F.-X.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Deep learning for brain disorder diagnosis based on fMRI images</article-title>. <source>Neurocomputing</source>, <volume>469</volume>, <fpage>332</fpage>â<lpage>345</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2020.05.113</pub-id></mixed-citation>
      </ref>
      <ref id="bib60">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Zhu</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</article-title>. <source>arXiv:1709.04875</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1709.04875</pub-id></mixed-citation>
      </ref>
      <ref id="bib61">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yu</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Yi</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Spatio-temporal graph transformer networks for pedestrian trajectory prediction</article-title>. In <source>European conference on computer vision</source> (pp. <fpage>507</fpage>â<lpage>523</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-58610-2_30</pub-id></mixed-citation>
      </ref>
      <ref id="bib62">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zerveas</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jayaraman</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Patel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bhamidipaty</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Eickhoff</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>A transformer-based framework for multivariate time series representation learning</article-title>. In <source>Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining</source> (pp. <fpage>2114</fpage>â<lpage>2124</lpage>). <pub-id pub-id-type="doi">10.1145/3447548.3467401</pub-id></mixed-citation>
      </ref>
      <ref id="bib63">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Makkie</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>M.</given-names></string-name>, â¦ <string-name><surname>Liu</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Modeling 4D fMRI data via spatio-temporal convolutional neural networks (ST-CNN)</article-title>. In <source>International conference on medical image computing and computer-assisted intervention</source> (pp. <fpage>181</fpage>â<lpage>189</lpage>). <pub-id pub-id-type="doi">10.1007/978-3-030-00931-1_21</pub-id></mixed-citation>
      </ref>
      <ref id="bib64">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhuang</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schwing</surname>, <given-names>A. G.</given-names></string-name>, &amp; <string-name><surname>Koyejo</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2019</year>). <article-title>fMRI data augmentation via synthesis</article-title>. In <source>2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019)</source> (pp. <fpage>1783</fpage>â<lpage>1787</lpage>). <pub-id pub-id-type="doi">10.1109/ISBI.2019.8759585</pub-id></mixed-citation>
      </ref>
    </ref-list>
    <sec>
      <title>Supporting Information</title>
      <p>
<ext-link xlink:href="https://github.com/ThisIsNima/Spatio-Temporal-Transformer" ext-link-type="uri">https://github.com/ThisIsNima/Spatio-Temporal-Transformer</ext-link>
</p>
    </sec>
  </back>
</article>
