<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">J Neurosci</journal-id>
      <journal-id journal-id-type="iso-abbrev">J Neurosci</journal-id>
      <journal-id journal-id-type="hwp">jneuro</journal-id>
      <journal-id journal-id-type="pmc">jneurosci</journal-id>
      <journal-id journal-id-type="publisher-id">J. Neurosci</journal-id>
      <journal-title-group>
        <journal-title>The Journal of Neuroscience</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0270-6474</issn>
      <issn pub-type="epub">1529-2401</issn>
      <publisher>
        <publisher-name>Society for Neuroscience</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">34021042</article-id>
      <article-id pub-id-type="pmc">8244967</article-id>
      <article-id pub-id-type="publisher-id">JN-RM-0261-21</article-id>
      <article-id pub-id-type="doi">10.1523/JNEUROSCI.0261-21.2021</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Articles</subject>
          <subj-group>
            <subject>Neurobiology of Disease</subject>
          </subj-group>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>The Motor Basis for Misophonia</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Kumar</surname>
            <given-names>Sukhbinder</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1575-154X</contrib-id>
          <name>
            <surname>Dheerendra</surname>
            <given-names>Pradeep</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Erfanian</surname>
            <given-names>Mercede</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Benzaquén</surname>
            <given-names>Ester</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Sedley</surname>
            <given-names>William</given-names>
          </name>
          <xref ref-type="aff" rid="aff7">
            <sup>7</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Gander</surname>
            <given-names>Phillip E.</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Lad</surname>
            <given-names>Meher</given-names>
          </name>
          <xref ref-type="aff" rid="aff7">
            <sup>7</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Bamiou</surname>
            <given-names>Doris E.</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">
            <sup>4</sup>
          </xref>
          <xref ref-type="aff" rid="aff6">
            <sup>6</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Griffiths</surname>
            <given-names>Timothy D.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="aff" rid="aff5">
            <sup>5</sup>
          </xref>
        </contrib>
        <aff id="aff1"><sup>1</sup>Biosciences Institute, Newcastle University, Newcastle upon Tyne, NE2 4HH, United Kingdom</aff>
        <aff id="aff2"><sup>2</sup>UCL Institute for Environmental Design and Engineering, The Bartlett, University College London, WC1H 0NN, United Kingdom</aff>
        <aff id="aff3"><sup>3</sup>Department of Neurosurgery, University of Iowa, Iowa City, Iowa 52242</aff>
        <aff id="aff4"><sup>4</sup>UCL Ear Institute, London, WC1X 8EE, United Kingdom</aff>
        <aff id="aff5"><sup>5</sup>Wellcome Centre for Human NeuroImaging, London, WC1N 3BG, United Kingdom</aff>
        <aff id="aff6"><sup>6</sup>Biomedical Research Centre, University College London Hospitals, London, WC1E 6AB, United Kingdom</aff>
        <aff id="aff7"><sup>7</sup>Translational and Clinical Research Institute, Newcastle University, Newcastle upon Tyne, NE2 4HH</aff>
      </contrib-group>
      <author-notes>
        <corresp>Correspondence should be addressed to Sukhbinder Kumar at <email>Sukhbinder.kumar@ncl.ac.uk</email></corresp>
        <fn fn-type="con">
          <p>Author contributions: S.K., W.S., and T.D.G. designed research; S.K. and M.E. performed research; S.K., P.D., M.E., E.B., and T.D.G. analyzed data; S.K. wrote the first draft of the paper; S.K., P.D., M.E., E.B., W.S., P.E.G., M.L., D.E.B., and T.D.G. edited the paper; S.K., W.S., P.E.G., M.L., D.E.B., and T.D.G. wrote the paper.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="ppub">
        <day>30</day>
        <month>6</month>
        <year>2021</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>30</day>
        <month>6</month>
        <year>2021</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="ppub"/>. -->
      <volume>41</volume>
      <issue>26</issue>
      <fpage>5762</fpage>
      <lpage>5770</lpage>
      <history>
        <date date-type="received">
          <day>3</day>
          <month>2</month>
          <year>2021</year>
        </date>
        <date date-type="rev-recd">
          <day>20</day>
          <month>4</month>
          <year>2021</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>4</month>
          <year>2021</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Copyright © 2021 Kumar et al.</copyright-statement>
        <copyright-year>2021</copyright-year>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</ext-link>, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</license-p>
        </license>
      </permissions>
      <self-uri content-type="pdf" xlink:href="zns02621005762.pdf"/>
      <abstract>
        <p>Misophonia is a common disorder characterized by the experience of strong negative emotions of anger and anxiety in response to certain everyday sounds, such as those generated by other people eating, drinking, and breathing. The commonplace nature of these “trigger” sounds makes misophonia a devastating disorder for sufferers and their families. How such innocuous sounds trigger this response is unknown. Since most trigger sounds are generated by orofacial movements (e.g., chewing) in others, we hypothesized that the mirror neuron system related to orofacial movements could underlie misophonia. We analyzed resting state fMRI (rs-fMRI) connectivity (<italic>N</italic> = 33, 16 females) and sound-evoked fMRI responses (<italic>N</italic> = 42, 29 females) in misophonia sufferers and controls. We demonstrate that, compared with controls, the misophonia group show no difference in auditory cortex responses to trigger sounds, but do show: (1) stronger rs-fMRI connectivity between both auditory and visual cortex and the ventral premotor cortex responsible for orofacial movements; (2) stronger functional connectivity between the auditory cortex and orofacial motor area during sound perception in general; and (3) stronger activation of the orofacial motor area, specifically, in response to trigger sounds. Our results support a model of misophonia based on “hyper-mirroring” of the orofacial actions of others with sounds being the “medium” via which action of others is excessively mirrored. Misophonia is therefore not an abreaction to sounds, per se, but a manifestation of activity in parts of the motor system involved in producing those sounds. This new framework to understand misophonia can explain behavioral and emotional responses and has important consequences for devising effective therapies.</p>
        <p><bold>SIGNIFICANCE STATEMENT</bold> Conventionally, misophonia, literally “hatred of sounds” has been considered as a disorder of sound emotion processing, in which “simple” eating and chewing sounds produced by others cause negative emotional responses. Our data provide an alternative but complementary perspective on misophonia that emphasizes the action of the trigger-person rather than the sounds which are a byproduct of that action. Sounds, in this new perspective, are only a “medium” via which action of the triggering-person is mirrored onto the listener. This change in perspective has important consequences for devising therapies and treatment methods for misophonia. It suggests that, instead of focusing on sounds, which many existing therapies do, effective therapies should target the brain representation of movement.</p>
      </abstract>
      <kwd-group>
        <kwd>auditory</kwd>
        <kwd>fMRI</kwd>
        <kwd>mirror neurons</kwd>
        <kwd>misophonia</kwd>
        <kwd>motor system</kwd>
        <kwd>resting state connectivity</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>Misophonia Research Fund, REAM foundation</funding-source>
          <award-id>NU-003397</award-id>
        </award-group>
        <award-group>
          <funding-source><named-content content-type="funder-id">http://doi.org/10.13039/100010269</named-content>Wellcome Trust (Wellcome)</funding-source>
          <award-id>WT106964MA</award-id>
        </award-group>
      </funding-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>twij</meta-name>
          <meta-value>true</meta-value>
        </custom-meta>
        <custom-meta>
          <meta-name>special-property</meta-name>
          <meta-value>cellular</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>Introduction</title>
      <p>Misophonia is a disorder of emotion processing in which ordinary day-to-day sounds, at normal volume, cause distress to the point that it has debilitating effects on the occupational, social, and domestic life of the sufferer. Typically, these sounds (termed “trigger” sounds) include eating, chewing, drinking, and breathing sounds made by people other than the sufferer. Reactions of a misophonia sufferer to trigger sounds include anger, irritation, disgust, anxiety, and, in some cases, violent rage accompanied by a strong urge to escape from the situation. Since trigger sounds are common, and almost inescapable in the company of others, misophonia can lead to social isolation, and cases of suicide and suicide attempts have been reported in the media (<xref rid="B39" ref-type="bibr">Nauman, 2017</xref>). Although comprehensive epidemiological data on misophonia are lacking, three studies (<xref rid="B58" ref-type="bibr">Wu et al., 2014</xref>; <xref rid="B59" ref-type="bibr">Zhou et al., 2017</xref>; <xref rid="B40" ref-type="bibr">Naylor et al., 2021</xref>) in undergraduate student samples found that 6%-20% had moderate to severe symptoms of misophonia.</p>
      <p>A considerable effort has been made in the last few years to understand the brain mechanisms of misophonia. <xref rid="B33" ref-type="bibr">Kumar et al. (2017)</xref> showed hyperactivity of anterior insula, specifically in response to trigger sounds in the misophonia group compared with normal healthy controls. Moreover, the functional connectivity of anterior insula, in response to trigger sounds, was stronger to a number of brain regions, including core nodes of the default mode network (ventromedial prefrontal cortex and posterior cingulate cortex). Using a similar paradigm with video clips as stimuli, <xref rid="B48" ref-type="bibr">Schroder et al. (2019)</xref> replicated the findings of hyperactivity in anterior insula to trigger stimuli in subjects with misophonia. Although the neuroimaging studies have identified a brain network underlying misophonia, the question of why such innocuous sounds cause distress and hyperactivity of brain regions remains unanswered.</p>
      <p>Interestingly, most of the triggers in misophonia happen to be human-generated sounds of eating and chewing (<xref rid="B32" ref-type="bibr">Kumar et al., 2014</xref>; <xref rid="B29" ref-type="bibr">Jager et al., 2020</xref>), which involve orofacial actions. Although sounds are most distressing, images and silent videos of eating and chewing can also cause distress. Trigger sounds in misophonia “automatically” elicit the emotional response (<xref rid="B13" ref-type="bibr">Dozier, 2015</xref>) without having any self-control, despite preserved insight into the disproportionate nature of the feelings and reactions evoked (<xref rid="B10" ref-type="bibr">Cavanna and Seri, 2015</xref>). Additionally, trigger sounds/actions can induce spontaneous mimicry of the triggering orofacial action in many misophonia sufferers (<xref rid="B14" ref-type="bibr">Edelstein et al., 2013</xref>). Since the mirror neuron system (<xref rid="B44" ref-type="bibr">Rizzolatti and Craighero, 2004</xref>) responds to the action of others and is known to underlie spontaneous mimicry (<xref rid="B43" ref-type="bibr">Prochazkova and Kret, 2017</xref>) and emotional responses (<xref rid="B3" ref-type="bibr">Bastiaansen et al., 2009</xref>), we speculated that a process of “mirroring” the action represented by trigger sounds might be a fundamental part of the mechanism underlying misophonia. That is, in misophonia sufferers, trigger stimuli activate the same part of motor cortex that is active during generation of trigger stimuli.</p>
      <p>Mirror neurons are a set of neurons discovered in the motor cortex of monkeys that fire not only when the monkey performs a particular action but also when the monkey sees another individual performing the same action (<xref rid="B42" ref-type="bibr">Pellegrino et al., 1992</xref>; <xref rid="B44" ref-type="bibr">Rizzolatti and Craighero, 2004</xref>). For example, there are mouth mirror neurons related to ingestive functions, such as chewing or sucking of food and lip-smacking (<xref rid="B18" ref-type="bibr">Ferrari et al., 2003</xref>). Moreover, the mirror neurons can be activated not only by the sight of action but also by the sounds of the action (<xref rid="B31" ref-type="bibr">Kohler et al., 2002</xref>). In humans, mirroring of actions takes place in a network of brain areas, including the ventral premotor cortex referred to as the Mirror Neuron System (MNS) (<xref rid="B27" ref-type="bibr">Iacoboni and Dapretto, 2006</xref>). The MNS has been shown to mirror mouth (e.g., biting and chewing an apple), hand and foot actions (<xref rid="B7" ref-type="bibr">Buccino et al., 2001</xref>) and many others (<xref rid="B45" ref-type="bibr">Rizzolatti and Sinigaglia, 2010</xref>).</p>
      <p>A defining feature of the mirror neuron system is that it associates a pattern of auditory or visual input (e.g., seeing or hearing somebody chewing) to a part of the motor cortex (orofacial motor cortex) involved in producing the motor movement associated with the input (e.g., the mouth or orofacial movements) in others. This requires connectivity between sensory and motor regions. We therefore first estimated connectivity of auditory and visual cortex to the rest of the brain in misophonia and control groups using analyses based on resting state fMRI (rsfMRI), when there was no specific stimulus or task performed. We next analyzed the change in functional connectivity of two groups in response to three categories of sounds (trigger sounds, which evoked misophonic reaction in a misophonia group; unpleasant sounds, which are perceived to be aversive by both groups and; neutral sounds). Last, activation of auditory cortex and orofacial motor cortex to the three sound categories was estimated. Put together, our data support a model of misophonia based on “hyper-mirroring” of actions of others in which there is an excessive engagement of the orofacial motor cortex by the auditory and visual sensory input associated with those actions. These results provide a new framework to understand misophonia, which has important consequences for the type of therapy and treatment options to be considered for misophonia.</p>
    </sec>
    <sec sec-type="materials|methods">
      <title>Materials and Methods</title>
      <p>We present data from two experiments: (1) rsfMRI and (2) sound-evoked fMRI. The rsfMRI data are new. The data for the sound-evoked fMRI experiment have been published previously (<xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>) and are reanalyzed here.</p>
      <sec>
        <title/>
        <sec sec-type="subjects">
          <title>Subjects</title>
          <p>Seventeen subjects with misophonia and 20 control subjects were recruited for participation in the rsfMRI study after providing written informed consent to procedures approved by the local ethics committee. The misophonia subjects were recruited via an advertisement on a misophonia support website (<ext-link ext-link-type="uri" xlink:href="https://www.allergictosound.com/">https://www.allergictosound.com/</ext-link>). Misophonia participants were first required to complete three questionnaires: (1) a misophonia questionnaire designed in our laboratory and used previously in our studies (<xref rid="B32" ref-type="bibr">Kumar et al., 2014</xref>, <xref rid="B33" ref-type="bibr">2017</xref>); (2) the Misophonia Amsterdam Questionnaire (<xref rid="B47" ref-type="bibr">Schroder et al., 2013</xref>); and (3) the Misophonia Questionnaire (<xref rid="B58" ref-type="bibr">Wu et al., 2014</xref>). A misophonia participant was recruited for the study if all of the following applied: (1) they identified sounds of eating, breathing, or chewing as trigger sounds; (2) sounds alone could trigger the misophonic reaction (i.e., no picture or video of the person producing trigger sounds was needed along with sounds); (3) the person producing trigger sounds did not have to be a specific person, such as a close family member; and (4) they scored 10 or higher (moderate to extreme misophonia) on the Misophonia Amsterdam Questionnaire. All subjects were screened by telephone by the first and third author to confirm their symptoms and questionnaire responses, and to rule out MRI contraindications.</p>
          <p>Controls were recruited via an advertisement on a local university website. In the advertisement, the exact purpose of the study was not mentioned. Instead, it was stated that the objective of the study was to determine brain responses to our day-to-day environmental sounds. Once participants signed up for the study, they were asked by telephone how they respond to environmental sounds, including sounds of eating and breathing. They were then asked to complete the Misophonia Questionnaire (<xref rid="B58" ref-type="bibr">Wu et al., 2014</xref>). If typical symptoms of misophonia were absent (e.g., responding angrily, leaving the situation in response to typical trigger sounds in misophonia), the subject was recruited. No subject who signed up for the study was incidentally identified to have misophonia. The misophonia and control groups were matched in age and sex. All participants were paid £10/h plus travel expenses.</p>
          <p>In the sound-evoked fMRI study, 20 misophonia sufferers and 22 age- and sex-matched controls were recruited following a procedure similar to that described above (for details, see <xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>).</p>
        </sec>
        <sec>
          <title>Experimental procedure</title>
          <sec>
            <title>rs-fMRI</title>
            <p>Ten minutes of rsfMRI data were acquired while participants kept their eyes open. An eye-tracker was used to check whether participants conformed to the instruction. Physiologic parameters, heart rate, and respiration were measured continuously using a pulse oximeter and respiration belt.</p>
          </sec>
          <sec>
            <title>Sound-evoked fMRI</title>
            <p>fMRI data were continually acquired during the presentation of three categories of sounds: (1) trigger sounds, which evoke a misophonic reaction in subjects with misophonia (e.g., eating/chewing sounds); (2) aversive sounds, which are perceived to be unpleasant by both groups but do not evoke a misophonic response (e.g., an infant cry); and (3) neutral sounds. A list of sounds is given in <xref rid="T1" ref-type="table">Table 1</xref>. After every sound presentation for 15 s, subjects gave two ratings: (1) how annoying the sound was (both groups) and (2) how effectively the sound triggered misophonic distress (misophonia subjects) or how antisocial the sound was, based on whether the subject would move away from the source of sound (control subjects). A total of 126 trials (42 for each sound category) were presented across 5 sessions each lasting ∼11 min. Further details of the procedure can be found from our previous publication (<xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>).</p>
            <table-wrap id="T1" orientation="portrait" position="float">
              <label>Table 1.</label>
              <caption>
                <p>List of sounds used in the sound-evoked experiment (<xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>)</p>
              </caption>
              <table frame="hsides" rules="groups">
                <thead valign="bottom">
                  <tr>
                    <th align="left" rowspan="1" colspan="1">Trigger sounds</th>
                    <th align="left" rowspan="1" colspan="1">Unpleasant sounds</th>
                    <th align="left" rowspan="1" colspan="1">Neutral sounds</th>
                  </tr>
                </thead>
                <tbody valign="top">
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Apple crunching</td>
                    <td align="left" rowspan="1" colspan="1">Baby crying</td>
                    <td align="left" rowspan="1" colspan="1">Busy cafe</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Coughing and sniffing</td>
                    <td align="left" rowspan="1" colspan="1">Belch sound</td>
                    <td align="left" rowspan="1" colspan="1">Fan sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Crisps eating</td>
                    <td align="left" rowspan="1" colspan="1">Buzzer sound</td>
                    <td align="left" rowspan="1" colspan="1">Faucet sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Breathing sound</td>
                    <td align="left" rowspan="1" colspan="1">Buzzing bees</td>
                    <td align="left" rowspan="1" colspan="1">Hair dryer sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Cutlery sounds</td>
                    <td align="left" rowspan="1" colspan="1">Dentist drill</td>
                    <td align="left" rowspan="1" colspan="1">Helicopter sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Eating food sound 1</td>
                    <td align="left" rowspan="1" colspan="1">Female crying</td>
                    <td align="left" rowspan="1" colspan="1">Kettle boiling</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Eating food sound 2</td>
                    <td align="left" rowspan="1" colspan="1">Male crying</td>
                    <td align="left" rowspan="1" colspan="1">Toilet flush</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Eating salad and cutlery</td>
                    <td align="left" rowspan="1" colspan="1">Multiple dogs barking</td>
                    <td align="left" rowspan="1" colspan="1">Traffic sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Gulping water</td>
                    <td align="left" rowspan="1" colspan="1">Vomit sound</td>
                    <td align="left" rowspan="1" colspan="1">Vacuum cleaner</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Slurping</td>
                    <td align="left" rowspan="1" colspan="1">Multiple infants crying</td>
                    <td align="left" rowspan="1" colspan="1">Washing machine</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Eating with slurping</td>
                    <td align="left" rowspan="1" colspan="1">Alarm sound</td>
                    <td align="left" rowspan="1" colspan="1">Rain sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Sniffing</td>
                    <td align="left" rowspan="1" colspan="1">Toddler crying</td>
                    <td align="left" rowspan="1" colspan="1">Shower sound</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Chewing</td>
                    <td align="left" rowspan="1" colspan="1">Jack hammer</td>
                    <td align="left" rowspan="1" colspan="1">Phone ringing</td>
                  </tr>
                  <tr>
                    <td align="left" rowspan="1" colspan="1">Packet opening and eating</td>
                    <td align="left" rowspan="1" colspan="1">Female scream</td>
                    <td align="left" rowspan="1" colspan="1">Brushing teeth</td>
                  </tr>
                </tbody>
              </table>
            </table-wrap>
          </sec>
        </sec>
        <sec>
          <title>Functional imaging data acquisition</title>
          <sec>
            <title>rs-fMRI</title>
            <p>Resting state MRI data were collected on a Siemens 3 Tesla Trio whole-body MRI scanner (Siemens Healthcare) with a 32-channel head coil at the Wellcome Center for Human Neuroimaging, University College London. The subject movement was discouraged by instruction and by use of soft padding within the head coil. The acquisition parameters used were as follows: TR = 3.36 s; in-plane resolution = 3 mm isotropic; TE = 30 ms; 48 slices (covering the whole brain); matrix size = 64 × 64; echo spacing = 0.5 ms; orientation = transverse; slice tilt = −30° relative to the AC-PC line. A total of 180 volumes were acquired for the resting state.</p>
          </sec>
          <sec>
            <title>Sound-evoked fMRI</title>
            <p>All imaging data were collected on a Siemens 3 Tesla whole-body MRI scanner (Siemens Healthcare) at the Wellcome Center for Human Neuroimaging, University College London. fMRI data were acquired continuously with a 12-channel coil using a sequence that was optimized for acquisition from the amygdala and orbitofrontal cortex (<xref rid="B54" ref-type="bibr">Weiskopf et al., 2006</xref>).The fMRI acquisition parameters were the same as in the resting state acquisition. A total of 1005 volumes were acquired across 5 sessions. Field maps were acquired (parameters: short TE = 10 ms; long TE = 12.46 ms; polarity of phase-encoding blips = −1; EPI readout time = 37 ms) for every subject after the third session.</p>
          </sec>
        </sec>
        <sec>
          <title>Structural data acquisition for use with rs-fMRI</title>
          <p>A structural scan for each participant was acquired using a whole-brain quantitative multiparameter maps protocol (<xref rid="B55" ref-type="bibr">Weiskopf et al., 2013</xref>), with 32-channel head coil, which consisted of a total of five sequences: three FLASH sequences and two calibration sequences for correcting field inhomogeneities (<xref rid="B34" ref-type="bibr">Lutti et al., 2010</xref>, <xref rid="B35" ref-type="bibr">2012</xref>). The three FLASH sequences were, respectively, proton density (PD), magnetization transfer (MT), and T1-weighted by choosing appropriate values of TR and flip angle (α) for each of them. The TRs and flip angles for the three FLASH sequences were as follows: PD, TR = 23.7 ms, α = 6°; MT: TR = 23.7 ms, α = 6°; T1: TR = 18.7 ms, α = 20°. For the MT-weighted acquisition, a Gaussian RF pulse of 4 ms duration and 220° nominal flip angle was applied 2 kHz off-resonance before nonselective excitation. Gradient echoes were acquired with alternating readout gradient polarity at 6 equidistant times between 2.2 and 14.7 ms. For PD-weighted acquisition, two additional echoes at 17.2 and 19.7 ms were acquired. A high readout bandwidth of 425 Hz/pixel was used to reduce off-resonance artifacts. During acquisition, subjects were encouraged to be as still as possible with eyes open or closed.</p>
        </sec>
        <sec>
          <title>Structural data acquisition for use with sound-evoked fMRI</title>
          <p>The procedure and parameters for the structural MRI in the sound-evoked fMRI experiment were the same as for the rsfMRI experiment.</p>
        </sec>
        <sec>
          <title>Analysis</title>
          <sec>
            <title>Resting state functional connectivity</title>
            <p>Functional connectivity analysis was performed using Conn toolbox (version 19.b) (<xref rid="B56" ref-type="bibr">Whitfield-Gabrieli and Nieto-Castanon, 2012</xref>). The first five volumes were discarded from the analysis to allow magnetic strength of the scanner to reach steady state. The data were preprocessed using the default preprocessing pipeline (realignment, slice time correction, outlier detection, segmentation and normalization, and smoothing with an 8 mm kernel). A T1-weighted scan acquired as a part of the multiparameter map protocol was used as a structural scan for each subject. The ROIs for auditory cortex (Heschl's gyrus [HG], planum temporale [PT]) were anatomically defined using the brain atlas accompanying Conn toolbox. The ROIs for visual cortex (primary visual cortex V1, secondary visual cortex V2) were extracted from Anatomy toolbox (<xref rid="B16" ref-type="bibr">Eickhoff et al., 2005</xref>). The ROIs for dorsal (dPMC) and ventral (vPMC) premotor areas were based on the Human Motor Area Template developed by <xref rid="B36" ref-type="bibr">Mayka et al. (2006)</xref>. The anterior insula seed regions were based on a sphere of 6 mm radius around the maxima of activation observed in <xref rid="B33" ref-type="bibr">Kumar et al. (2017)</xref>. The time series was extracted from each voxel of the ROI and then averaged across voxels to define a single time series for the ROI. The effect of movement on the BOLD signal was reduced by regressing out motion parameters, along with their first-order temporal derivative, by running whole-brain voxelwise regression. The effect of physiological noise (cardiac and respiratory) was removed by generating 14 regressors (6 for cardiac phase, 6 for respiratory phase, 1 for heart rate, 1 for change in respiratory volume) using the Physio Toolbox (<xref rid="B26" ref-type="bibr">Hutton et al., 2011</xref>) and their first-order temporal derivative and using these regressors of no interest at the first level of analysis. Additionally, five covariates were generated using the aCompCor method (<xref rid="B4" ref-type="bibr">Behzadi et al., 2007</xref>), which uses principal component analysis on the measurements made in the white matter and CSF of each individual subject's segmented white matter and CSF masks. The data were bandpass filtered in the range 0.008-0.09 Hz, and first-level functional connectivity for each group was computed using bivariate correlation coefficient between the seed time series and time series from all other voxels in the brain (seed-to-voxel analysis). Comparison of connectivity between the two groups at the second level was undertaken using two-sample <italic>t</italic> tests with “age” and “sex” as regressors of no interest. All results shown are cluster-corrected with (one-sided) cluster defining threshold of <italic>p</italic> = 0.001.</p>
          </sec>
          <sec>
            <title>Sound-evoked fMRI activations</title>
            <p>The analysis was performed using a standard GLM as implemented in SPM12 toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/software/spm12/">https://www.fil.ion.ucl.ac.uk/spm/software/spm12/</ext-link>). The three categories of sounds were separately modeled as events of 15 s duration and then convolved with the HRF to generate three regressors. Button presses and motion regressors were used as regressors of no interest. Further details of the analysis can be found in <xref rid="B33" ref-type="bibr">Kumar et al. (2017)</xref>. Interaction between group and sound type in the orofacial motor area was computed using a 2 × 3 ANOVA with group (Misophonia and Control) and sound type (trigger, unpleasant, and neutral) as factors, and subject effects modeled. The ROI for orofacial motor cortex was defined based on the resting state connectivity. Specifically, the part of vPMC which showed stronger connectivity to PT in resting state was chosen as an ROI. Beta values from within an ROI were extracted using in-house MATLAB scripts. Variation of β values in the orofacial motor cortex with rating was estimated using regression analysis and then comparing the regression coefficients between two groups using two-sample <italic>t</italic> test.</p>
          </sec>
          <sec>
            <title>Sound-evoked functional connectivity</title>
            <p>The ROI for the orofacial motor area was chosen as above. Three regressors for the sound categories were defined as in the GLM analysis. Other regressors (cardiac, breathing, and motion) and procedure for analysis were similar to the one used for resting state functional connectivity, except that the data were high pass filtered with a cutoff frequency of 1/125 Hz. All results shown are cluster-corrected with (one-sided) cluster defining the threshold of <italic>p</italic> = 0.001.</p>
          </sec>
        </sec>
      </sec>
    </sec>
    <sec sec-type="results">
      <title>Results</title>
      <p>One misophonia subject was excluded from the resting state analysis because the physiological measurements could not be accessed. Three control subjects were excluded because of excessive movements in the MRI scanner. The final sample for analysis of resting state data consisted of 16 misophonia and 17 control subjects.</p>
      <sec>
        <title>Demographics and questionnaire scores</title>
        <p>There was no significant difference between age (<italic>p</italic> = 0.38) and sex (<italic>p</italic> = 0.86) of the two groups. The misophonia group scored higher (<italic>p</italic> &lt; 0.001) on the Misophonia Questionnaire (<xref rid="B58" ref-type="bibr">Wu et al., 2014</xref>) compared with controls. Misophonia subjects scored an average of 42.6 on the Misophonia Questionnaire compared with 17.6 in controls. Additionally, misophonia subjects scored an average of 15.5 of a maximum of 24 on the Amsterdam Misophonia Questionnaire (<xref rid="B47" ref-type="bibr">Schroder et al., 2013</xref>), which corresponds to “severe” misophonia. The demography and questionnaire scores are summarized in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
        <table-wrap id="T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Demographics and questionnaire scores (mean ± SD) for the two groups</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead valign="bottom">
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1">Misophonia</th>
                <th align="center" rowspan="1" colspan="1">Control</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of subjects (<italic>N</italic>)</td>
                <td align="char" char="." rowspan="1" colspan="1">16</td>
                <td align="char" char="." rowspan="1" colspan="1">17</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Sex (female)</td>
                <td align="char" char="." rowspan="1" colspan="1">8</td>
                <td align="char" char="." rowspan="1" colspan="1">8</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Age (mean ± SD)</td>
                <td align="char" char="±" rowspan="1" colspan="1">38.7 ± 10.3</td>
                <td align="char" char="±" rowspan="1" colspan="1">35.6 ± 9.6</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Misophonia Questionnaire (<xref rid="B58" ref-type="bibr">Wu et al., 2014</xref>) (symptoms + behavioral)</td>
                <td align="char" char="±" rowspan="1" colspan="1">42.6 ± 10.7</td>
                <td align="char" char="±" rowspan="1" colspan="1">17.6 ± 8.8</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Amsterdam Questionnaire (mean ± SD) (<xref rid="B47" ref-type="bibr">Schroder et al., 2013</xref>)</td>
                <td align="char" char="±" rowspan="1" colspan="1">15.5 ± 3.4</td>
                <td align="center" rowspan="1" colspan="1">—</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>Resting state functional connectivity</title>
        <p>Since the misophonic reaction begins with audio/visual input, connectivity of auditory and visual cortex was estimated and compared between the two groups. Anatomically defined HG (which contains primary auditory cortex) and PT (secondary auditory cortex) were separately chosen as seed regions, and their connectivity to every other voxel in the brain (seed-to-voxel analysis) was estimated. Relative to controls, the misophonia group showed a significant increase in functional connectivity of right PT to vPMC (<xref ref-type="fig" rid="F1">Fig. 1</xref><italic>A</italic>; peak at: 60, 12, 24; <italic>t</italic><sub>(29)</sub> = 4.62; <italic>p</italic> &lt; 0.001, q(FDR) &lt; 0.05; cluster size = 283). No other cluster in the brain showed significant increased connectivity to PT. There was no significant difference in HG connectivity between the two groups. As in the case of auditory cortex, we chose both primary (V1) and secondary (V2) visual cortex as seed regions. Relative to controls, the misophonia group showed significantly increased connectivity of right V2 to vPMC (<xref ref-type="fig" rid="F1">Fig. 1</xref><italic>B</italic>; peak at: 58, 0, 24; <italic>t</italic><sub>(29)</sub> = 4.34; <italic>p</italic> &lt; 0.001, q(FDR) &lt; 0.05; cluster size = 221; <xref rid="T3" ref-type="table">Table 3</xref>), which is very close to the part of vPMC that showed increased connectivity to right PT. This area of the motor cortex, when electrically stimulated, produces mouth and lip movements (<xref rid="B30" ref-type="bibr">Kern et al., 2019</xref>) and is active when either mouth or lip movements are made (<xref rid="B15" ref-type="bibr">Ehrsson et al., 2003</xref>; <xref rid="B20" ref-type="bibr">Grabski et al., 2012</xref>; <xref rid="B30" ref-type="bibr">Kern et al., 2019</xref>) or when these actions (e.g., someone biting an apple) made by others are passively observed (<xref rid="B7" ref-type="bibr">Buccino et al., 2001</xref>; <xref rid="B30" ref-type="bibr">Kern et al., 2019</xref>). Moreover, the vPMC is a part of the human mirror neuron system (<xref rid="B44" ref-type="bibr">Rizzolatti and Craighero, 2004</xref>). Other areas to which connectivity of V2 was stronger in misophonia included right anterior insula and right parietal operculum/PT (<xref rid="T3" ref-type="table">Table 3</xref>). There was no significant difference between the connectivity of V1 to vPMC in the two groups, but stronger connectivity of left V1 to right anterior insula and planum polare (<xref rid="T3" ref-type="table">Table 3</xref>) in the misophonia group was observed.</p>
        <table-wrap id="T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>List of brain areas that show significant change in resting state connectivity in misophonia compared with controls</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead valign="bottom">
              <tr>
                <th align="left" rowspan="1" colspan="1">Region name</th>
                <th align="left" rowspan="1" colspan="1">MNI coordinates (mm) of the maxima</th>
                <th align="left" rowspan="1" colspan="1">No. of voxels</th>
                <th align="left" rowspan="1" colspan="1"><italic>t</italic> value at the maxima</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td align="left" rowspan="1" colspan="1">Stronger connectivity to right V2 in misophonia compared with controls</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Right vPMC</td>
                <td align="char" char="," rowspan="1" colspan="1">58, 0, 24</td>
                <td align="char" char="." rowspan="1" colspan="1">221</td>
                <td align="char" char="." rowspan="1" colspan="1">4.34</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Right anterior insula</td>
                <td align="char" char="," rowspan="1" colspan="1">34, 2,10</td>
                <td align="char" char="." rowspan="1" colspan="1">180</td>
                <td align="char" char="." rowspan="1" colspan="1">4.88</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Right parietal operculum/PT</td>
                <td align="char" char="," rowspan="1" colspan="1">30, −34, 20</td>
                <td align="char" char="." rowspan="1" colspan="1">136</td>
                <td align="char" char="." rowspan="1" colspan="1">4.94</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Stronger connectivity to left V1 in misophonia compared with controls</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Right anterior insula</td>
                <td align="char" char="," rowspan="1" colspan="1">42, 8, −10</td>
                <td align="char" char="." rowspan="1" colspan="1">169</td>
                <td align="char" char="." rowspan="1" colspan="1">4.51</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Left planum polare/STG</td>
                <td align="char" char="," rowspan="1" colspan="1">−60, 6, −4</td>
                <td align="char" char="." rowspan="1" colspan="1">118</td>
                <td align="char" char="." rowspan="1" colspan="1">4.84</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Lateral occipital cortex, occipital pole</td>
                <td align="char" char="," rowspan="1" colspan="1">22, −84, 14</td>
                <td align="char" char="." rowspan="1" colspan="1">978</td>
                <td align="char" char="." rowspan="1" colspan="1">5.57</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Stronger connectivity to right vPMC in misophonia compared with controls</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Occipital cortex, middle temporal gyrus</td>
                <td align="char" char="," rowspan="1" colspan="1">40, −64, 2</td>
                <td align="char" char="." rowspan="1" colspan="1">342</td>
                <td align="char" char="." rowspan="1" colspan="1">5.78</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Brainstem</td>
                <td align="char" char="," rowspan="1" colspan="1">−2, −10, −34</td>
                <td align="char" char="." rowspan="1" colspan="1">230</td>
                <td align="char" char="." rowspan="1" colspan="1">6.91</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Fusiform gyrus</td>
                <td align="char" char="," rowspan="1" colspan="1">24, −72, −12</td>
                <td align="char" char="." rowspan="1" colspan="1">204</td>
                <td align="char" char="." rowspan="1" colspan="1">5.03</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Anterior insula, planum polare, STG</td>
                <td align="char" char="," rowspan="1" colspan="1">42, 6, −6</td>
                <td align="char" char="." rowspan="1" colspan="1">165</td>
                <td align="char" char="." rowspan="1" colspan="1">4.64</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Stronger connectivity to the left anterior insula in misophonia compared with controls</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Motor/somatosensory cortex (M1/S1)</td>
                <td align="char" char="," rowspan="1" colspan="1">−18, −16, 78</td>
                <td align="char" char="." rowspan="1" colspan="1">383</td>
                <td align="char" char="." rowspan="1" colspan="1">4.82</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">    Cerebellum (lobule 6)</td>
                <td align="char" char="," rowspan="1" colspan="1">28, −54, −22</td>
                <td align="char" char="." rowspan="1" colspan="1">150</td>
                <td align="char" char="." rowspan="1" colspan="1">5.24</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <fig id="F1" orientation="portrait" position="float">
          <label>Figure 1.</label>
          <caption>
            <p>Resting state functional connectivity of (<bold><italic>A</italic></bold>) right PT, (<bold><italic>B</italic></bold>) right V2, (<bold><italic>C</italic></bold>) right vPMC (the black curve in the top panel indicates the boundary of part of vPMC, which shows stronger connectivity to PT), and (<bold><italic>D</italic></bold>) left anterior insula. Connectivity of each of these seed regions was analyzed with the rest of the brain (seed-to-voxel analysis). The results shown are cluster-corrected with a cluster defining threshold of <italic>p</italic> = 0.001. The top row shows the seed regions; the middle two rows show the connectivity pattern overlaid on the sagittal and coronal sections of the structural image; the bottom row shows bar plots of connectivity strength in the two groups. Data in the bar plots represent mean ± SEM.</p>
          </caption>
          <graphic xlink:href="SN-JNSJ210368F001"/>
        </fig>
        <p>To investigate whether “mirroring” could underlie the misophonic distress, we used rsfMRI data to estimate the connectivity of vPMC, which contains orofacial motor cortex that showed stronger connectivity to PT (<xref ref-type="fig" rid="F1">Fig. 1</xref><italic>C</italic>, top, boundary marked in black) and the part of anterior insula whose responses most strongly correlated with misophonic distress in our previous work (<xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>). To further test the specificity of connectivity of vPMC, we also chose dPMC as an ROI and estimated its connectivity. Increased connectivity of the right vPMC to anterior insula was observed in the misophonia group (<xref ref-type="fig" rid="F1">Fig. 1</xref><italic>C</italic>; peak at: 42, 6, −6, <italic>t</italic><sub>(29)</sub> = 4.64; <italic>p</italic> &lt; 0.001, q(FDR) &lt; 0.05; cluster size = 165). Other regions included occipital cortex, fusiform gyrus, and middle temporal gyrus (<xref rid="T3" ref-type="table">Table 3</xref>). No significant difference in the connectivity of left or right dPMC or left vPMC between the two groups was observed indicating the specificity of hyperconnectivity of right vPMC to insula in misophonia.</p>
        <p>Finally, the misophonia group also showed a significant increase in connectivity between left anterior insula and left motor and somatosensory (M1/S1) cortex (<xref ref-type="fig" rid="F1">Fig. 1</xref><italic>D</italic>; peak at: −18, −16, 78; <italic>t</italic><sub>(29)</sub> = 4.82; <italic>p</italic> &lt; 0.001, q(FDR) &lt; 0.05; cluster size = 383) and cerebellum (<xref rid="T3" ref-type="table">Table 3</xref>). This part of the cerebellum (lobule 6) is known to be involved in mirroring and observation of bodily actions of others (<xref rid="B53" ref-type="bibr">Van Overwalle et al., 2014</xref>; <xref rid="B1" ref-type="bibr">Abdelgabar et al., 2019</xref>). Stronger coupling of vPMC containing orofacial motor cortex and cerebellum to anterior insula supports “mirroring” as a likely mechanism for the autonomic and emotional reaction in misophonia mediated by the anterior insula.</p>
      </sec>
      <sec>
        <title>Functional connectivity of orofacial motor area in response to sounds</title>
        <p>We next examined sound-driven changes in functional connectivity. This was done using fMRI data acquired on a separate misophonia and control cohort during the presentation with three categories of sounds: trigger sounds (which evoke misophonic reaction in the misophonia group), unpleasant sounds (which are perceived as aversive by both the groups but do not evoke misophonic reaction in the misophonia group), and neutral sounds (for rating data, see <xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>, their Fig. 1). We first defined an ROI in the orofacial area showing stronger connectivity to PT in the resting state and then analyzed its connectivity to the rest of the brain in response to sounds. A significant main effect of group (Misophonia Group &gt; Controls) was observed in the auditory cortex covering right PT and right lateral HG (<xref ref-type="fig" rid="F2">Fig. 2</xref>, top; peak at: 46, −26, 16; <italic>p</italic> &lt; 0.001, q(FDR) &lt; 0.05; cluster size = 137), but no significant group × sound interaction was seen in the auditory cortex. Plots of connectivity values (<xref ref-type="fig" rid="F2">Fig. 2</xref>, bottom) in the auditory cortex confirm the main effect.</p>
        <fig id="F2" orientation="portrait" position="float">
          <label>Figure 2.</label>
          <caption>
            <p>Sound-evoked functional connectivity of right orofacial motor cortex. The orofacial motor ROI (top) is selected from the resting state connectivity analysis. The two middle rows show brain regions with increased connectivity in the misophonia group in response to all sounds. Bar plot in the bottom row plots connectivity strengths in the two groups in response to sounds. The results shown are cluster-corrected with a cluster defining threshold of <italic>p</italic> = 0.001. Data in the bar plots represent mean ± SEM.</p>
          </caption>
          <graphic xlink:href="SN-JNSJ210368F002"/>
        </fig>
      </sec>
      <sec>
        <title>Activation of the orofacial motor area to sounds</title>
        <p>The increased connectivity between audiovisual sensory and orofacial motor cortical areas in misophonia in the resting state and in the presence of auditory stimulation made us consider whether the orofacial motor area would show increased activation to trigger sounds in the misophonia group. With the ROI of the orofacial motor area as defined above, a significant group × sound category interaction was observed (<xref ref-type="fig" rid="F3">Fig. 3</xref><italic>Ai</italic>; <italic>p</italic> = 0.002 after small volume correction). Plot of β values (<xref ref-type="fig" rid="F3">Fig. 3</xref><italic>Aii</italic>) for individual sound categories shows that the interaction is driven by greater activity in misophonia subjects compared with controls for trigger sounds but not for unpleasant or neutral sounds. Importantly, this specific increase in activity for trigger sounds was not shown in auditory cortex (<xref ref-type="fig" rid="F3">Fig. 3</xref><italic>B</italic>), suggesting that a trigger sounds-specific increase in activity first arises in the orofacial motor cortex, rather than being carried forward from earlier auditory system hyperactivity.</p>
        <fig id="F3" orientation="portrait" position="float">
          <label>Figure 3.</label>
          <caption>
            <p>Activation of orofacial motor cortex and auditory cortex in response to three categories of sounds. <bold><italic>Ai</italic></bold>, The orofacial motor area represents a statistically significant (<italic>p</italic> = 0.002) group × sound category interaction. <bold><italic>Aii</italic></bold>, Bar plots represent β values for the orofacial motor cortex for the two groups in response to three sound categories. <bold><italic>Aiii</italic></bold>, Plots of variation of β values with the rating of misophonic distress in misophonia sufferers and of annoyance in control subjects. No group × sound category interaction is seen in the auditory cortex (<bold><italic>B</italic></bold>, middle column), which is confirmed by the bar plots of activation (<bold><italic>B</italic></bold>, first and third column) in response to sounds for the two groups. <bold><italic>B</italic></bold>, Middle column: white represents HG; black represents PT. Data in the bar plots represent mean ± SEM.</p>
          </caption>
          <graphic xlink:href="SN-JNSJ210368F003"/>
        </fig>
        <p>To further confirm the relation between behavioral data and BOLD activity in the orofacial motor cortex, we measured activity (β values) for each sound individually (without reference to the group to which it belongs) and determined how the activity of orofacial motor cortex relates to the rating given to the sound by the subjects. <xref ref-type="fig" rid="F3">Figure 3</xref><italic>Aiii</italic> plots the variation of β values with misophonic distress rating for misophonia sufferers and annoyance ratings for control subjects. As can be seen, the activity of orofacial motor area increases in proportion to the rating. However, this increase is stronger (<italic>t</italic><sub>(40)</sub> = 6.8, <italic>p</italic> &lt; 0.001) for misophonia sufferers compared with controls.</p>
      </sec>
    </sec>
    <sec sec-type="discussion">
      <title>Discussion</title>
      <p>In this study, we have demonstrated that misophonia is characterized by (1) increased resting state functional connectivity between the orofacial motor area and both auditory and visual cortex; (2) increased functional connectivity between auditory cortex and orofacial motor areas in response to all types of sound; (3) increased activation of orofacial motor area in response to trigger sounds specifically; (4) activation of orofacial motor area increases in proportion to the misophonic distress; (5) no difference from the control group in the activation of auditory cortex to trigger and other sounds; and (6) increased resting state functional connectivity between vPMC containing orofacial motor area and insula in the resting state.</p>
      <p>Conventionally, misophonia is considered as a disorder of sound emotion processing, in which “simple” sounds of eating/chewing produced by others cause abnormally high negative emotional responses. However, sounds generated by others can represent the actions that produce them (<xref rid="B2" ref-type="bibr">Aglioti and Pazzaglia, 2010</xref>) and can also modulate the actions of listeners (<xref rid="B19" ref-type="bibr">Ferrari et al., 2005</xref>; <xref rid="B2" ref-type="bibr">Aglioti and Pazzaglia, 2010</xref>). We consider here whether sounds in misophonia may be only a “medium” via which action of the trigger-person is mirrored onto the listeners. In that case, the primary abnormality might be excessive engagement by the auditory system, and/or intrinsic hyper-responsiveness of, the orofacial motor system. The present findings of normal auditory cortex responses to trigger sounds, yet increased connectivity between auditory and orofacial motor areas and orofacial motor hyper-responsiveness specifically to trigger sounds, support this notion.</p>
      <p>Resting state connectivity measures spontaneous fluctuations of brain activity, which are correlated between two regions. It is known to be predictive of task/stimulus activations of brain regions (<xref rid="B51" ref-type="bibr">Tavor et al., 2016</xref>; <xref rid="B41" ref-type="bibr">Parker Jones et al., 2017</xref>) and behavior (<xref rid="B49" ref-type="bibr">Spisak et al., 2020</xref>). Our results show that spontaneous fluctuations in auditory/visual cortex and orofacial motor cortex are synchronized to a greater extent in a misophonia group compared with the control group. This stronger spontaneous coupling between auditory or visual and orofacial motor cortex in the absence of any systematic external stimulation implies that the orofacial motor cortex in misophonia sufferers is primed to respond to sensory stimulation related to the production of trigger sounds.</p>
      <p>Further evidence that input sounds in misophonia are transformed into motor representations comes from stronger coupling of orofacial motor areas to auditory cortex in response to auditory stimulation. However, this increased connectivity is in response to sounds in general and not specific to trigger sounds. Increased resting state connectivity between the auditory and motor cortex in misophonia is in the absence of any sound stimulation and, therefore, also not specific to trigger sounds. Since functional connectivity, particularly in the resting state, is known to be constrained and explained by structural connectivity (<xref rid="B21" ref-type="bibr">Greicius et al., 2009</xref>; <xref rid="B24" ref-type="bibr">Honey et al., 2009</xref>), it is likely that in misophonia there is a stronger structural connectivity between the auditory and orofacial motor cortices. This needs to be investigated in future studies.</p>
      <p>Does stronger coupling to auditory cortex in the resting state translate to stronger activation of orofacial motor cortex? Our findings suggest that, in misophonia, trigger sounds cause hyperactivation of the orofacial motor cortex implying possible excessive “mirroring” of the orofacial actions via trigger sounds. Although this mirroring of action via sensory stimuli occurs in normal and healthy subjects (<xref rid="B7" ref-type="bibr">Buccino et al., 2001</xref>), the mirroring is stronger (hyper-mirroring) and specific to trigger sounds in misophonia. Importantly, such specificity of hyperactivation to trigger sounds is absent in the auditory cortex, which further demonstrates that misophonia is a not a disorder of sound processing per se but relates to the (orofacial) actions that the sounds represent. Putting together results of resting state connectivity, sound-evoked functional connectivity, and activation, our data strongly support the hypothesis that misophonia is characterized by aberrant functioning of the mirror neuron system which “mirrors” the action of the trigger-person represented by sounds.</p>
      <p>A consequence of mirroring of action is that it leads to “automatic imitation” (<xref rid="B28" ref-type="bibr">Iacoboni et al., 1999</xref>; <xref rid="B23" ref-type="bibr">Heyes, 2011</xref>) in which subjects mimic the action of the producer. This imitation, which is unconscious and unintentional, facilitates actions and responses that are congruent with the actions of the producer (e.g., mouth movement in response to eating sounds of the trigger-person). What exactly makes excessive mirroring trigger the extreme negative characteristic reaction in misophonia is presently uncertain, but possibilities include a sense of loss of control, invasion of personal space, or interference with current goals and actions. A recent study, using a Stroop task, did show that misophonia sufferers are impaired in maintaining goals of the task, specifically in the presence of trigger sounds (<xref rid="B12" ref-type="bibr">Daniels et al., 2020</xref>). Since anger can be described as perceived goal interference (<xref rid="B25" ref-type="bibr">Hufendiek, 2016</xref>), the drive to imitate others would make anger (or aggression) the dominating emotional response. This is consistent with the phenomenological reports of emotional responses to trigger sounds in misophonia (<xref rid="B29" ref-type="bibr">Jager et al., 2020</xref>). Mimicry of the action producing the trigger sound may be a direct consequence of mirror system activation (i.e., overt as opposed to covert), or a coping strategy (<xref rid="B14" ref-type="bibr">Edelstein et al., 2013</xref>; <xref rid="B52" ref-type="bibr">Taylor, 2017</xref>) to dampen sensory activity, much like attenuation of sensory input following a self-generated action (<xref rid="B6" ref-type="bibr">Blakemore et al., 2000</xref>).</p>
      <p>Interestingly, outside of the context of misophonia, automatic mimicry of eating actions is common among family members (<xref rid="B22" ref-type="bibr">Hermans et al., 2012</xref>; <xref rid="B5" ref-type="bibr">Bell et al., 2019</xref>), which may explain why a family member most commonly acts as a source of triggers in the initial phase of misophonia. Although we have confined our discussions to sounds of eating/chewing as trigger, our model of hyper-mirroring in misophonia can be used to explain less-common visual triggers, such as foot/leg shaking because mimicking of these actions occurs in normal subjects (<xref rid="B11" ref-type="bibr">Chartrand and Bargh, 1999</xref>).</p>
      <p>Our previous work (<xref rid="B33" ref-type="bibr">Kumar et al., 2017</xref>) showed that misophonic distress correlates with the activity of anterior insula. The current results on resting state connectivity indicate stronger connectivity of orofacial motor cortex to insular cortex in misophonia, providing a link between mirroring of action, insular activity, and misophonic distress. Moreover, anterior insula has stronger resting state connectivity to cerebellum (lobule 6), which is known to be involved in social cognition in general and mirroring of actions of others in particular (<xref rid="B53" ref-type="bibr">Van Overwalle et al., 2014</xref>). Interestingly, neuroimaging of automatic imitation in normal subjects shows that countering of mirrored actions (e.g., closing one's hand when viewing a hand opening and vice versa), either intentionally or incidentally, engages anterior insula (<xref rid="B8" ref-type="bibr">Campbell et al., 2018</xref>). In summary, our data support that mirroring of action underlies the previously observed anterior insula-based network in misophonia.</p>
      <p>Misophonia as an aversive “reflex” has been argued previously by <xref rid="B13" ref-type="bibr">Dozier (2015)</xref>. In this model, sound triggers a reflex-like bodily (physical) response, which is then followed by emotional response. While our data are in agreement with the larger point of the model emphasizing role of motor system in misophonia, the fact that higher-order motor cortex is involved suggests a complex role of the motor system not consistent with the reflex-like response in the <xref rid="B13" ref-type="bibr">Dozier (2015)</xref> model. It is, however, likely that the mirroring response is automatically triggered from the vPMC and subsequently expands to other part of motor cortex, including primary motor cortex, which can explain sensations and muscle activity in other parts of the body in response to trigger sounds.</p>
      <p>Our data provide an alternative but complementary perspective on misophonia that emphasizes the action of the trigger-person rather than the sounds which are a byproduct of that action. In doing so, misophonia can be understood within a wider framework of social cognition, which concerns how people perceive auditory/visual cues from other people. This change in perspective has important consequences for devising therapies and treatment methods for misophonia. For instance, associative learning has been shown to configure the MNS (<xref rid="B9" ref-type="bibr">Catmur et al., 2007</xref>): if this process could be harnessed to associate misophonic trigger sounds with sound sources other than orofacial actions, then they might no longer evoke the misophonic reaction. Evidence for association having effects on perceived aversiveness of sounds comes from <xref rid="B46" ref-type="bibr">Samermit et al. (2019)</xref> in which aversive sounds were either associated with a positive or negative source. The same sound when associated with a positive source was rated as less unpleasant and also produced fewer bodily sensations compared with when association was with its original negative source. How this association is mediated by the MNS needs to be investigated further.</p>
      <p>Finally, we outline some limitations of our work and the future direction to overcome these limitations. First, demonstration of mirror neurons requires invasive single-neuron recording, which is not normally possible in humans. Measurements using fMRI are too coarse at spatial and temporal resolution to reveal the working of single neurons. Our evidence of involvement of mirror neurons in misophonia, therefore, like most of the human neuroimaging studies implicating mirror neurons, is indirect. There have been a few studies of single-cell recording demonstrating mirror neurons in human epileptic patients undergoing neurosurgery (<xref rid="B37" ref-type="bibr">Mukamel et al., 2010</xref>). Future studies may use such rare opportunities to provide direct evidence for the role of mirror neurons in misophonia. Second, although the distinctness of misophonia as a disorder on its own has been argued (<xref rid="B47" ref-type="bibr">Schroder et al., 2013</xref>; <xref rid="B17" ref-type="bibr">Erfanian et al., 2019</xref>; <xref rid="B50" ref-type="bibr">Swedo et al., 2021</xref>), there is still debate (<xref rid="B52" ref-type="bibr">Taylor, 2017</xref>) of how much of it can be explained by other disorders. In order to test its distinctness or overlap with respect to functioning of the mirror neuron system, future studies could include a clinical control group in addition to the normal healthy control group. Third, although less common compared with typical triggers, some misophonia sufferers also report nonhuman (e.g., animal sounds) and environmental sounds as their triggers (<xref rid="B29" ref-type="bibr">Jager et al., 2020</xref>). How does our hyper-mirroring model of misophonia explain distress to these triggers? It should be noted that nonhuman and environmental sounds as triggers in misophonia do not occur in isolation. That is, these nontypical triggers occur along with the typical triggers (e.g., eating/chewing sounds) in misophonia sufferers (<xref rid="B38" ref-type="bibr">Muller et al., 2018</xref>; <xref rid="B29" ref-type="bibr">Jager et al., 2020</xref>; <xref rid="B57" ref-type="bibr">Wiese et al., 2021</xref>). A plausible explanation, therefore, is that, after the typical triggers are learned via the mirror system, the nontypical stimuli become triggers via associative learning. This is also consistent with the observation that the set of trigger stimuli for misophonia sufferers expand over time. Further work, however, is needed to empirically validate this hypothesis.</p>
    </sec>
  </body>
  <back>
    <fn-group>
      <fn fn-type="supported-by">
        <p>S.K. was supported by Misophonia Research Fund, REAM Foundation; and Wellcome Trust Grant WT106964MA to T.D.G.</p>
      </fn>
      <fn fn-type="financial-disclosure">
        <p>The authors declare no competing financial interests.</p>
      </fn>
    </fn-group>
    <ref-list>
      <title>References</title>
      <ref id="B1">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Abdelgabar</surname>
<given-names>AR</given-names></string-name>, <string-name><surname>Suttrup</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Broersen</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Bhandari</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Picard</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Keysers</surname>
<given-names>C</given-names></string-name>, <string-name><surname>De Zeeuw</surname>
<given-names>CI</given-names></string-name>, <string-name><surname>Gazzola</surname>
<given-names>V</given-names></string-name></person-group> (<year>2019</year>) <article-title>Action perception recruits the cerebellum and is impaired in patients with spinocerebellar ataxia</article-title>. <source>Brain</source>
<volume>142</volume>:<fpage>3791</fpage>–<lpage>3805</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awz337</pub-id>
<pub-id pub-id-type="pmid">31747689</pub-id></mixed-citation>
      </ref>
      <ref id="B2">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Aglioti</surname>
<given-names>SM</given-names></string-name>, <string-name><surname>Pazzaglia</surname>
<given-names>M</given-names></string-name></person-group> (<year>2010</year>) <article-title>Representing actions through their sound</article-title>. <source>Exp Brain Res</source>
<volume>206</volume>:<fpage>141</fpage>–<lpage>151</lpage>. <pub-id pub-id-type="doi">10.1007/s00221-010-2344-x</pub-id><pub-id pub-id-type="pmid">20602092</pub-id></mixed-citation>
      </ref>
      <ref id="B3">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Bastiaansen</surname>
<given-names>JA</given-names></string-name>, <string-name><surname>Thioux</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Keysers</surname>
<given-names>C</given-names></string-name></person-group> (<year>2009</year>) <article-title>Evidence for mirror systems in emotions</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>
<volume>364</volume>:<fpage>2391</fpage>–<lpage>2404</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2009.0058</pub-id>
<pub-id pub-id-type="pmid">19620110</pub-id></mixed-citation>
      </ref>
      <ref id="B4">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Behzadi</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Restom</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Liau</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>TT</given-names></string-name></person-group> (<year>2007</year>) <article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title>. <source>Neuroimage</source>
<volume>37</volume>:<fpage>90</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></mixed-citation>
      </ref>
      <ref id="B5">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Bell</surname>
<given-names>BM</given-names></string-name>, <string-name><surname>Spruijt-Metz</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Vega Yon</surname>
<given-names>GG</given-names></string-name>, <string-name><surname>Mondol</surname>
<given-names>AS</given-names></string-name>, <string-name><surname>Alam</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ma</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Emi</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Lach</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Stankovic</surname>
<given-names>JA</given-names></string-name>, <string-name><surname>De la Haye</surname>
<given-names>K</given-names></string-name></person-group> (<year>2019</year>) <article-title>Sensing eating mimicry among family members</article-title>. <source>Transl Behav Med</source>
<volume>9</volume>:<fpage>422</fpage>–<lpage>430</lpage>. <pub-id pub-id-type="doi">10.1093/tbm/ibz051</pub-id><pub-id pub-id-type="pmid">31094447</pub-id></mixed-citation>
      </ref>
      <ref id="B6">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Blakemore</surname>
<given-names>SJ</given-names></string-name>, <string-name><surname>Wolpert</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Frith</surname>
<given-names>C</given-names></string-name></person-group> (<year>2000</year>) <article-title>Why can't you tickle yourself?</article-title>
<source>Neuroreport</source>
<volume>11</volume>:<fpage>R11</fpage>–<lpage>R16</lpage>.<pub-id pub-id-type="pmid">10943682</pub-id></mixed-citation>
      </ref>
      <ref id="B7">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Buccino</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Binkofski</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Fink</surname>
<given-names>GR</given-names></string-name>, <string-name><surname>Fadiga</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Fogassi</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Gallese</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Seitz</surname>
<given-names>RJ</given-names></string-name>, <string-name><surname>Zilles</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Freund</surname>
<given-names>HJ</given-names></string-name></person-group> (<year>2001</year>) <article-title>Action observation activates premotor and parietal areas in a somatotopic manner: an fMRI study</article-title>. <source>Eur J Neurosci</source>
<volume>13</volume>:<fpage>400</fpage>–<lpage>404</lpage>. <pub-id pub-id-type="doi">10.1111/j.1460-9568.2001.01385.x</pub-id><pub-id pub-id-type="pmid">11168545</pub-id></mixed-citation>
      </ref>
      <ref id="B8">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Campbell</surname>
<given-names>ME</given-names></string-name>, <string-name><surname>Mehrkanoon</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Cunnington</surname>
<given-names>R</given-names></string-name></person-group> (<year>2018</year>) <article-title>Intentionally not imitating: insula cortex engaged for top-down control of action mirroring</article-title>. <source>Neuropsychologia</source>
<volume>111</volume>:<fpage>241</fpage>–<lpage>251</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.01.037</pub-id>
<pub-id pub-id-type="pmid">29408525</pub-id></mixed-citation>
      </ref>
      <ref id="B9">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Catmur</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Walsh</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Heyes</surname>
<given-names>C</given-names></string-name></person-group> (<year>2007</year>) <article-title>Sensorimotor learning configures the human mirror system</article-title>. <source>Curr Biol</source>
<volume>17</volume>:<fpage>1527</fpage>–<lpage>1531</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2007.08.006</pub-id>
<pub-id pub-id-type="pmid">17716898</pub-id></mixed-citation>
      </ref>
      <ref id="B10">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Cavanna</surname>
<given-names>AE</given-names></string-name>, <string-name><surname>Seri</surname>
<given-names>S</given-names></string-name></person-group> (<year>2015</year>) <article-title>Misophonia: current perspectives</article-title>. <source>Neuropsychiatr Dis Treat</source>
<volume>11</volume>:<fpage>2117</fpage>–<lpage>2123</lpage>. <pub-id pub-id-type="doi">10.2147/NDT.S81438</pub-id>
<pub-id pub-id-type="pmid">26316758</pub-id></mixed-citation>
      </ref>
      <ref id="B11">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Chartrand</surname>
<given-names>TL</given-names></string-name>, <string-name><surname>Bargh</surname>
<given-names>JA</given-names></string-name></person-group> (<year>1999</year>) <article-title>The chameleon effect: the perception-behavior link and social interaction</article-title>. <source>J Pers Soc Psychol</source>
<volume>76</volume>:<fpage>893</fpage>–<lpage>910</lpage>. <pub-id pub-id-type="doi">10.1037//0022-3514.76.6.893</pub-id>
<pub-id pub-id-type="pmid">10402679</pub-id></mixed-citation>
      </ref>
      <ref id="B12">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Daniels</surname>
<given-names>EC</given-names></string-name>, <string-name><surname>Rodriguez</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Zabelina</surname>
<given-names>DL</given-names></string-name></person-group> (<year>2020</year>) <article-title>Severity of misophonia symptoms is associated with worse cognitive control when exposed to misophonia trigger sounds</article-title>. <source>PLoS One</source>
<volume>15</volume>:<fpage>e0227118</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0227118</pub-id>
<pub-id pub-id-type="pmid">31945068</pub-id></mixed-citation>
      </ref>
      <ref id="B13">
        <mixed-citation publication-type="book">
<person-group person-group-type="author"><string-name><surname>Dozier</surname>
<given-names>T</given-names></string-name></person-group> (<year>2015</year>) <article-title>Understanding and overcoming misophonia: a conditioned aversive reflex disorder</article-title>. <publisher-loc>Livermore, CA</publisher-loc>: <publisher-name>Misophonia Treatment Institute</publisher-name>.</mixed-citation>
      </ref>
      <ref id="B14">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Edelstein</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Brang</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Rouw</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ramachandran</surname>
<given-names>VS</given-names></string-name></person-group> (<year>2013</year>) <article-title>Misophonia: physiological investigations and case descriptions</article-title>. <source>Front Hum Neurosci</source>
<volume>7</volume>:<fpage>296</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2013.00296</pub-id>
<pub-id pub-id-type="pmid">23805089</pub-id></mixed-citation>
      </ref>
      <ref id="B15">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Ehrsson</surname>
<given-names>HH</given-names></string-name>, <string-name><surname>Geyer</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Naito</surname>
<given-names>E</given-names></string-name></person-group> (<year>2003</year>) <article-title>Imagery of voluntary movement of fingers, toes, and tongue activates corresponding body-part–specific motor representations</article-title>. <source>J Neurophysiol</source>
<volume>90</volume>:<fpage>3304</fpage>–<lpage>3316</lpage>. <pub-id pub-id-type="doi">10.1152/jn.01113.2002</pub-id>
<pub-id pub-id-type="pmid">14615433</pub-id></mixed-citation>
      </ref>
      <ref id="B16">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Eickhoff</surname>
<given-names>SB</given-names></string-name>, <string-name><surname>Stephan</surname>
<given-names>KE</given-names></string-name>, <string-name><surname>Mohlberg</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Grefkes</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Fink</surname>
<given-names>GR</given-names></string-name>, <string-name><surname>Amunts</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zilles</surname>
<given-names>K</given-names></string-name></person-group> (<year>2005</year>) <article-title>A new SPM toolbox for combining probabilistic cytoarchitectonic maps and functional imaging data</article-title>. <source>Neuroimage</source>
<volume>25</volume>:<fpage>1325</fpage>–<lpage>1335</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.034</pub-id>
<pub-id pub-id-type="pmid">15850749</pub-id></mixed-citation>
      </ref>
      <ref id="B17">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Erfanian</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Kartsonaki</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Keshavarz</surname>
<given-names>A</given-names></string-name></person-group> (<year>2019</year>) <article-title>Misophonia and comorbid psychiatric symptoms: a preliminary study of clinical findings</article-title>. <source>Nord J Psychiatry</source>
<volume>73</volume>:<fpage>219</fpage>–<lpage>228</lpage>. <pub-id pub-id-type="doi">10.1080/08039488.2019.1609086</pub-id>
<pub-id pub-id-type="pmid">31066600</pub-id></mixed-citation>
      </ref>
      <ref id="B18">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Ferrari</surname>
<given-names>PF</given-names></string-name>, <string-name><surname>Gallese</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Fogassi</surname>
<given-names>L</given-names></string-name></person-group> (<year>2003</year>) <article-title>Mirror neurons responding to the observation of ingestive and communicative mouth actions in the monkey ventral premotor cortex</article-title>. <source>Eur J Neurosci</source>
<volume>17</volume>:<fpage>1703</fpage>–<lpage>1714</lpage>. <pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02601.x</pub-id>
<pub-id pub-id-type="pmid">12752388</pub-id></mixed-citation>
      </ref>
      <ref id="B19">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Ferrari</surname>
<given-names>PF</given-names></string-name>, <string-name><surname>Maiolini</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Addessi</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Fogassi</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Visalberghi</surname>
<given-names>E</given-names></string-name></person-group> (<year>2005</year>) <article-title>The observation and hearing of eating actions activates motor programs related to eating in macaque monkeys</article-title>. <source>Behav Brain Res</source>
<volume>161</volume>:<fpage>95</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/j.bbr.2005.01.009</pub-id>
<pub-id pub-id-type="pmid">15904715</pub-id></mixed-citation>
      </ref>
      <ref id="B20">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Grabski</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Lamalle</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Vilain</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Schwartz</surname>
<given-names>JL</given-names></string-name>, <string-name><surname>Vallee</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Tropres</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Baciu</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Le Bas</surname>
<given-names>JF</given-names></string-name>, <string-name><surname>Sato</surname>
<given-names>M</given-names></string-name></person-group> (<year>2012</year>) <article-title>Functional MRI assessment of orofacial articulators: neural correlates of lip, jaw, larynx, and tongue movements</article-title>. <source>Hum Brain Mapp</source>
<volume>33</volume>:<fpage>2306</fpage>–<lpage>2321</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21363</pub-id>
<pub-id pub-id-type="pmid">21826760</pub-id></mixed-citation>
      </ref>
      <ref id="B21">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Greicius</surname>
<given-names>MD</given-names></string-name>, <string-name><surname>Supekar</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Menon</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Dougherty</surname>
<given-names>RF</given-names></string-name></person-group> (<year>2009</year>) <article-title>Resting-state functional connectivity reflects structural connectivity in the default mode network</article-title>. <source>Cereb Cortex</source>
<volume>19</volume>:<fpage>72</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhn059</pub-id>
<pub-id pub-id-type="pmid">18403396</pub-id></mixed-citation>
      </ref>
      <ref id="B22">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Hermans</surname>
<given-names>RC</given-names></string-name>, <string-name><surname>Lichtwarck-Aschoff</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Bevelander</surname>
<given-names>KE</given-names></string-name>, <string-name><surname>Herman</surname>
<given-names>CP</given-names></string-name>, <string-name><surname>Larsen</surname>
<given-names>JK</given-names></string-name>, <string-name><surname>Engels</surname>
<given-names>RC</given-names></string-name></person-group> (<year>2012</year>) <article-title>Mimicry of food intake: the dynamic interplay between eating companions</article-title>. <source>PLoS One</source>
<volume>7</volume>:<fpage>e31027</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0031027</pub-id>
<pub-id pub-id-type="pmid">22312438</pub-id></mixed-citation>
      </ref>
      <ref id="B23">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Heyes</surname>
<given-names>C</given-names></string-name></person-group> (<year>2011</year>) <article-title>Automatic imitation</article-title>. <source>Psychol Bull</source>
<volume>137</volume>:<fpage>463</fpage>–<lpage>483</lpage>. <pub-id pub-id-type="doi">10.1037/a0022288</pub-id>
<pub-id pub-id-type="pmid">21280938</pub-id></mixed-citation>
      </ref>
      <ref id="B24">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Honey</surname>
<given-names>CJ</given-names></string-name>, <string-name><surname>Sporns</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Cammoun</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Gigandet</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Thiran</surname>
<given-names>JP</given-names></string-name>, <string-name><surname>Meuli</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Hagmann</surname>
<given-names>P</given-names></string-name></person-group> (<year>2009</year>) <article-title>Predicting human resting-state functional connectivity from structural connectivity</article-title>. <source>Proc Natl Acad Sci USA</source>
<volume>106</volume>:<fpage>2035</fpage>–<lpage>2040</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0811168106</pub-id><pub-id pub-id-type="pmid">19188601</pub-id></mixed-citation>
      </ref>
      <ref id="B25">
        <mixed-citation publication-type="book">
<person-group person-group-type="author"><string-name><surname>Hufendiek</surname>
<given-names>R</given-names></string-name></person-group> (<year>2016</year>) <source>Embodied emotions: a naturalist approach to a normative phenomenon</source>. <publisher-loc>Milton Park, UK</publisher-loc>: <publisher-name>Taylor and Francis</publisher-name>.</mixed-citation>
      </ref>
      <ref id="B26">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Hutton</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Josephs</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Stadler</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Featherstone</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Reid</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Speck</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Bernarding</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Weiskopf</surname>
<given-names>N</given-names></string-name></person-group> (<year>2011</year>) <article-title>The impact of physiological noise correction on fMRI at 7 T</article-title>. <source>Neuroimage</source>
<volume>57</volume>:<fpage>101</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.04.018</pub-id>
<pub-id pub-id-type="pmid">21515386</pub-id></mixed-citation>
      </ref>
      <ref id="B27">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Iacoboni</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Dapretto</surname>
<given-names>M</given-names></string-name></person-group> (<year>2006</year>) <article-title>The mirror neuron system and the consequences of its dysfunction</article-title>. <source>Nat Rev Neurosci</source>
<volume>7</volume>:<fpage>942</fpage>–<lpage>951</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2024</pub-id>
<pub-id pub-id-type="pmid">17115076</pub-id></mixed-citation>
      </ref>
      <ref id="B28">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Iacoboni</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Woods</surname>
<given-names>RP</given-names></string-name>, <string-name><surname>Brass</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Bekkering</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Mazziotta</surname>
<given-names>JC</given-names></string-name>, <string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name></person-group> (<year>1999</year>) <article-title>Cortical mechanisms of human imitation</article-title>. <source>Science</source>
<volume>286</volume>:<fpage>2526</fpage>–<lpage>2528</lpage>. <pub-id pub-id-type="doi">10.1126/science.286.5449.2526</pub-id><pub-id pub-id-type="pmid">10617472</pub-id></mixed-citation>
      </ref>
      <ref id="B29">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Jager</surname>
<given-names>I</given-names></string-name>, <string-name><surname>de Koning</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Bost</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Denys</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Vulink</surname>
<given-names>N</given-names></string-name></person-group> (<year>2020</year>) <article-title>Misophonia: phenomenology, comorbidity and demographics in a large sample</article-title>. <source>PLoS One</source>
<volume>15</volume>:<fpage>e0231390</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0231390</pub-id><pub-id pub-id-type="pmid">32294104</pub-id></mixed-citation>
      </ref>
      <ref id="B30">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Kern</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Bert</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Glanz</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Schulze-Bonhage</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Ball</surname>
<given-names>T</given-names></string-name></person-group> (<year>2019</year>) <article-title>Human motor cortex relies on sparse and action-specific activation during laughing, smiling and speech production</article-title>. <source>Commun Biol</source>
<volume>2</volume>:<fpage>118</fpage>. <pub-id pub-id-type="doi">10.1038/s42003-019-0360-3</pub-id>
<pub-id pub-id-type="pmid">30937400</pub-id></mixed-citation>
      </ref>
      <ref id="B31">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Kohler</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Keysers</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Umilta</surname>
<given-names>MA</given-names></string-name>, <string-name><surname>Fogassi</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Gallese</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name></person-group> (<year>2002</year>) <article-title>Hearing sounds, understanding actions: action representation in mirror neurons</article-title>. <source>Science</source>
<volume>297</volume>:<fpage>846</fpage>–<lpage>848</lpage>. <pub-id pub-id-type="doi">10.1126/science.1070311</pub-id>
<pub-id pub-id-type="pmid">12161656</pub-id></mixed-citation>
      </ref>
      <ref id="B32">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Kumar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Hancock</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Cope</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Sedley</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Winston</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Griffiths</surname>
<given-names>T</given-names></string-name></person-group> (<year>2014</year>) <article-title>Misophonia: a disorder of emotion processing of sounds</article-title>. <source>J Neurol Neurosurg Psychiatry</source>
<volume>85</volume>:<fpage>15</fpage>.</mixed-citation>
      </ref>
      <ref id="B33">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Kumar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Tansley-Hancock</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Sedley</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Winston</surname>
<given-names>JS</given-names></string-name>, <string-name><surname>Callaghan</surname>
<given-names>MF</given-names></string-name>, <string-name><surname>Allen</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Cope</surname>
<given-names>TE</given-names></string-name>, <string-name><surname>Gander</surname>
<given-names>PE</given-names></string-name>, <string-name><surname>Bamiou</surname>
<given-names>DE</given-names></string-name>, <string-name><surname>Griffiths</surname>
<given-names>TD</given-names></string-name></person-group> (<year>2017</year>) <article-title>The brain basis for misophonia</article-title>. <source>Curr Biol</source>
<volume>27</volume>:<fpage>527</fpage>–<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2016.12.048</pub-id>
<pub-id pub-id-type="pmid">28162895</pub-id></mixed-citation>
      </ref>
      <ref id="B34">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Lutti</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Hutton</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Finsterbusch</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Helms</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Weiskopf</surname>
<given-names>N</given-names></string-name></person-group> (<year>2010</year>) <article-title>Optimization and validation of methods for mapping of the radiofrequency transmit field at 3T</article-title>. <source>Magn Reson Med</source>
<volume>64</volume>:<fpage>229</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.22421</pub-id>
<pub-id pub-id-type="pmid">20572153</pub-id></mixed-citation>
      </ref>
      <ref id="B35">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Lutti</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Stadler</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Josephs</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Windischberger</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Speck</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Bernarding</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Hutton</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Weiskopf</surname>
<given-names>N</given-names></string-name></person-group> (<year>2012</year>) <article-title>Robust and fast whole brain mapping of the RF transmit field B1 at 7T</article-title>. <source>PLoS One</source>
<volume>7</volume>:<fpage>e32379</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0032379</pub-id>
<pub-id pub-id-type="pmid">22427831</pub-id></mixed-citation>
      </ref>
      <ref id="B36">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Mayka</surname>
<given-names>MA</given-names></string-name>, <string-name><surname>Corcos</surname>
<given-names>DM</given-names></string-name>, <string-name><surname>Leurgans</surname>
<given-names>SE</given-names></string-name>, <string-name><surname>Vaillancourt</surname>
<given-names>DE</given-names></string-name></person-group> (<year>2006</year>) <article-title>Three-dimensional locations and boundaries of motor and premotor cortices as defined by functional brain imaging: a meta-analysis</article-title>. <source>Neuroimage</source>
<volume>31</volume>:<fpage>1453</fpage>–<lpage>1474</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.02.004</pub-id>
<pub-id pub-id-type="pmid">16571375</pub-id></mixed-citation>
      </ref>
      <ref id="B37">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Mukamel</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ekstrom</surname>
<given-names>AD</given-names></string-name>, <string-name><surname>Kaplan</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Iacoboni</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Fried</surname>
<given-names>I</given-names></string-name></person-group> (<year>2010</year>) <article-title>Single-neuron responses in humans during execution and observation of actions</article-title>. <source>Curr Biol</source>
<volume>20</volume>:<fpage>750</fpage>–<lpage>756</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2010.02.045</pub-id>
<pub-id pub-id-type="pmid">20381353</pub-id></mixed-citation>
      </ref>
      <ref id="B38">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Muller</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Khemlani-Patel</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Neziroglu</surname>
<given-names>F</given-names></string-name></person-group> (<year>2018</year>) <article-title>Cognitive-behavioral therapy for an adolescent female presenting with misophonia: a case example</article-title>. <source>Clin Case Stud</source>
<volume>17</volume>:<fpage>249</fpage>–<lpage>258</lpage>. <pub-id pub-id-type="doi">10.1177/1534650118782650</pub-id></mixed-citation>
      </ref>
      <ref id="B39">
        <mixed-citation publication-type="book">
<person-group person-group-type="author"><string-name><surname>Nauman</surname>
<given-names>Z</given-names></string-name></person-group> (<year>2017</year>) <article-title>HEARING RED: Woman took her own life after 'suffering from rare condition called sound rage' which triggers anger attacks over noise of eating and breathing</article-title>. <publisher-loc>London</publisher-loc>: <publisher-name>The Sun</publisher-name>.</mixed-citation>
      </ref>
      <ref id="B40">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Naylor</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Caimino</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Scutt</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Hoare</surname>
<given-names>DJ</given-names></string-name>, <string-name><surname>Baguley</surname>
<given-names>DM</given-names></string-name></person-group> (<year>2021</year>) <article-title>The prevalence and severity of misophonia in a UK undergraduate medical student population and validation of the Amsterdam Misophonia Scale</article-title>. <source>Psychiatr Q</source>
<volume>92</volume>:<fpage>609</fpage>–<lpage>619</lpage>. <pub-id pub-id-type="doi">10.1007/s11126-020-09825-3</pub-id><pub-id pub-id-type="pmid">32829440</pub-id></mixed-citation>
      </ref>
      <ref id="B41">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Parker Jones</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Voets</surname>
<given-names>NL</given-names></string-name>, <string-name><surname>Adcock</surname>
<given-names>JE</given-names></string-name>, <string-name><surname>Stacey</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Jbabdi</surname>
<given-names>S</given-names></string-name></person-group> (<year>2017</year>) <article-title>Resting connectivity predicts task activation in pre-surgical populations</article-title>. <source>Neuroimage Clin</source>
<volume>13</volume>:<fpage>378</fpage>–<lpage>385</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2016.12.028</pub-id>
<pub-id pub-id-type="pmid">28123949</pub-id></mixed-citation>
      </ref>
      <ref id="B42">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Pellegrino</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Fadiga</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Fogassi</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Gallese</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name></person-group> (<year>1992</year>) <article-title>Understanding motor events: a neurophysiological study</article-title>. <source>Exp Brain Res</source>
<volume>91</volume>:<fpage>176</fpage>–<lpage>180</lpage>. <pub-id pub-id-type="doi">10.1007/BF00230027</pub-id>
<pub-id pub-id-type="pmid">1301372</pub-id></mixed-citation>
      </ref>
      <ref id="B43">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Prochazkova</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Kret</surname>
<given-names>ME</given-names></string-name></person-group> (<year>2017</year>) <article-title>Connecting minds and sharing emotions through mimicry: a neurocognitive model of emotional contagion</article-title>. <source>Neurosci Biobehav Rev</source>
<volume>80</volume>:<fpage>99</fpage>–<lpage>114</lpage>. <pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.05.013</pub-id>
<pub-id pub-id-type="pmid">28506927</pub-id></mixed-citation>
      </ref>
      <ref id="B44">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Craighero</surname>
<given-names>L</given-names></string-name></person-group> (<year>2004</year>) <article-title>The mirror-neuron system</article-title>. <source>Annu Rev Neurosci</source>
<volume>27</volume>:<fpage>169</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144230</pub-id>
<pub-id pub-id-type="pmid">15217330</pub-id></mixed-citation>
      </ref>
      <ref id="B45">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Rizzolatti</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Sinigaglia</surname>
<given-names>C</given-names></string-name></person-group> (<year>2010</year>) <article-title>The functional role of the parieto-frontal mirror circuit: interpretations and misinterpretations</article-title>. <source>Nat Rev Neurosci</source>
<volume>11</volume>:<fpage>264</fpage>–<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1038/nrn2805</pub-id>
<pub-id pub-id-type="pmid">20216547</pub-id></mixed-citation>
      </ref>
      <ref id="B46">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Samermit</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Saal</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Davidenko</surname>
<given-names>N</given-names></string-name></person-group> (<year>2019</year>) <article-title>Cross-sensory stimuli modulate reactions to aversive sounds</article-title>. <source>Multisens Res</source>
<volume>32</volume>:<fpage>197</fpage>–<lpage>213</lpage>. <pub-id pub-id-type="doi">10.1163/22134808-20191344</pub-id>
<pub-id pub-id-type="pmid">31059490</pub-id></mixed-citation>
      </ref>
      <ref id="B47">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Schroder</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Vulink</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Denys</surname>
<given-names>D</given-names></string-name></person-group> (<year>2013</year>) <article-title>Misophonia: diagnostic criteria for a new psychiatric disorder</article-title>. <source>PLoS One</source>
<volume>8</volume>:<fpage>e54706</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0054706</pub-id><pub-id pub-id-type="pmid">23372758</pub-id></mixed-citation>
      </ref>
      <ref id="B48">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Schroder</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Wingen</surname>
<given-names>GV</given-names></string-name>, <string-name><surname>Eijsker</surname>
<given-names>N</given-names></string-name>, <string-name><surname>San Giorgi</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Vulink</surname>
<given-names>NC</given-names></string-name>, <string-name><surname>Turbyne</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Denys</surname>
<given-names>D</given-names></string-name></person-group> (<year>2019</year>) <article-title>Misophonia is associated with altered brain activity in the auditory cortex and salience network</article-title>. <source>Sci Rep</source>
<volume>9</volume>:<fpage>7542</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-44084-8</pub-id>
<pub-id pub-id-type="pmid">31101901</pub-id></mixed-citation>
      </ref>
      <ref id="B49">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Spisak</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Kincses</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Schlitt</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Zunhammer</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Schmidt-Wilcke</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Kincses</surname>
<given-names>ZT</given-names></string-name>, <string-name><surname>Bingel</surname>
<given-names>U</given-names></string-name></person-group> (<year>2020</year>) <article-title>Pain-free resting-state functional brain connectivity predicts individual pain sensitivity</article-title>. <source>Nat Commun</source>
<volume>11</volume>:<fpage>187</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-13785-z</pub-id>
<pub-id pub-id-type="pmid">31924769</pub-id></mixed-citation>
      </ref>
      <ref id="B50">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Swedo</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Baguley</surname>
<given-names>DM</given-names></string-name>, <string-name><surname>Denys</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Dixon</surname>
<given-names>LJ</given-names></string-name>, <string-name><surname>Erfanian</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Fioretti</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Jastreboff</surname>
<given-names>PJ</given-names></string-name>, <string-name><surname>Kumar</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Rosenthal</surname>
<given-names>MZ</given-names></string-name>, <string-name><surname>Rouw</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Schiller</surname>
<given-names>D</given-names></string-name>, <string-name><surname>Simner</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Storch</surname>
<given-names>EA</given-names></string-name>, <string-name><surname>Taylor</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Werff</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Raver</surname>
<given-names>SM</given-names></string-name></person-group> (<year>2021</year>) <article-title>A consensus definition of misophonia: using a delphi process to reach expert agreement</article-title>. <source>medRxiv</source>
<fpage>21254951</fpage>.</mixed-citation>
      </ref>
      <ref id="B51">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Tavor</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Parker Jones</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Mars</surname>
<given-names>RB</given-names></string-name>, <string-name><surname>Smith</surname>
<given-names>SM</given-names></string-name>, <string-name><surname>Behrens</surname>
<given-names>TE</given-names></string-name>, <string-name><surname>Jbabdi</surname>
<given-names>S</given-names></string-name></person-group> (<year>2016</year>) <article-title>Task-free MRI predicts individual differences in brain activity during task performance</article-title>. <source>Science</source>
<volume>352</volume>:<fpage>216</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1126/science.aad8127</pub-id>
<pub-id pub-id-type="pmid">27124457</pub-id></mixed-citation>
      </ref>
      <ref id="B52">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Taylor</surname>
<given-names>S</given-names></string-name></person-group> (<year>2017</year>) <article-title>Misophonia: a new mental disorder?</article-title>
<source>Med Hypotheses</source>
<volume>103</volume>:<fpage>109</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.mehy.2017.05.003</pub-id>
<pub-id pub-id-type="pmid">28571795</pub-id></mixed-citation>
      </ref>
      <ref id="B53">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Van Overwalle</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Baetens</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Marien</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Vandekerckhove</surname>
<given-names>M</given-names></string-name></person-group> (<year>2014</year>) <article-title>Social cognition and the cerebellum: a meta-analysis of over 350 fMRI studies</article-title>. <source>Neuroimage</source>
<volume>86</volume>:<fpage>554</fpage>–<lpage>572</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.09.033</pub-id>
<pub-id pub-id-type="pmid">24076206</pub-id></mixed-citation>
      </ref>
      <ref id="B54">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Weiskopf</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Hutton</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Josephs</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Deichmann</surname>
<given-names>R</given-names></string-name></person-group> (<year>2006</year>) <article-title>Optimal EPI parameters for reduction of susceptibility-induced BOLD sensitivity losses: a whole-brain analysis at 3 T and 1.5 T</article-title>. <source>Neuroimage</source>
<volume>33</volume>:<fpage>493</fpage>–<lpage>504</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.07.029</pub-id>
<pub-id pub-id-type="pmid">16959495</pub-id></mixed-citation>
      </ref>
      <ref id="B55">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Weiskopf</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Suckling</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Williams</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Correia</surname>
<given-names>MM</given-names></string-name>, <string-name><surname>Inkster</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Tait</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Ooi</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Bullmore</surname>
<given-names>ET</given-names></string-name>, <string-name><surname>Lutti</surname>
<given-names>A</given-names></string-name></person-group> (<year>2013</year>) <article-title>Quantitative multi-parameter mapping of R1, PD(*), MT, and R2(*) at 3T: a multi-center validation</article-title>. <source>Front Neurosci</source>
<volume>7</volume>:<fpage>95</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00095</pub-id><pub-id pub-id-type="pmid">23772204</pub-id></mixed-citation>
      </ref>
      <ref id="B56">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Whitfield-Gabrieli</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Nieto-Castanon</surname>
<given-names>A</given-names></string-name></person-group> (<year>2012</year>) <article-title>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</article-title>. <source>Brain Connect</source>
<volume>2</volume>:<fpage>125</fpage>–<lpage>141</lpage>. <pub-id pub-id-type="doi">10.1089/brain.2012.0073</pub-id>
<pub-id pub-id-type="pmid">22642651</pub-id></mixed-citation>
      </ref>
      <ref id="B57">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Wiese</surname>
<given-names>AD</given-names></string-name>, <string-name><surname>Wojcik</surname>
<given-names>KD</given-names></string-name>, <string-name><surname>Storch</surname>
<given-names>EA</given-names></string-name></person-group> (<year>2021</year>) <article-title>Assessment and intervention for individuals with misophonia</article-title>. <source>J Health Serv Psychol</source>
<volume>47</volume>:<fpage>51</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1007/s42843-021-00025-6</pub-id></mixed-citation>
      </ref>
      <ref id="B58">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Wu</surname>
<given-names>MS</given-names></string-name>, <string-name><surname>Lewin</surname>
<given-names>AB</given-names></string-name>, <string-name><surname>Murphy</surname>
<given-names>TK</given-names></string-name>, <string-name><surname>Storch</surname>
<given-names>EA</given-names></string-name></person-group> (<year>2014</year>) <article-title>Misophonia: incidence, phenomenology, and clinical correlates in an undergraduate student sample</article-title>. <source>J Clin Psychol</source>
<volume>70</volume>:<fpage>994</fpage>–<lpage>1007</lpage>. <pub-id pub-id-type="doi">10.1002/jclp.22098</pub-id>
<pub-id pub-id-type="pmid">24752915</pub-id></mixed-citation>
      </ref>
      <ref id="B59">
        <mixed-citation publication-type="journal">
<person-group person-group-type="author"><string-name><surname>Zhou</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Wu</surname>
<given-names>MS</given-names></string-name>, <string-name><surname>Storch</surname>
<given-names>EA</given-names></string-name></person-group> (<year>2017</year>) <article-title>Misophonia symptoms among Chinese university students: incidence, associated impairment, and clinical correlates</article-title>. <source>J Obsessive-Compulsive Relat Disord</source>
<volume>14</volume>:<fpage>7</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1016/j.jocrd.2017.05.001</pub-id></mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
