<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article">
  <?properties open_access?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
      <journal-id journal-id-type="iso-abbrev">Neuroimage</journal-id>
      <journal-title-group>
        <journal-title>Neuroimage</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1053-8119</issn>
      <issn pub-type="epub">1095-9572</issn>
      <publisher>
        <publisher-name>Academic Press</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">29746906</article-id>
      <article-id pub-id-type="pmc">6565932</article-id>
      <article-id pub-id-type="publisher-id">S1053-8119(18)30397-5</article-id>
      <article-id pub-id-type="doi">10.1016/j.neuroimage.2018.04.077</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Multi-subject hierarchical inverse covariance modelling improves estimation of functional brain networks</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Colclough</surname>
            <given-names>Giles L.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="aff3" ref-type="aff">c</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Woolrich</surname>
            <given-names>Mark W.</given-names>
          </name>
          <email>Mark.Woolrich@ohba.ox.ac.uk</email>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
          <xref rid="cor1" ref-type="corresp">∗</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Harrison</surname>
            <given-names>Samuel J.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">a</xref>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Rojas López</surname>
            <given-names>Pedro A.</given-names>
          </name>
          <xref rid="aff4" ref-type="aff">d</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Valdes-Sosa</surname>
            <given-names>Pedro A.</given-names>
          </name>
          <xref rid="aff4" ref-type="aff">d</xref>
          <xref rid="aff5" ref-type="aff">e</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>Stephen M.</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">b</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>Oxford Centre for Human Brain Activity (OHBA), Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford, Oxford, UK</aff>
      <aff id="aff2"><label>b</label>Oxford Centre for Functional MRI of the Brain (FMRIB), Wellcome Centre for Integrative Neuroimaging, Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK</aff>
      <aff id="aff3"><label>c</label>Centre for Doctoral Training in Healthcare Innovation, Institute of Biomedical Engineering Science, Department of Engineering, University of Oxford, Oxford, UK</aff>
      <aff id="aff4"><label>d</label>Neuroinformatics Department, El Centro de Neurociencias de Cuba (CNEURO), La Habana, Cuba</aff>
      <aff id="aff5"><label>e</label>The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, China</aff>
      <author-notes>
        <corresp id="cor1"><label>∗</label>Corresponding author. Oxford Centre for Functional MRI of the Brain (FMRIB), Wellcome Centre for Integrative Neuroimaging, Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK. <email>Mark.Woolrich@ohba.ox.ac.uk</email></corresp>
      </author-notes>
      <pub-date pub-type="pmc-release">
        <day>1</day>
        <month>9</month>
        <year>2018</year>
      </pub-date>
      <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.-->
      <pub-date pub-type="ppub">
        <month>9</month>
        <year>2018</year>
      </pub-date>
      <volume>178</volume>
      <fpage>370</fpage>
      <lpage>384</lpage>
      <history>
        <date date-type="received">
          <day>19</day>
          <month>1</month>
          <year>2017</year>
        </date>
        <date date-type="rev-recd">
          <day>28</day>
          <month>3</month>
          <year>2018</year>
        </date>
        <date date-type="accepted">
          <day>30</day>
          <month>4</month>
          <year>2018</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2018 The Authors</copyright-statement>
        <copyright-year>2018</copyright-year>
        <license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/">
          <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
        </license>
      </permissions>
      <abstract id="abs0010">
        <p>A Bayesian model for sparse, hierarchical, inver-covariance estimation is presented, and applied to multi-subject functional connectivity estimation in the human brain. It enables simultaneous inference of the strength of connectivity between brain regions at both subject and population level, and is applicable to fMRI, MEG and EEG data. Two versions of the model can encourage sparse connectivity, either using continuous priors to suppress irrelevant connections, or using an explicit description of the network structure to estimate the connection probability between each pair of regions. A large evaluation of this model, and thirteen methods that represent the state of the art of inverse covariance modelling, is conducted using both simulated and resting-state functional imaging datasets. Our novel Bayesian approach has similar performance to the best extant alternative, Ng et al.'s Sparse Group Gaussian Graphical Model algorithm, which also is based on a hierarchical structure. Using data from the Human Connectome Project, we show that these hierarchical models are able to reduce the measurement error in MEG beta-band functional networks by 10%, producing concomitant increases in estimates of the genetic influence on functional connectivity.</p>
      </abstract>
      <kwd-group id="kwrds0010">
        <title>Keywords</title>
        <kwd>fMRI</kwd>
        <kwd>MEG</kwd>
        <kwd>Functional connectivity</kwd>
        <kwd>Gaussian Graphical models</kwd>
        <kwd>Hierarchical Bayesian models</kwd>
        <kwd>Concentration graph</kwd>
        <kwd>Precision model</kwd>
        <kwd>Inverse covariance model</kwd>
        <kwd>MCMC</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="sec1">
      <title>Introduction</title>
      <p id="p0010">The estimation of functional connectivity in the human brain (<xref rid="bib29" ref-type="bibr">Friston, 2011</xref>; <xref rid="bib93" ref-type="bibr">Smith et al., 2013</xref>) is becoming a key tool forenhancing our understanding of disease and cognition as part of functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) studies. The most important and exciting uses of this type of analysis focus on individual differences in connectivity patterns. Subjects' functional connectomes are heritable (<xref rid="bib17" ref-type="bibr">Colclough et al., 2017</xref>; <xref rid="bib37" ref-type="bibr">Glahn et al., 2010</xref>); are associated with cognitive ability (<xref rid="bib25" ref-type="bibr">Finn et al., 2015</xref>), and with wealth, health and life satisfaction (<xref rid="bib94" ref-type="bibr">Smith et al., 2015</xref>); provide neuromarkers for sustained attention (<xref rid="bib86" ref-type="bibr">Rosenberg et al., 2016</xref>); are implicated with a range of diseases and disorders (<xref rid="bib41" ref-type="bibr">Greicius, 2008</xref>; <xref rid="bib95" ref-type="bibr">Stam, 2014</xref>); and predict task-evoked activity (<xref rid="bib99" ref-type="bibr">Tavor et al., 2016</xref>). For all of these forms of analysis, accurate estimation of single-subject functional networks is crucial.</p>
      <p id="p0015">Despite the recent explosion of research and high-quality findings, whole-brain functional connectivity estimation is relatively immature. Most of the key developments highlighted above use very simple Gaussian graphical models (GGMs) for the covariance of the data, in which the partial correlations between regions indicate the strengths of connections. We focus on this approach. However, accurate estimation of individual subjects' functional networks using GGMs can be difficult, particularly without long acquisition times. In an effort to improve the accuracy of network estimation, sparsity in the networks tends to be encouraged by suppressing weak connections (<xref rid="bib19" ref-type="bibr">Dempster, 1972</xref>; <xref rid="bib20" ref-type="bibr">Duff et al., 2013</xref>; <xref rid="bib93" ref-type="bibr">Smith et al., 2013</xref>; <xref rid="bib102" ref-type="bibr">Varoquaux and Craddock, 2013</xref>). Imposition of sparsity can also aid interpretation, by explicitly suggesting that certain individual functional connections are absent. There is even an entire field that attempts to characterise the function and dysfunction of cognitive networks using certain properties of this underlying graph structure (<xref rid="bib8" ref-type="bibr">Bullmore and Sporns, 2009</xref>; <xref rid="bib73" ref-type="bibr">de Pasquale et al., 2012</xref>, <xref rid="bib74" ref-type="bibr">2015</xref>; <xref rid="bib96" ref-type="bibr">Stam and van Straaten, 2012</xref>; <xref rid="bib98" ref-type="bibr">van Straaten and Stam, 2013</xref>).</p>
      <p id="p0020">While considerable work has been expended upon sparse network estimation for individual datasets (<xref rid="bib27" ref-type="bibr">Friedman et al., 2008</xref>; <xref rid="bib49" ref-type="bibr">Hinne et al., 2014</xref>, <xref rid="bib50" ref-type="bibr">2015</xref>; <xref rid="bib56" ref-type="bibr">Lenkoski, 2013</xref>; <xref rid="bib64" ref-type="bibr">Mazumder and Hastie, 2012a</xref>,<xref rid="bib65" ref-type="bibr">b</xref>; <xref rid="bib69" ref-type="bibr">Mohammadi and Wit, 2015</xref>; <xref rid="bib87" ref-type="bibr">Ryali et al., 2012</xref>; <xref rid="bib105" ref-type="bibr">Wang, 2012a</xref>,<xref rid="bib106" ref-type="bibr">b</xref>, <xref rid="bib107" ref-type="bibr">2015</xref>), relatively little effort has been made towards the joint inverse covariance estimation relevant for multi-subject, whole-brain network inference (Our most complete list is <xref rid="bib18" ref-type="bibr">Danaher et al., 2015</xref>; <xref rid="bib44" ref-type="bibr">Guo et al., 2011</xref>; <xref rid="bib46" ref-type="bibr">Harrison et al., 2015</xref>; <xref rid="bib55" ref-type="bibr">Lee and Liu, 2015</xref>; <xref rid="bib58" ref-type="bibr">Liang et al., 2016</xref>; <xref rid="bib63" ref-type="bibr">Marrelec et al., 2006</xref>; <xref rid="bib66" ref-type="bibr">Mejia et al., 2018</xref>; <xref rid="bib71" ref-type="bibr">Ng et al., 2013</xref>; <xref rid="bib78" ref-type="bibr">Peterson et al., 2015</xref>; <xref rid="bib80" ref-type="bibr">Qiu et al., 2015</xref>; <xref rid="bib103" ref-type="bibr">Varoquaux et al., 2010</xref>; and <xref rid="bib112" ref-type="bibr">Yang et al., 2015</xref>. Also of note is the work of <xref rid="bib70" ref-type="bibr">Nadkarni et al., 2017</xref>, who fit multiple Gaussian networks under auto-regressive processes to model MEG data, and Hinne et al., who in <xref rid="bib48" ref-type="bibr">2013</xref> developed a hierarchical connectivity model for structural brain networks inferred from diffusion MRI data.). Models with a ‘hierarchical’ structure, which simultaneously estimate the population connectivity and each individual's network strengths, should improve the quality of inference (<xref rid="bib34" ref-type="bibr">Gelman et al., 2014</xref>; <xref rid="bib109" ref-type="bibr">Woolrich, 2008</xref>). Some of the existing methods attempt a hierarchical model for the structure of the network, so that the probability of a connection existing in each subject is influenced by the group's connection map. When it comes to the connection <italic>strengths</italic>, only Ng et al. model the relationship between subject and group-level connectivities within their penalised maximum-likelihood approach. None combine sparse network priors, a hierarchical design that shares information on the strengths of connections over the whole dataset, and a computationally-efficient Bayesian inference framework that can be applied to large multi-subject neuroimaging datasets.</p>
      <p id="p0025">We present a new hierarchical model and scalable inference framework for sparse Bayesian modelling of multiple inverse covariance matrices. It is applied to the estimation of functional brain networks, with joint characterisation of subject-level and population-average connectivities. We model functional connectivity simply as undirected partial correlations between the network nodes—this model can be applied to MEG data (<xref rid="bib15" ref-type="bibr">Colclough et al., 2015</xref>) in addition to fMRI, and is among the most successful and repeatable of measures in either modality (<xref rid="bib16" ref-type="bibr">Colclough et al., 2016</xref>; <xref rid="bib82" ref-type="bibr">Ramsey et al., 2014</xref>; <xref rid="bib92" ref-type="bibr">Smith et al., 2011</xref>). We show that the posterior can be reformulated as a series of linked linear regressions, allowing a broad class of sparse priors to be applied to covariance modelling. Two particular priors are compared. The first imposes an explicit shared sparsity structure on the network graph, producing a posterior distribution over the edges present in the network. The second uses continuous priors to regularise the group connection strengths, more weakly encouraging network sparsity. A custom Markov chain Monte Carlo (MCMC) approach is used for inference, and we characterise how the computation time scales with model dimension and the number of subjects.</p>
      <p id="p0030">We run a large evaluation of the performance of our model and the current state of the art in GGM estimation. This evaluation uses simulated data to test models' ability to reconstruct connection strengths and sparse network patterns. We also use truncated segments of resting-state fMRI and MEG recordings from the Human Connectome Project (HCP) to assess inference quality with very short or noisy datasets. Finally, we use trait prediction analyses from the fMRI networks and genetic influence analyses on the MEG networks to demonstrate noise reductions when subject and population connectivities are estimated with a hierarchical framework.</p>
      <p id="p0035">We start with an overview of our new Bayesian model and inference approach.</p>
    </sec>
    <sec id="sec2">
      <title>A hierarchical model for inverse covariance matrices</title>
      <p id="p0040">In order to jointly estimate connectivity over many subjects, we need a scalable covariance inference framework that can be formulated as a hierarchical model. Most existing Bayesian models for GGMs use <italic>G</italic>-Wishart priors (<xref rid="bib57" ref-type="bibr">Letac and Massam, 2007</xref>). These are challenging to incorporate into a hierarchy because of the difficulty in computing the normalising constant of the distribution, itself a function of the underlying graph structure. Trans-dimensional MCMC approaches that avoid this computation have been developed for models of single covariance matrices (<xref rid="bib49" ref-type="bibr">Hinne et al., 2014</xref>, <xref rid="bib50" ref-type="bibr">2015</xref>; <xref rid="bib56" ref-type="bibr">Lenkoski, 2013</xref>; <xref rid="bib69" ref-type="bibr">Mohammadi and Wit, 2015</xref>; <xref rid="bib106" ref-type="bibr">Wang, 2012b</xref>), and an analytic expression for the troublesome normalising constant has been recently proposed (<xref rid="bib101" ref-type="bibr">Uhler et al., 2018</xref>), but building a sampler for multiple <italic>G</italic>-Wishart distributions with an inferred group prior and shared graph structure is not trivial.</p>
      <p id="p0045">Instead, we take a different approach, inspired by an alternative prior structure. Wang describes, in 2012<italic>a</italic> and 2015, two different priors that allow simple block-Gibbs sampling along the columns of matrices to draw from the posterior of two specific models for covariance. We build on this idea, by demonstrating that the conditional distribution of one column of a precision matrix takes the form of a linear regression, and that this reformulation gives access to most of the existing priors and inference engines from the Bayesian linear regression literature, enabling a range of hierarchical GGM models to be implemented.</p>
      <p id="p0050">Like most other Bayesian GGM or covariance models, we build sparse priors for the precision (or inverse covariance) matrix. Dempster argued in 1972 that introducing sparsity to the precision, rather than the covariance matrix, was the more desirable option, because this choice maximises the entropy of the resulting distribution. It also makes for a more interpretable approach, as promotion of sparsity in the precision or partial correlation matrix can be directly understood as promoting sparsity in the underlying GGM. The zeros in the partial correlation matrix directly indicate the lack of an edge in the network. Additionally, in our application of functional connectivity estimation, previous studies suggest that partial correlations derived from the precision matrix may be more robust network estimators than full correlations, particularly if there are sufficient data to make good estimates (<xref rid="bib20" ref-type="bibr">Duff et al., 2013</xref>; <xref rid="bib63" ref-type="bibr">Marrelec et al., 2006</xref>; <xref rid="bib92" ref-type="bibr">Smith et al., 2011</xref>). In his 2015 paper, Wang designed an additional prior that imposes sparsity in the covariance, not the precision matrix. Our extension of his prior for precisions could be easily adapted for sparse covariance matrices if desired.</p>
      <p id="p0055">The models we propose have three principal features. First, the connection strengths of each subject, for a particular network edge, are distributed with some variance about the population connectivity strength. This regularises subjects towards the group mean, in a similar fashion to the <inline-formula><mml:math id="M1" altimg="si1.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> penalty used in <xref rid="bib71" ref-type="bibr">Ng et al. (2013)</xref>. Second, the population connectivity is constrained using a Cauchy prior (<xref rid="bib79" ref-type="bibr">Polson and Scott, 2012</xref>), which has a large mass near zero. This prior has many similarities to the double-exponential prior distribution, which has the same form as the widely-used <inline-formula><mml:math id="M2" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> penalty for sparsity promotion (<xref rid="bib18" ref-type="bibr">Danaher et al., 2015</xref>; <xref rid="bib27" ref-type="bibr">Friedman et al., 2008</xref>; <xref rid="bib71" ref-type="bibr">Ng et al., 2013</xref>; <xref rid="bib103" ref-type="bibr">Varoquaux et al., 2010</xref>; <xref rid="bib105" ref-type="bibr">Wang, 2012a</xref>). These two features alone create a sparse, hierarchical inverse covariance model. We form a second model by adding a final feature that regularises using an explicit sparse network structure. The probability of each network connection being present or absent is directly inferred using a spike and slab prior (<xref rid="bib68" ref-type="bibr">Mitchell and Beauchamp, 1988</xref>). This strong sparsity modelling is a feature of the Bayesian approach, and is not possible to frame as a convex optimisation problem.</p>
      <p id="p0060">Below, we set out the likelihood of the region of interest (ROI) data in each subject. Then we reformulate the inference of inverse covariance matrices as a linear regression problem under a broad range of priors, and position our two forms of the hierarchical model within this framework. Full details of our inference program and MCMC algorithm for this model, denoted HIPPO (Hierarchical Inference of Posterior Precisions in OSL<xref rid="fn1" ref-type="fn">1</xref>), are given in the supplementary material.</p>
      <sec id="sec2.1">
        <title>Likelihood for the connectivity model</title>
        <p id="p0065">We describe the (temporally demeaned) activations <inline-formula><mml:math id="M3" altimg="si3.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>1</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>2</mml:mn><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">ℝ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, within <italic>p</italic> ROIs, sampled at <inline-formula><mml:math id="M4" altimg="si4.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> time points, for each subject <italic>s</italic>, as being drawn independently from a multivariate Gaussian distribution with zero mean and precision matrix <inline-formula><mml:math id="M5" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><disp-formula id="fd1"><label>(1)</label><mml:math id="M6" display="block" altimg="si6.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M7" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the cone of positive definite <inline-formula><mml:math id="M8" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> matrices restricted to the graph <inline-formula><mml:math id="M9" altimg="si9.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> such that an absence of an edge from set <italic>E</italic> implies conditional independence of the two relevant variables in all subjects,<disp-formula id="ufd1"><mml:math id="M10" altimg="si10.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∉</mml:mo><mml:mi>E</mml:mi><mml:mo>⇒</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.25em"/><mml:mo>∀</mml:mo><mml:mspace width="0.25em"/><mml:mi>s</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p id="p0070">We use the general term <italic>activation</italic> to encompass changes in blood-oxygenation-level dependent (BOLD) response over time in fMRI, or fluctuations in the power envelope of oscillatory activity measured with MEG or electroencephalography (EEG).</p>
      </sec>
      <sec id="sec2.2">
        <title>Precision modelling as linked linear regression</title>
        <p id="p0075">Building on the work in <xref rid="bib105" ref-type="bibr">Wang, 2012a</xref>, <xref rid="bib107" ref-type="bibr">Wang, 2015</xref>, we show that a very broad range of priors from the linear regression literature can be applied to the elements of a precision matrix, with a simple restriction on the prior for the diagonal elements. Inference can be performed as a series of draws from the conditional distributions of linked columns of variables over all subjects. As long as the prior factorises over the elements of the precision matrix, it is possible to introduce layers of hyper-parameters without breaking this sampling approach. This will enable us to build a large hierarchical model within a tractable sampling framework.</p>
        <p id="p0080">Some notation is useful. We partition subjects' precision matrices as follows,<disp-formula id="fd2"><label>(2)</label><mml:math id="M11" display="block" altimg="si11.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p id="p0085">Without loss of generality, we can discuss just the final column of precision matrix <inline-formula><mml:math id="M12" altimg="si5.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M13" altimg="si12.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="M14" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> represent the first principal minor (the block matrix without the final row or column), † the conjugate transpose operator, and let <inline-formula><mml:math id="M15" altimg="si14.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> be the sample inner product matrix of subject <italic>s</italic>. Similar subscripts indicate identical partitions of other matrices, so for an inner product matrix, <inline-formula><mml:math id="M16" altimg="si15.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the diagonal element of the selected column and <inline-formula><mml:math id="M17" altimg="si16.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> the off-diagonal elements of the column. This is the same convention employed in Friedman et al.'s exposition of the graphical least absolute shrinkage and selection operator (LASSO) in 2008.</p>
        <p id="p0090">We define independent exponential prior distributions on the diagonal elements of the precision matrices, and require the priors on the off-diagonal elements to factorise over the elements (although in addition to any hyper-parameter matrices, <inline-formula><mml:math id="M18" altimg="si17.gif" overflow="scroll"><mml:mi mathvariant="bold">Ψ</mml:mi></mml:math></inline-formula>, such as group-level connectivity strengths, they may share some common scalar hyper-parameters, <inline-formula><mml:math id="M19" altimg="si18.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>),<disp-formula id="fd3"><label>(3)</label><mml:math id="M20" display="block" altimg="si19.gif" overflow="scroll"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>using <inline-formula><mml:math id="M21" altimg="si20.gif" overflow="scroll"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to denote a prior probability density. Combining (1) and (3), we can extract the conditional posterior for a column of the precision matrix,<disp-formula id="fd4"><label>(4)</label><mml:math id="M22" altimg="si110.gif" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>∝</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>×</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mo>†</mml:mo></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">Performing the variable substitution (<xref rid="bib105" ref-type="bibr">Wang, 2012a</xref>)<disp-formula id="fd5"><label>(5)</label><mml:math id="M23" display="block" altimg="si21.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>for which the Jacobian is the identity matrix, we obtain<disp-formula id="fd6"><label>(6)</label><mml:math id="M24" altimg="si22.gif" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mo>†</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p id="p0100">We can tidy up with the substitution <inline-formula><mml:math id="M25" altimg="si23.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="normal">ϒ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> to give a normal form for <inline-formula><mml:math id="M26" altimg="si24.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and a Gamma distribution on <inline-formula><mml:math id="M27" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>,<disp-formula id="fd7"><label>(7)</label><mml:math id="M28" display="block" altimg="si26.gif" overflow="scroll"><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mi mathvariant="normal">ϒ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mo>†</mml:mo></mml:msubsup><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula><disp-formula id="fd8"><label>(8)</label><mml:math id="M29" display="block" altimg="si27.gif" overflow="scroll"><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
        <p id="p0105">Equations <xref rid="fd7" ref-type="disp-formula">(7)</xref>, <xref rid="fd8" ref-type="disp-formula">(8)</xref> provide a basic block-Gibbs sampling scheme in which all variables associated with a column, across all subjects, are drawn together. It is important that the sampled matrices are positive definite, to qualify as valid precision matrices. The design of this aspect of the sampling algorithm (as described for a single precision matrix by Wang) ensures this condition. If, on each draw of the variables within a column, the principal minors <inline-formula><mml:math id="M30" altimg="si13.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are positive definite, then the updated matrices will by definition be positive definite if the Schur complement <inline-formula><mml:math id="M31" altimg="si28.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mn>22</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>†</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is greater than zero (<xref rid="bib5" ref-type="bibr">Boyd and Vandenberghe, 2004</xref>). This inequality is enforced by the strictly positive Gamma distribution on <inline-formula><mml:math id="M32" altimg="si25.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ν</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Assuming the sampler is well initialised, the algorithm guarantees positive definite precision matrices on each and every update.</p>
        <p id="p0110">We draw the comparison to conventional linear regression, <inline-formula><mml:math id="M33" altimg="si29.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">Xβ</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ε</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M34" altimg="si30.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">ε</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, conditional on <inline-formula><mml:math id="M35" altimg="si31.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>,<disp-formula id="ufd2"><mml:math id="M36" altimg="si32.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>p</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>†</mml:mo></mml:msup><mml:mi mathvariant="normal">ϒ</mml:mi><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>†</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ϒ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>†</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>†</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>to see that this factorisation into column-conditionals leads to inference as a set of <italic>p</italic> linked regressions on one variable and its interactions, given all the other variables. The link between partial correlation estimation and regression problems has been identified previously (<xref rid="bib76" ref-type="bibr">Peng et al., 2009</xref>), but within this Bayesian inference context, the key point is that we can now borrow sparse priors from the extensive linear regression literature. We are free to choose any prior that can factorise over the off-diagonal elements of the precision matrix and retain the simple block-Gibbs sampling scheme (7) and (8). Moreover, we are free to build a hierarchy of prior distributions over the elements of the precision matrices, so long as priors factorise over <inline-formula><mml:math id="M37" altimg="si33.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> when conditioned on the hyper-parameters. Sampling is possible in this framework by alternating block-Gibbs draws of <inline-formula><mml:math id="M38" altimg="si34.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>|</mml:mo><mml:mspace width="0.25em"/><mml:mo>−</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the conditional distribution of the columns of precision matrices within each subject <inline-formula><mml:math id="M39" altimg="si35.gif" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and of hyper-parameter matrices <inline-formula><mml:math id="M40" altimg="si36.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">ψ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (which might represent group-level connection strengths or a sparsity structure, for example), with Gibbs draws from the conditional distributions of any common hyper-parameters, <inline-formula><mml:math id="M41" altimg="si37.gif" overflow="scroll"><mml:mi>p</mml:mi><mml:mfenced><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mspace width="0.25em"/><mml:mo>|</mml:mo><mml:mspace width="0.25em"/><mml:mo>−</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p>
      </sec>
      <sec id="sec2.3">
        <title>Hierarchical sparse priors for precision matrices</title>
        <p id="p0115">Using the framework developed above, we describe two Bayesian sparse hierarchical models for inverse covariance matrices. The first explicitly models the presence or absence of edges within the functional network, strongly promoting sparsity in the system. The second removes this feature, and relies on continuous priors on the group connection strengths to suppress weak edges towards zero.</p>
        <sec id="sec2.3.1">
          <title>Model 1: a strongly sparse prior</title>
          <p id="p0120">For each subject, we place an exponential prior on the diagonal elements of the precision matrix, as in (3). This choice allows us to implement the column-wise sampling scheme described in equations <xref rid="fd4" ref-type="disp-formula">(4)–(8)</xref>. The free parameter in this distribution, <inline-formula><mml:math id="M42" altimg="si38.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, is given (for each subject) the ‘neutral’ Gamma conjugate hyperprior (<xref rid="bib53" ref-type="bibr">Kerman, 2011</xref>). There is normally plenty of information with which to estimate the diagonal elements (inverse variances), and so an uninformative prior is appropriate. The neutral Gamma prior is relatively uninformative on <inline-formula><mml:math id="M43" altimg="si39.gif" overflow="scroll"><mml:mrow><mml:mtext>log</mml:mtext><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> and is claimed to perform better than traditional <inline-formula><mml:math id="M44" altimg="si40.gif" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">ε</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">ε</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> priors (<italic>ibid.</italic>),<disp-formula id="fd9"><label>(9)</label><mml:math id="M45" display="block" altimg="si41.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>∼</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0125">The full prior on the off-diagonal elements is given in equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>, discussed in detail below, and is illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. In essence, it is a spike-and-slab prior with shared sparsity over subjects, normally-distributed connection strengths about the population mean, and regularisation on the mean effect.<disp-formula id="fd10"><label>(10)</label><mml:math id="M46" altimg="si42.gif" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>m</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>χ</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Bernoulli</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>a</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mo>∼</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Beta</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>π</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><fig id="fig1"><label>Fig. 1</label><caption><p>Hierarchical prior on precision matrices. A spike and non-central slab prior <italic>(2)</italic> is placed on each off-diagonal element of the precision matrices <italic>(1)</italic>. In the strongly sparse version of the prior, selection of the slab or spike (presence or absence of a network connection) is controlled by an adjacency matrix <italic>(3)</italic>, with a learnt sparsity level <italic>(4)</italic>. In the weakly sparse model, this feature is not used and only the slab imposed as a prior on connection strengths. The slab is modelled as a normal distribution describing the mean <italic>(5)</italic> and variance over connection strengths. The mean is regularised by a sparsity-inducing prior <italic>(6)</italic>, and the variance by a weakly informative log-normal prior <italic>(7)</italic>. We call this model and its inference scheme Hierarchical Inference of Posterior Precisions in OSL (HIPPO).</p></caption><alt-text id="alttext0025">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
          <p id="p0130">Each off-diagonal element of the precision matrices is given a spike and non-central slab prior. The spike, a delta-function at zero, imposes a common sparsity structure over all subjects, using edge inclusion variables <inline-formula><mml:math id="M47" altimg="si43.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula>. For those edges that are included, the non-central slab is a normal distribution, whose variance <inline-formula><mml:math id="M48" altimg="si44.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> characterises the between-subject variability of that particular connection strength, and whose mean <inline-formula><mml:math id="M49" altimg="si45.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> captures the group-level behaviour.</p>
          <p id="p0135">Following advice in <xref rid="bib30" ref-type="bibr">Gelman (2006)</xref> and <xref rid="bib79" ref-type="bibr">Polson and Scott (2012)</xref> on the inference of higher-level group parameters in regression, we apply regularisation to the mean edge strengths (<inline-formula><mml:math id="M50" altimg="si46.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:math></inline-formula>) towards zero using a normal distribution, learning the rough scale of these connectivities, <inline-formula><mml:math id="M51" altimg="si47.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, from the data and pooling this information over all the edges. This learnt variance parameter <inline-formula><mml:math id="M52" altimg="si47.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is constrained with a proper, sparsity-promoting, weakly informative prior. Gelman and Polson &amp; Scott recommend the half-Cauchy distribution for this application, denoted <inline-formula><mml:math id="M53" altimg="si48.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, because it has a finite mass at zero, is undefined for negative values, drops off near a scaling point <italic>A</italic>, and has a heavy tail which can allow the likelihood to dominate. An additional advantage is that the half-Cauchy and can be expressed in a conditionally-conjugate fashion through a scale mixture of normals (<xref rid="bib30" ref-type="bibr">Gelman, 2006</xref>; <xref rid="bib33" ref-type="bibr">Gelman et al., 2008</xref>; <xref rid="bib79" ref-type="bibr">Polson and Scott, 2012</xref>). This parameter expansion technique ensures that sampling for the group-level parameters requires only simple draws from multivariate normal distributions, and these parameters can be integrated over when sampling the top-level edge inclusion variables <inline-formula><mml:math id="M54" altimg="si43.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula>. An alternative choice to the Cauchy, from the same family but imposing stronger sparsity on the group connection strengths, would be the Laplace or LASSO prior (<xref rid="bib10" ref-type="bibr">Carvalho et al., 2010</xref>). The scale of the Cauchy distribution can be set sensibly based on the data: we use <inline-formula><mml:math id="M55" altimg="si49.gif" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula> as an appropriate value for variance-scaled data where correlations and partial correlations do not frequently exceed this number.</p>
          <p id="p0140">A broad normal prior is placed on the logarithm of <inline-formula><mml:math id="M56" altimg="si44.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, centred on <inline-formula><mml:math id="M57" altimg="si50.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and with a standard deviation <inline-formula><mml:math id="M58" altimg="si51.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to allow order-of-magnitude deviations from this value. Finally, a Beta-Bernoulli conjugate prior is placed on the edge inclusion variables, with a shared sparsity parameter <italic>a</italic> inferred from the data. The hyper-parameters in the top-level Beta distribution can be set to weakly encourage levels of sparsity encountered in functional networks. Using values of <inline-formula><mml:math id="M59" altimg="si52.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>π</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>π</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> places most of the prior mass between 0.3 and 0.7.</p>
          <p id="p0145">The values of the hyper-parameters we use are set out in <xref rid="tbl1" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Values of hyper-parameters employed for functional network modelling.</p></caption><alt-text id="alttext0085">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td align="left"><inline-formula><mml:math id="M60" altimg="si53.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left">0.5</td></tr><tr><td align="left"><inline-formula><mml:math id="M61" altimg="si54.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left">1</td></tr><tr><td align="left"><italic>A</italic></td><td align="left">0.7</td></tr><tr><td align="left"><inline-formula><mml:math id="M62" altimg="si55.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>π</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left">6</td></tr><tr><td align="left"><inline-formula><mml:math id="M63" altimg="si56.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>π</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left">6</td></tr></tbody></table></table-wrap></p>
        </sec>
        <sec id="sec2.3.2">
          <title>Model 2: a weakly sparse prior</title>
          <p id="p0150">The explicit sparsity can be removed from the model by setting <inline-formula><mml:math id="M64" altimg="si57.gif" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M65" altimg="si58.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mo>∀</mml:mo><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> to create a <italic>weakly sparse</italic> prior. It is weakly sparse in the sense that the group mean connectivities are still shrunk towards zero, using the Cauchy prior, and the subjects' precisions are distributed about these connection strengths. However, the model for the underlying GGM assumes a full graph, and the connectivity estimates should therefore be less sparse than from the first model. Inference is performed in exactly the same fashion as for the strongly sparse model, but without the need for updates on <inline-formula><mml:math id="M66" altimg="si43.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula>.</p>
        </sec>
      </sec>
      <sec id="sec2.4">
        <title>Model inference</title>
        <p id="p0155">The procedure taken for inference on the HIPPO model is described in full in supplementary information A. The sampler moves through a series of Gibbs steps, based on (7) and (8) above, in which all of the variables associated with a single column of the matrices are drawn together, <inline-formula><mml:math id="M67" altimg="si59.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>|</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mo>,</mml:mo><mml:mi>χ</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Within each of these column-conditionals, we exploit ideas from <xref rid="bib75" ref-type="bibr">Peltola et al. (2012)</xref> to collapse the conditional distribution over edge-strength parameters <inline-formula><mml:math id="M68" altimg="si60.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M69" altimg="si46.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:math></inline-formula>. This leaves a simple Metropolis-Hastings (MH) sampler on <inline-formula><mml:math id="M70" altimg="si61.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at the top level, checking for network edges to add or remove by testing the model evidence of each proposal. The parameters describing group-average and subject-level connectivities can then be sampled directly.</p>
        <p id="p0160">Draws from the posterior distribution of each subject's precision matrix can be used to construct a posterior over correlation or partial correlation matrices, and on the group average of these quantities. We use the posterior mean of the partial correlation distribution as a summary estimate of each subject's functional connectivity.</p>
      </sec>
    </sec>
    <sec id="sec3">
      <title>Methods</title>
      <sec id="sec3.1">
        <title>Evaluating sparse connectivity estimation using simulated data</title>
        <sec id="sec3.1.1">
          <title>Data generation</title>
          <p id="p0165">Ten simulated datasets were created to test the hierarchical models over a range of sparse network structures, model sizes, quantities of data, and amount of subject variability in connectivity. Except for some minor differences detailed below, each dataset consists of a number of subjects, with an individual precision matrix to represent each subject's functional connectivity; the data for each subject is a draw of samples from a zero-mean multivariate normal distribution, using the appropriate precision matrix. The general properties of each simulation are summarised in <xref rid="tbl2" ref-type="table">Table 2</xref>.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Description of simulated datasets used in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. Datasets are characterised by their size (number of subjects, network nodes and links, and data samples), the amount of subject variability (expressed as the standard deviation of connection strengths, over subjects, divided by the mean connection strength; we take the mean of this coefficient of variation over all connections present in the network), the sparsity of the network, and the type of network structure. We use simple circle models of varying sizes, first set out in <xref rid="bib106" ref-type="bibr">Wang (2012b)</xref> and used in <xref rid="bib105" ref-type="bibr">Wang (2012a)</xref> and <xref rid="bib50" ref-type="bibr">Hinne et al. (2015)</xref>; together with the largest simulation (netsim 4) from <xref rid="bib92" ref-type="bibr">Smith et al. (2011)</xref>; a random graph structure; and connection matrices built on estimates of the networks in cat cortex (<xref rid="bib89" ref-type="bibr">Scannell et al., 1999</xref>) and macaque visual cortex (<xref rid="bib23" ref-type="bibr">Felleman and Van Essen, 1991</xref>).</p></caption><alt-text id="alttext0090">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>ID</th><th>Subjects</th><th>Nodes</th><th>Links (full model)</th><th>Links</th><th>Samples</th><th>Variability</th><th>Sparsity</th><th>Model Structure</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">5</td><td align="left">6</td><td align="left">15</td><td align="left">6</td><td align="left">18</td><td align="left">0</td><td align="left">0.60</td><td align="left">Circle</td></tr><tr><td align="left">2</td><td align="left">25</td><td align="left">6</td><td align="left">15</td><td align="left">6</td><td align="left">18</td><td align="left">0</td><td align="left">0.60</td><td align="left">Circle</td></tr><tr><td align="left">3</td><td align="left">25</td><td align="left">6</td><td align="left">15</td><td align="left">6</td><td align="left">50</td><td align="left">0</td><td align="left">0.60</td><td align="left">Circle</td></tr><tr><td align="left">4</td><td align="left">25</td><td align="left">6</td><td align="left">15</td><td align="left">6</td><td align="left">100</td><td align="left">0</td><td align="left">0.60</td><td align="left">Circle</td></tr><tr><td align="left">5</td><td align="left">50</td><td align="left">50</td><td align="left">1225</td><td align="left">61</td><td align="left">200</td><td align="left">0</td><td align="left">0.95</td><td align="left">Netsim 4</td></tr><tr><td align="left">6</td><td align="left">25</td><td align="left">25</td><td align="left">300</td><td align="left">216</td><td align="left">500</td><td align="left">0.2</td><td align="left">0.28</td><td align="left">Random</td></tr><tr><td align="left">7</td><td align="left">25</td><td align="left">6</td><td align="left">15</td><td align="left">6</td><td align="left">25</td><td align="left">0.5</td><td align="left">0.60</td><td align="left">Circle</td></tr><tr><td align="left">8</td><td align="left">25</td><td align="left">30</td><td align="left">435</td><td align="left">21</td><td align="left">100</td><td align="left">1.1</td><td align="left">0.93</td><td align="left">Circle</td></tr><tr><td align="left">9</td><td align="left">30</td><td align="left">52</td><td align="left">1326</td><td align="left">438</td><td align="left">500</td><td align="left">1.8</td><td align="left">0.67</td><td align="left">Cat cortex</td></tr><tr><td align="left">10</td><td align="left">25</td><td align="left">30</td><td align="left">435</td><td align="left">161</td><td align="left">100</td><td align="left">2.7</td><td align="left">0.63</td><td align="left">Macaque<break/>visual cortex</td></tr></tbody></table></table-wrap></p>
          <p id="p0170"><italic>Circle models</italic> Simulations 1–4, 7 and 8 use a simple circular network structure. This structure has been used extensively in the sparse GGM literature: it was set out in <xref rid="bib106" ref-type="bibr">Wang (2012b)</xref>, and used in <xref rid="bib50" ref-type="bibr">Hinne et al. (2015)</xref>; <xref rid="bib69" ref-type="bibr">Mohammadi and Wit (2015)</xref>; and <xref rid="bib105" ref-type="bibr">Wang (2012a)</xref>. A precision matrix of any dimension <italic>p</italic> is constructed as <inline-formula><mml:math id="M71" altimg="si62.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si63.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M73" altimg="si64.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn></mml:mrow></mml:math></inline-formula>, with the lower diagonal elements matched to ensure symmetry.</p>
          <p id="p0175">Simulations 1–4 use the same precision matrix for each subject. Simulations 7 and 8 treat the circle model as the group mean matrix, assign random signs to the connection strengths, and draw single-subject connection strengths from a normal distribution about the group mean with a standard deviation of 0.05 and 0.15 respectively. Subjects' connectivity matrices were then adjusted to be positive semi-definite (<xref rid="bib47" ref-type="bibr">Higham, 1988</xref>)<xref rid="fn2" ref-type="fn">2</xref> and rescaled to unit variance.</p>
          <p id="p0180"><italic>Netsim</italic> Dataset 5 is the 4th network simulation from <xref rid="bib92" ref-type="bibr">Smith et al. (2011)</xref>, chosen because the 4th simulation was the largest model in that work. Smith et al. generated autocorrelated fMRI-like data from an asymmetric network model. As we are estimating symmetric precision matrices, we make the simple assumption that all subjects share the same precision matrix, and we estimate this ground truth as the unregularised partial correlation computed from the entire concatenated dataset, masked by the symmetrised adjacency matrix of the original simulation.</p>
          <p id="p0185"><italic>Random</italic> Dataset 6 is designed to mimic the prior structure. Each network edge is randomly assigned a probability of 0, 0.5 or 1 of being included in the network. Subjects' connection strengths on each edge are normally distributed about 0.25 with a standard deviation of 0.05. As each data point is drawn for each subject, edges in the model are turned on or off in accordance with their probability of edge inclusion. The ‘true’ matrix for each subject is the product of the edge inclusion probability matrix and the subject's connection strengths.</p>
          <p id="p0190"><italic>Cat cortex and macaque visual cortex</italic> Simulations 9 and 10 use the structures of mammalian cortical networks as their basis. Simulation 9 employs the cat cortical network from <xref rid="bib89" ref-type="bibr">Scannell et al. (1999)</xref> and simulation 10 that of the macaque visual cortex from <xref rid="bib23" ref-type="bibr">Felleman and Van Essen (1991)</xref><sup>.</sup><xref rid="fn3" ref-type="fn">3</xref> In each case, the network connection matrix is binary and asymmetrical. We used only the upper triangular part, symmetrising to the lower half. For each simulation, the group mean connection matrix was drawn from a conditional <italic>G</italic>-Wishart distribution (<xref rid="bib56" ref-type="bibr">Lenkoski, 2013</xref>) with identity scale matrix and degrees of freedom equal to one less than the number of network nodes. Individual subjects' network matrices were drawn from a conditional <italic>G</italic>-Wishart distribution using the mean connection matrix as the scale, and degrees of freedom set to 50 for the simulations from macaque visual cortex and 400 for those from cat cortex. The <italic>G</italic>-Wishart distributions were constrained using the relevant network matrices (cat, macaque) as the underlying graph.</p>
        </sec>
        <sec id="sec3.1.2">
          <title>Models tested</title>
          <p id="p0195">Sixteen different models were fitted to the test datasets, representing the range of methods in current use for covariance modelling from the most naïve to the most advanced. Their basic properties are set out in <xref rid="tbl3" ref-type="table">Table 3</xref>.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Characterisation of methods used on simulated data in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. We classify the methods under test by their inference method, the style of sparsity imposition, and whether they are fitted to individual subjects, the concatenated dataset, or infer individual connectivity matrices using information from the whole group. HIPPO is our acronym for the hierarchical sparse Bayesian model.</p></caption><alt-text id="alttext0095">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Fitted to</th><th>Sparsity</th><th>Inference</th><th>Reference</th></tr></thead><tbody><tr><td align="left">Partial correlation</td><td align="left">individuals</td><td align="left">none</td><td align="left">analytic</td><td align="left"><xref rid="bib26" ref-type="bibr">Fisher (1924)</xref></td></tr><tr><td align="left">Tikhonov</td><td align="left">individuals</td><td align="left">continuous</td><td align="left">optimised</td><td/></tr><tr><td align="left">Graphical LASSO (GLASSO)</td><td align="left">individuals</td><td align="left">continuous</td><td align="left">optimised</td><td align="left"><xref rid="bib65" ref-type="bibr">Mazumder and Hastie (2012b)</xref></td></tr><tr><td align="left">Group GLASSO (Varoquaux)</td><td align="left">group</td><td align="left">continuous</td><td align="left">optimised</td><td align="left"><xref rid="bib103" ref-type="bibr">Varoquaux et al. (2010)</xref></td></tr><tr><td align="left">Group GLASSO (Danaher)</td><td align="left">group</td><td align="left">continuous</td><td align="left">optimised</td><td align="left"><xref rid="bib18" ref-type="bibr">Danaher et al. (2015)</xref></td></tr><tr><td align="left">Fused GLASSO</td><td align="left">group</td><td align="left">continuous</td><td align="left">optimised</td><td align="left"><xref rid="bib18" ref-type="bibr">Danaher et al. (2015)</xref></td></tr><tr><td align="left">Sparse Group Gaussian Graphical Model (SGGGM)</td><td align="left">group</td><td align="left">continuous</td><td align="left">optimised</td><td align="left"><xref rid="bib71" ref-type="bibr">Ng et al. (2013)</xref></td></tr><tr><td align="left">Wishart</td><td align="left">individuals</td><td align="left">continuous</td><td align="left">analytic</td><td align="left"><xref rid="bib34" ref-type="bibr">Gelman et al. (2014)</xref></td></tr><tr><td align="left">Hierarchical Wishart</td><td align="left">group</td><td align="left">none</td><td align="left">MCMC</td><td align="left"><xref rid="bib63" ref-type="bibr">Marrelec et al. (2006)</xref></td></tr><tr><td align="left">Bayesian GLASSO</td><td align="left">individuals</td><td align="left">continuous</td><td align="left">MCMC</td><td align="left"><xref rid="bib105" ref-type="bibr">Wang (2012a)</xref></td></tr><tr><td align="left">Stochastic Search Variable Selection (SSVS)</td><td align="left">individuals</td><td align="left">normal-mixture</td><td align="left">MCMC</td><td align="left"><xref rid="bib107" ref-type="bibr">Wang (2015)</xref></td></tr><tr><td align="left"><italic>G</italic>-Wishart</td><td align="left">concatenation</td><td align="left">spike &amp; slab</td><td align="left">MCMC</td><td align="left"><xref rid="bib50" ref-type="bibr">Hinne et al. (2015)</xref></td></tr><tr><td align="left">Bayesian Multiple Gaussian Graphical Models (MGGM)</td><td align="left">group</td><td align="left">spike &amp; slab</td><td align="left">MCMC</td><td align="left"><xref rid="bib78" ref-type="bibr">Peterson et al. (2015)</xref></td></tr><tr><td align="left">Single-subject HIPPO</td><td align="left">concatenation</td><td align="left">spike &amp; slab</td><td align="left">MCMC</td><td/></tr><tr><td align="left">Weakly-sparse HIPPO</td><td align="left">group &amp; individuals</td><td align="left">continuous</td><td align="left">MCMC</td><td/></tr><tr><td align="left">Strongly-sparse HIPPO</td><td align="left">group &amp; individuals</td><td align="left">spike &amp; slab</td><td align="left">MCMC</td><td/></tr></tbody></table></table-wrap></p>
          <p id="p0200"><italic>Partial correlation</italic> The sample covariance matrix for each subject is inverted, using the Cholesky algorithm, and normalised to produce the unregularised partial correlation matrix.</p>
          <p id="p0205"><italic>Tikhonov</italic> A Tikhonov-regularised estimate of the precision matrix is constructed by slightly augmenting the diagonal of the sample covariance matrix,<disp-formula id="fd11"><label>(11)</label><mml:math id="M74" display="block" altimg="si65.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0210">The regularisation parameter <italic>λ</italic> was chosen to minimise the RMS distance between the subjects' matrices and their unregularised group average.</p>
          <p id="p0215">This is the procedure used by the HCP and the UK Biobank imaging project in their estimation of fMRI network matrices.</p>
          <p id="p0220"><italic>GLASSO</italic> The graphical LASSO algorithm of <xref rid="bib27" ref-type="bibr">Friedman et al. (2008)</xref>, with modifications for computational efficiency (<xref rid="bib64" ref-type="bibr">Mazumder and Hastie, 2012a</xref>; <xref rid="bib65" ref-type="bibr">b</xref>), solves the optimisation problem<disp-formula id="fd12"><label>(12)</label><mml:math id="M75" display="block" altimg="si66.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="0.25em"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mtext>det</mml:mtext><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi mathvariant="bold">Ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M76" altimg="si67.gif" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>⋅</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> indicates the <inline-formula><mml:math id="M77" altimg="si2.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> norm (sum of the absolute values of the elements of the matrix). The regularisation parameter <italic>λ</italic> was chosen to minimise the RMS distance between the subjects' matrices and their unregularised group average. GLASSO is a very common method for estimating partial correlation brain networks, and the most successful tested in <xref rid="bib92" ref-type="bibr">Smith et al. (2011)</xref>, making it a good benchmark for our work.</p>
          <p id="p0225"><italic>Group GLASSO</italic> The group graphical LASSO of <xref rid="bib103" ref-type="bibr">Varoquaux et al. (2010)</xref> is fitted to all subjects at once, and encourages a similar sparsity pattern across them. It solves the optimisation problem,<xref rid="fn4" ref-type="fn">4</xref><disp-formula id="ufd3"><mml:math id="M78" altimg="si68.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="0.25em"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mtext>det</mml:mtext><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0230">The regularisation parameters were chosen to maximise the predictive log-likelihood under the default cross-validation scheme, which repeatedly narrows down the hyper-parameter search space.</p>
          <p id="p0235">The group graphical LASSO of <xref rid="bib18" ref-type="bibr">Danaher et al. (2015)</xref> is a generalisation of Varoquaux et al.'s model. It solves the optimisation problem,<disp-formula id="fd13"><label>(13)</label><mml:math id="M79" display="block" altimg="si69.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="0.25em"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mtext>det</mml:mtext><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p id="p0240">The regularisation parameters were chosen to minimise the Bayesian information criterion associated with this likelihood. Inference was performed in Matlab.</p>
          <p id="p0245"><italic>Fused GLASSO</italic> The fused graphical LASSO of <xref rid="bib18" ref-type="bibr">Danaher et al. (2015)</xref> is also fitted to all subjects at once, and seeks to impose collective sparsity on all subjects' network elements, while encouraging networks from different subjects to be alike. Inference is set up as an optimisation problem with two penalty terms, solved using alternating directions method of multipliers (ADMM),<xref rid="fn5" ref-type="fn">5</xref><disp-formula id="fd14"><label>(14)</label><mml:math id="M80" display="block" altimg="si70.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="0.25em"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mtext>det</mml:mtext><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>'</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p id="p0250">The regularisation parameters were chosen to minimise the difference between the group mean connectivity inferred using half of the dataset and the unregularised mean of the other half of the dataset.</p>
          <p id="p0255"><italic>SGGGM</italic> The Sparse Group Gaussian Graphical Model (SGGM) proposed by Ng et al. defines group-level connection strengths, and regularises each subject's estimates towards this central representation. A restricted maximum-likelihood solution, found using ADMM,<xref rid="fn6" ref-type="fn">6</xref> solves the optimisation problem,<disp-formula id="fd15"><label>(15)</label><mml:math id="M81" display="block" altimg="si71.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mspace width="0.25em"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mtext>det</mml:mtext><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:msubsup><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
          <p id="p0260">By imposing sparsity on the group network, and using a Frobenius norm penalty on the difference between elements of subjects' matrices and the group, it has a hierarchical structure that is very similar in form to the weakly sparse Bayesian hierarchical model that we propose. The regularisation parameters were chosen to minimise the distance between the group mean connectivity inferred using half of the dataset and the unregularised mean of the other half of the dataset.</p>
          <p id="p0265"><italic>Wishart</italic> Following <xref rid="bib34" ref-type="bibr">Gelman et al. (2014)</xref>, a simple Wishart prior distribution is placed independently over each subject's precision matrix, <inline-formula><mml:math id="M82" altimg="si72.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This leads analytically to the posterior for each subject, which we summarise by its expectation.<xref rid="fn7" ref-type="fn">7</xref> The similarity to Tikhonov regularisation is clear,<disp-formula id="fd16"><label>(16)</label><mml:math id="M83" display="block" altimg="si73.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0270"><italic>Hierarchical Wishart</italic>
<xref rid="bib63" ref-type="bibr">Marrelec et al. (2006)</xref> proposed a hierarchical model for the covariance structure of fMRI recordings. No encouragement of sparsity was introduced in the prior structure, but it makes a useful comparison point for our hierarchical models. Rather than Marrelec et al.'s hierarchy of Inverse-Wishart distributions on covariance matrices, we use an equivalent hierarchy of Wishart distributions on precision matrices,<disp-formula id="fd17"><label>(17)</label><mml:math id="M84" display="block" altimg="si74.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0275">Marrelec et al. do not mention placing a hyperprior on the group-level parameters, so we presume they used a flat prior, <inline-formula><mml:math id="M85" altimg="si75.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>∼</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. We prefer to use a very weakly informative prior, and follow Hinne et al. and Gelman et al. in selecting a very weak Wishart hyperprior for the group connection strengths,<disp-formula id="fd18"><label>(18)</label><mml:math id="M86" display="block" altimg="si76.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M87" altimg="si77.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">I</mml:mi></mml:math></inline-formula> is the identity matrix. This model leads to a simple Gibbs inference scheme,<disp-formula id="fd19"><label>(19)</label><mml:math id="M88" display="block" altimg="si78.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="fd20"><label>(20)</label><mml:math id="M89" display="block" altimg="si79.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mi>N</mml:mi><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p>
          <p id="p0280">Marrelec et al. also did not discuss methods for inferring the degrees of freedom of the group-level prior, <inline-formula><mml:math id="M90" altimg="si80.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, which controls the strength of the regularisation. There is no simple conjugate hyperprior that can be used, so we take a simple empirical approach. We set <inline-formula><mml:math id="M91" altimg="si80.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to be the value that, under 5 bootstrapped cross-validation splits of the subjects into two halves, minimises the error between the mean of the partial correlation matrices inferred with the hierarchical Wishart model and the mean of the remaining partial correlation matrices inferred with the GLASSO, using mild regularisation (<italic>λ</italic> = 0.01). We run the Gibbs sampler for 1500 iterations, using an additional 1000 as warm-up.</p>
          <p id="p0285"><italic>Bayesian GLASSO</italic> The Bayesian graphical LASSO of <xref rid="bib105" ref-type="bibr">Wang (2012a)</xref> places a Laplace or double-exponential prior on the off-diagonal elements of the precision matrix,<disp-formula id="fd21"><label>(21)</label><mml:math id="M92" display="block" altimg="si81.gif" overflow="scroll"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula>For each subject, 3000 samples were drawn using Wang's algorithm,<xref rid="fn8" ref-type="fn">8</xref> after discarding 1000 as warm-up.</p>
          <p id="p0290"><italic>SSVS</italic> The Bayesian Stochastic Search Variable Selection algorithm (<xref rid="bib107" ref-type="bibr">Wang, 2015</xref>) for covariance selection places a mixture of normal priors on the off-diagonal elements of the precision matrix,<disp-formula id="fd22"><label>(22)</label><mml:math id="M93" display="block" altimg="si82.gif" overflow="scroll"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M94" altimg="si83.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is chosen to be much smaller than <inline-formula><mml:math id="M95" altimg="si84.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and sampling proceeds using Gibbs sampling along the columns. We follow Wang's recommendations and set <inline-formula><mml:math id="M96" altimg="si83.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to 0.05, <inline-formula><mml:math id="M97" altimg="si84.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to 2.5 and <italic>a</italic> to 0.5. For each subject, 3000 samples were drawn after 1000 warm-up samples.<xref rid="fn9" ref-type="fn">9</xref> Neither this model, nor the Bayesian GLASSO above, has been used for neuroimaging, to our knowledge.</p>
          <p id="p0295"><italic>G-Wishart</italic> The <italic>G</italic>-Wishart distribution is the conjugate prior on the multivariate normal, describing a single precision matrix <inline-formula><mml:math id="M98" altimg="si85.gif" overflow="scroll"><mml:mi mathvariant="bold">Ω</mml:mi></mml:math></inline-formula>, conditional on the graph <inline-formula><mml:math id="M99" altimg="si86.gif" overflow="scroll"><mml:mi mathvariant="script">G</mml:mi></mml:math></inline-formula> on which it is supported,<disp-formula id="fd23"><label>(23)</label><mml:math id="M100" altimg="si87.gif" overflow="scroll"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mo>,</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>δ</mml:mi><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mi>exp</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">Ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M101" altimg="si88.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">V</mml:mi></mml:math></inline-formula> is the scale matrix, <italic>δ</italic> indicates the degrees of freedom, <inline-formula><mml:math id="M102" altimg="si89.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the intractable normalising constant, and <inline-formula><mml:math id="M103" altimg="si85.gif" overflow="scroll"><mml:mi mathvariant="bold">Ω</mml:mi></mml:math></inline-formula> is constrained to live on the cone <inline-formula><mml:math id="M104" altimg="si7.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of positive definite <inline-formula><mml:math id="M105" altimg="si8.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> matrices with zeros indicated by the graph <inline-formula><mml:math id="M106" altimg="si86.gif" overflow="scroll"><mml:mi mathvariant="script">G</mml:mi></mml:math></inline-formula>.</p>
          <p id="p0300">Most Bayesian sparse precision modelling efforts have focussed on this prior. Sampling from the <italic>G</italic>-Wishart distribution, conditional on a known graph <inline-formula><mml:math id="M107" altimg="si86.gif" overflow="scroll"><mml:mi mathvariant="script">G</mml:mi></mml:math></inline-formula>, can be performed easily (<xref rid="bib56" ref-type="bibr">Lenkoski, 2013</xref>). Sampling from the joint distribution <inline-formula><mml:math id="M108" altimg="si90.gif" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is much harder. A scalable inference solution is still elusive, and no attempt has been made at a hierarchical model that could learn the scale matrix <inline-formula><mml:math id="M109" altimg="si88.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">V</mml:mi></mml:math></inline-formula>. The most efficient <italic>G</italic>-Wishart approach perhaps is set out in <xref rid="bib50" ref-type="bibr">Hinne et al. (2015)</xref>, which applies the model for inference of subcortical functional connectivity in fMRI.</p>
          <p id="p0305">We fitted the <italic>G</italic>-Wishart model to the entire dataset concatenated over subjects, using software provided by Hinne et al.,<xref rid="fn10" ref-type="fn">10</xref> using 5000 warm-up samples and 10 000 draws from the distribution. We follow Hinne et al. in using an uninformative prior specification, <inline-formula><mml:math id="M110" altimg="si91.gif" overflow="scroll"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M111" altimg="si92.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Fitting the model to the concatenated data provides a useful comparison to the hierarchical models, using all of the data for inference, but assuming that each subject shares the same network matrix. It would not be computationally feasible to run the <italic>G</italic>-Wishart algorithm separately on individual subjects for the larger network models.</p>
          <p id="p0310"><italic>MGGM</italic> The Bayesian Multiple Gaussian Graphical Models (MGGM) approach, proposed by Peterson et al., is a hierarchical generalisation of the network structure used by the <italic>G</italic>-Wishart model. It posits that each subject (or sub-group) can have a different graphical model structure (although it shares no information about the connection strengths), and links these models using a Markov random field (MRF) prior,<disp-formula id="fd24"><label>(24)</label><mml:math id="M112" display="block" altimg="si93.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>π</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">Ω</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Θ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mspace width="0.25em"/><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">Θ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where the <inline-formula><mml:math id="M113" altimg="si94.gif" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> binary vector <inline-formula><mml:math id="M114" altimg="si95.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> defines the presence of an edge in each subject, the edge-specific hyper-parameter <inline-formula><mml:math id="M115" altimg="si96.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> indicates the likelihood of an edge and is given a Beta hyper-prior. The <inline-formula><mml:math id="M116" altimg="si97.gif" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:math></inline-formula> symmetric matrix <inline-formula><mml:math id="M117" altimg="si98.gif" overflow="scroll"><mml:mtext>Θ</mml:mtext></mml:math></inline-formula> encodes the pairwise similarity of each graph <inline-formula><mml:math id="M118" altimg="si99.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and is in turn given a spike-and-slab prior.</p>
          <p id="p0315">Unfortunately, the flexibility of this MRF prior also brings complexity: the computational burden of Peterson et al.'s algorithm<xref rid="fn11" ref-type="fn">11</xref> scales as <inline-formula><mml:math id="M119" altimg="si100.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>S</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Allowing each subject to have its own sparse model structure becomes infeasible for most practical purposes. We tested the performance of the model only for our first simulated dataset of five subjects, using 5000 warm-up samples and 10 000 draws from the posterior. The model would be more practical for exploring differences in network structure between two or three groups of subjects.</p>
          <p id="p0320"><italic>Single-subject HIPPO</italic> A model based on the sparse hierarchical prior presented here (equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>), but simplified for single-subject inference, was designed as a comparison to the performance of the <italic>G</italic>-Wishart model. The prior can be expressed as<disp-formula id="fd25"><label>(25)</label><mml:math id="M120" display="block" altimg="si101.gif" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>0.7</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:mtext>Bernoulli</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:mtext>Beta</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>6,6</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>λ</mml:mi><mml:mspace width="0.25em"/><mml:mo>∼</mml:mo><mml:mspace width="0.25em"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mtext>.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
          <p id="p0325">Inference follows the format above. We use a single chain for inference, drawing 5000 warm-up samples and 10 000 samples from the distribution.</p>
          <p id="p0330"><italic>Weakly-sparse HIPPO</italic> The HIPPO hierarchical model set up without the explicit sparsity prior. There is still regularisation of the group connection strengths towards zero—in this sense, it is <italic>weakly sparse</italic>. Inference is the same as under the strongly sparse HIPPO model, but conditional on a full graph: all edge inclusion variables <inline-formula><mml:math id="M121" altimg="si102.gif" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are set to 1 (as described in section <xref rid="sec2.3.2" ref-type="sec">2.3.2</xref>, Cauchy priors are chosen that suppress the mean connectivities towards zero with the subjects distributed around this point, without imposing absolute edge sparsity). We draw 30 000 samples, with 10 000 as warm-up.</p>
          <p id="p0335"><italic>Strongly-sparse HIPPO</italic> The full sparse hierarchical model (Hierarchical Inference of Posterior Precisions in OSL) set out in equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>. We draw 30 000 samples in a single chain, with an additional 10 000 as warm-up.</p>
        </sec>
        <sec id="sec3.1.3">
          <title>Analysis</title>
          <p id="p0340">After fitting each model, we compute the root-mean-square (RMS) error (over edges) between each subject's simulated connection strengths and the inferred partial correlation matrices. We compare the mean and standard deviation of this metric over subjects. We also compute the area under the receiver-operator characteristic (ROC) curve, which traces the trade-off between specificity and sensitivity in detection of network edges in the simulated sparse network as a threshold is applied to the inferred connection strengths. For all of the Bayesian models, we use the mean of the posterior over partial correlation matrices as the summary estimate of connectivity in each subject.</p>
        </sec>
      </sec>
      <sec id="sec3.2">
        <title>Performance evaluation using resting-state data</title>
        <p id="p0345">To evaluate models' ability to accurately reconstruct functional networks using real data, we test how well they can estimate connectivity from very limited samples of fMRI and MEG data. Using the best-performing models, we illustrate two additional analyses. We look at the models' ability to predict subjects' biological and behavioural traits from their fMRI connectomes, and estimate the proportion of variation in MEG functional connectivity that could be attributable to genetic factors.</p>
        <sec id="sec3.2.1">
          <title>Dataset</title>
          <p id="p0350">We use fMRI data from the first 200 subjects of the Human Connectome Project's HCP900 data release (<xref rid="bib22" ref-type="bibr">Van Essen et al., 2013</xref>). All subjects provided four 15-min resting-state fMRI scans. We also use the 61 subjects from the MEG2 data release (<xref rid="bib54" ref-type="bibr">Larson-Prior et al., 2013</xref>), who provided three resting-state MEG scans of 6 mins' duration. All subjects are young (22–35 years of age) and healthy.</p>
          <p id="p0355">A heritability analysis on the MEG data exploits the family structures of the subjects. Of the 61, 28 are monozygotic twins and 16 are dizygotic twins. Zygosity of twin subjects was determined by genotype where available, and otherwise by self report.</p>
          <p id="p0360">HCP data were acquired using protocols approved by the Washington University institutional review board. Informed consent was obtained from subjects. Anonymised data are publicly available online from ConnectomeDB.<xref rid="fn12" ref-type="fn">12</xref></p>
        </sec>
        <sec id="sec3.2.2">
          <title>fMRI preprocessing and predictive analyses</title>
          <p id="p0365">Resting-state fMRI data were acquired with 2 mm isotropic spatial resolution and a temporal resolution of 0.72 s. The HCP provides comprehensively pre-processed data (<xref rid="bib38" ref-type="bibr">Glasser et al., 2013</xref>) that are registered to a standard cortical surface with the MSMAll algorithm (<xref rid="bib39" ref-type="bibr">Glasser et al., 2016</xref>; <xref rid="bib85" ref-type="bibr">Robinson et al., 2014</xref>; a high-quality registration approach that combines descriptions of brain structure, function and connectivity from multiple imaging modalities to precisely align functional regions), and for which structured artefacts have been removed by a combination of independent component analysis (ICA) and FIX (<xref rid="bib88" ref-type="bibr">Salimi-Khorshidi et al., 2014</xref>), FSL's automated noise component classifier.</p>
          <p id="p0370">We modelled connectivity between the 25 non-contiguous spatial components, computed by group ICA, that are released by the HCP. For simplicity, we fitted our models to the concatenated data over all four scans. We fitted both the strongly sparse and weakly sparse hierarchical models, running three sampling chains for 40 000 samples in the sparse model, with 20 000 needed for convergence in the weakly sparse model, using an additional 10 000 samples as warm-up. Additionally, we fitted Ng et al.'s SGGM, choosing the regularisation parameters to minimise the root mean square distance between individual subjects' partial correlation matrices inferred from half of the available data, and unregularised estimates from the remaining half. Finally, we estimated Tikhonov-regularised precision matrices for each subject. We followed the procedure used for the connectomes released from the HCP, applying only gentle regularisation with <italic>λ</italic> set to 0.01.</p>
          <p id="p0375">Having computed precision matrices for each subject with these three methods, and converted into partial correlations (taking the posterior mean from the Bayesian models as a summary estimate), we fitted linear predictive models to two traits recorded as part of the HCP: sex, and the number of correct scores on a picture vocabulary test. We used the partial correlations on each network edge, for each subject, as the predictors, after regressing out the confounding effects of age, the square of age, sex and an age–sex interaction term,<xref rid="fn13" ref-type="fn">13</xref> the cube root of cortical volume and of intra-cranial volume, both computed with Freesurfer, the software version for image reconstruction, and an estimate of each subject's motion in the scanner (rfmri_motion). Sex was predicted using logistic regression with elastic net regularisation (<xref rid="bib28" ref-type="bibr">Friedman et al., 2010</xref>; <xref rid="bib113" ref-type="bibr">Zou and Hastie, 2005</xref>).<xref rid="fn14" ref-type="fn">14</xref> Scores on the picture vocabulary test were demeaned and standardised, and predicted with linear regression using elastic net regularisation. Parameters for the elastic net were tuned in both cases by two nested loops of 5-fold cross-validation. Performance of the models was assessed by computing accuracy (sex) or correlation between predicted scores and real performance (picture vocabulary task), using a 5-fold cross-validation loop for training and prediction. The stratification of subjects into the cross-validation folds was designed such that families were not split over the fold groupings (<xref rid="bib108" ref-type="bibr">Winkler et al., 2015</xref>).</p>
        </sec>
        <sec id="sec3.2.3">
          <title>MEG preprocessing and genetic analyses</title>
          <p id="p0380">Resting-state MEG data were acquired on a whole-head Magnes 3600 scanner (4D Neuroimaging, San Diego, CA, USA). The data were pre-processed to compensate for head movement, to remove artefactual segments of time from the recordings, identify recording channels which are faulty, and to regress out artefacts with clear artefactual temporal signatures (such as eye-blinks or cardiac interference) using ICA (<xref rid="bib54" ref-type="bibr">Larson-Prior et al., 2013</xref>). Sensor-space data were down-sampled from 509 Hz to 300 Hz, with the application of an anti-aliasing filter.</p>
          <p id="p0385">MEG data from each session were source-reconstructed using a scalar beamformer (<xref rid="bib84" ref-type="bibr">Robinson and Vrba, 1999</xref>; <xref rid="bib104" ref-type="bibr">Van Veen et al., 1997</xref>; <xref rid="bib110" ref-type="bibr">Woolrich et al., 2011</xref>). Pre-computed single-shell source models are provided by the HCP at multiple resolutions, registered into the standard co-ordinate space of the Montreal Neuroimaging Institute. Data were filtered into the 1–30 Hz band before being beamformed onto a 6 mm grid using normalised lead fields. Covariance estimation was regularised using principal component analysis (PCA) rank reduction (<xref rid="bib110" ref-type="bibr">Woolrich et al., 2011</xref>). The rank was conservatively reduced by five more than the number of ICA components removed during preprocessing. Source estimates were normalised by the power of the projected sensor noise. Source-space data were filtered into the beta (13–30 Hz) band, which is associated with a range of resting-state network profiles (<xref rid="bib1" ref-type="bibr">Baker et al., 2014</xref>; <xref rid="bib7" ref-type="bibr">Brookes et al., 2012</xref>, <xref rid="bib6" ref-type="bibr">2011</xref>; <xref rid="bib15" ref-type="bibr">Colclough et al., 2015</xref>; <xref rid="bib51" ref-type="bibr">Hipp et al., 2012</xref>; <xref rid="bib62" ref-type="bibr">Mantini et al., 2007</xref>; <xref rid="bib73" ref-type="bibr">de Pasquale et al., 2012</xref>, <xref rid="bib74" ref-type="bibr">2015</xref>) and exhibits strong heritability in its functional connectivity profile (<xref rid="bib17" ref-type="bibr">Colclough et al., 2017</xref>). We employed the parcellation from <xref rid="bib16" ref-type="bibr">Colclough et al. (2016</xref>, <xref rid="bib17" ref-type="bibr">2017)</xref>, which consists of contiguous regions extracted from components of an ICA decomposition of the resting-state fMRI recordings of the first 200 HCP subjects. A single time course was constructed to represent each node, following <xref rid="bib15" ref-type="bibr">Colclough et al. (2015)</xref>, as the first principal component of the ROI, after weighting the PCA over voxels by the strength of the ICA spatial map. This analysis yielded 39 time courses for each resting-state session. Spatial leakage confounds were reduced using a symmetric orthogonalisation procedure (<xref rid="bib15" ref-type="bibr">Colclough et al., 2015</xref>) to reduce shared signal at zero lag between the network nodes. Lastly, power envelopes of the leakage-reduced ROI time courses were computed by taking the absolute value of the Hilbert transform of the signals, low-pass filtering with a cut-off of 1 Hz, and down-sampling to 2 Hz (<xref rid="bib60" ref-type="bibr">Luckhoo et al., 2012</xref>). Time courses were concatenated over sessions for the purpose of functional connectivity estimation.</p>
          <p id="p0390">We estimated functional connectivity in the same manner as for the fMRI data, using both the strong and weakly sparse HIPPO models, Ng et al.'s SGGM and lightly Tikhonov-regularised inversion of the sample covariance matrices. Identical inference procedures were followed to the fMRI data.</p>
          <p id="p0395">The mean heritability of functional connectivity was estimated from an ACE model, computed using APACE (<xref rid="bib13" ref-type="bibr">Chen et al., 2014</xref>).<xref rid="fn15" ref-type="fn">15</xref> The ACE model is a linear variance-components decomposition that ascribes a portion of the variability in each phenotype (functional network connection) to either additive genetics (A, <inline-formula><mml:math id="M122" altimg="si103.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>), developmental and common environmental factors shared between twins (C, <inline-formula><mml:math id="M123" altimg="si104.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) and other unmodelled variabilities and noise sources (E, <inline-formula><mml:math id="M124" altimg="si105.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>). The twin structure of the HCP dataset is sufficient to infer on all three components of the model; see <xref rid="bib13" ref-type="bibr">Chen et al. (2014)</xref> and <xref rid="bib12" ref-type="bibr">Chen (2014)</xref> for details. This analysis of heritability replicates previous work (<xref rid="bib17" ref-type="bibr">Colclough et al., 2017</xref>), although with a smaller set of subjects. To provide comparable results, we followed similar analysis steps by fitting the ACE models to correlation matrices, rather than to partial correlation matrices. (For the Bayesian models, we summarise the connectivity for each subject as the posterior mean of the distribution over correlation matrices.) This decision was originally made as correlation matrices are among the most repeatable forms of network analysis in MEG, with better reliability than partial correlation matrices (<xref rid="bib16" ref-type="bibr">Colclough et al., 2016</xref>). Heritability (<inline-formula><mml:math id="M125" altimg="si103.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) was computed for each network edge, after regressing out the effect of age, the square of age, sex, an age and sex interaction, the interaction between sex and the square of age, the cube root of intra-cranial volume and of cortical volume (both estimated with FreeSurfer), a measure of subject motion from fMRI recordings (a proxy as no motion measure is available for the MEG scans), an estimate of the noise passed by the beamformer for each subject, and finally two measures of node power, one formed from the standard deviation of the MEG power envelope and the other from the coefficient of variation of the power envelopes. The mean heritability was computed over the network connections, with 95% bootstrapped confidence intervals estimated using 10 000 sub-samples of the data, and permutation-based <italic>p</italic>-values computed using 1000 relabellings of the twin pairs.</p>
        </sec>
        <sec id="sec3.2.4">
          <title>Performance evaluations using limited data</title>
          <p id="p0400">Lastly, we estimated functional connectivities with the strongly and weakly sparse hierarchical models (HIPPO) for each meg and fMRI subject using only a small portion of the available data: the first resting-state session (of 6 min) in MEG and the first 5 min of recording for fMRI. This allows us to compare network estimations from limited amounts of data to the assumedly much more accurate estimates derived from the entire dataset. Tikhonov estimates from the full datasets (18 min and 60 min, respectively) were computed using the HCP's standard setting of the regularisation parameter <italic>λ</italic> to 0.01. Additionally on the restricted data samples, we tested network estimation using the same Tikhonov regularisation approach; naïve covariance inversion; the original GLASSO, Varoquaux et al.'s group GLSSO, Ng et al.'s SGGM, Danaher et al.'s fused glasso, and Peterson et al.'s MGGM. Where appropriate, regularisation parameters were chosen to minimise the distance between individual subjects' network matrices inferred from half of the available data, and unregularised estimates from the remaining half.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec4">
      <title>Results</title>
      <sec id="sec4.1">
        <title>Inference of simulated sparse networks</title>
        <p id="p0405">The performance of the strongly sparse and weakly sparse Bayesian hierarchical models presented in section <xref rid="sec2" ref-type="sec">2</xref> is compared with that of 13 additional models, over 10 different simulated datasets, in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The models are summarised in <xref rid="tbl3" ref-type="table">Table 3</xref> and the datasets in <xref rid="tbl2" ref-type="table">Table 2</xref>. It is worth noting some general trends. Firstly, as the amount of data increases for inference, covariance estimation becomes less noisy and the error in reconstruction goes down (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, simulations 1–4). Of particular note is the difference between datasets 1 and 2, where the number of subjects increases but the amount of data per subject is constant. The hierarchical Bayesian models are able to use this increase in information to reduce reconstruction error, whereas models fitted to individual subjects self-evidently are not. Secondly, attempting to perform inference on precision matrices without any form of regularisation is in general a bad idea: all methods tested outperform the simple inversion of the sample covariance matrix. Thirdly, in these simulations of sparse networks, models which build in explicit sparsity with spike-and-slab priors (the <italic>G</italic>-Wishart models and the single- and multi-subject sparse HIPPO models) show improved reconstruction compared to models with differentiable regularisation terms (e.g. GLASSO) or continuous priors (the Bayesian GLASSO and the weakly sparse HIPPO). Fourthly, in simulations with little-to-no between-subject variability, models fitted to the concatenated group data perform the best (unsurprisingly). However, as the variability between subjects increases, the hierarchical models that allow individual subject network estimation win out.<fig id="fig2"><label>Fig. 2</label><caption><p>Comparison of sparse network modelling methods on simulated datasets. The rms error between the simulated precision matrices and estimated precision matrices is shown for 10 multi-subject artificial datasets, using 15 different models for inference (indexed by colour). The error is expressed as a proportion of the rms error from a simple partial correlation estimate (naïve matrix inversion). Bars indicate the mean error over subjects, with the standard deviation over subjects given by the associated dark red line. Datasets 1–5 have no subject-variability in the simulated networks; subject variability increases through datasets 6–10. With limited subject variability, models fitted to the concatenated data perform the best. As this variation increases, the Bayesian hierarchical models win out. There is no result for the fused GLASSO on simulation 5, because the model would not run in a feasible time-frame (it would take longer than a week). Inference for the MGGM approach was only possible in the first, six-subject dataset.</p></caption><alt-text id="alttext0030">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
        <p id="p0410">The best performing non-Bayesian model is Ng et al.'s hierarchically-structured SGGM. In a similar fashion to our weakly sparse HIPPO prior, it uses a regularisation term to encourage similarity between subjects' networks and the group connectivity, as well as suppression of group-level connectivities towards zero. It performs well on the small datasets with no subject variability, although the strongly sparse Bayesian hierarchical model produces better estimates on datasets 1–3 (albeit by a very small margin). On datasets 5–10, SGGM is beaten by both hierarchical Bayesian models.</p>
        <p id="p0415">We also evaluated each model's ability to discover the underlying structure of the simulated GGMs. <xref rid="fig3" ref-type="fig">Fig. 3</xref> shows the area under the ROC curve for each model, indicating its ability to identify the GGM of each dataset. In general, Bayesian models with explicit sparse priors (the <italic>G</italic>-Wishart and the sparse HIPPO models) outperform Bayesian models with continuous priors. Ng et al.'s SGGM is the best-performing non-Bayesian model. It underperforms relative to the HIPPO models on datasets 6, 8 and 10, but outperforms them on datasets 1 and 9, giving no clear overall best performer.<fig id="fig3"><label>Fig. 3</label><caption><p>Comparison of network modelling methods' ability to discover network structure. The area under the ROC curve for discovery of the underlying graph structure is shown for 16 models (indexed by colour) applied to 10 multi-subject artificial datasets. Bars indicate the mean area under the curve over subjects; the standard deviation over subjects is given by the dark red line. All subjects in these datasets share the same network structure, so the models fit to the concatenated data perform well. Apart from these, the sparse hierarchcal models (both Bayesian and not) generally outperform the rest. A score of 1.0 for a particular model indicates that there exists a threshold that perfectly identifies the network graph when applied to the inferred connections. A score of 0.5 indicates no better performance than chance.</p></caption><alt-text id="alttext0035">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
        <p id="p0420">No results are given for the fused GLASSO on the largest dataset, number 5, or for the MGGM approach on datasets other than the first, because inference times exceeded a week, without convergence.</p>
        <p id="p0425">In summary, these simulations demonstrate that more accurate individual-subject connectivity estimation is possible with our hierarchical Bayesian framework than with the existing approaches. Discovery of graphical model network structure, too, is at least as good as the state of the art. Close competition comes from the SGGM approach, which, although not Bayesian, has a very similar hierarchical model structure.</p>
      </sec>
      <sec id="sec4.2">
        <title>Network estimation from limited datasets</title>
        <p id="p0430">A useful metric for assessing improvements to network estimation using real data is the ability of a model to estimate connectivity from a short section of a recording. We compare beta-band network matrices inferred from resting-state MEG recordings from the HCP, using either all three sessions of data, or only a single 6-min session of data. Additionally, we compare fMRI network matrices inferred from resting-state HCP data, using either all four sessions of 15 min, or only the first 5 min of recording. Treating networks inferred from the full datasets using mild Tikhonov regularisation (<italic>λ</italic> = 0.01) as a good approximation of ‘the truth,’ <xref rid="fig4" ref-type="fig">Fig. 4</xref> compares the RMS differences to these estimates from seven inference methods, which only had access to the first portion of the data.<fig id="fig4"><label>Fig. 4</label><caption><p>Estimation of network matrices from small samples of data. Single-subject networks estimated from the first 5 min of resting-state fMRI data (A), and single-subject networks estimated from the first resting-state session of MEG recordings in the beta band (B), were compared to the average of each subject's complete data, for each modality. Networks were estimated using the strongly and weakly sparse hierarchical model HIPPO, Ng et al.'s SGGM, with the graphical LASSO, Varoquaux et al.'s group GLASSO, with Tikhonov regularisation and with unregularised partial correlation. The results were compared in each subject to a mildly Tikhonov-regularised estimate from all three sessions' concatenated data; the RMS error from this estimate is displayed as a percentage of the mean connectivity of each subject's network matrix. Coloured dots identify individual subjects. Black crosses denote the mean of each distribution.</p></caption><alt-text id="alttext0040">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
        <p id="p0435">In the fMRI data, the two Bayesian hierarchical models (HIPPO) and the hierarchically-structured SGGM significantly outperform the standard regularised solutions, producing a reduction in error that is on the same order of magnitude as the subject-to-subject variation in this metric. They also outperform the group GLASSO of <xref rid="bib103" ref-type="bibr">Varoquaux et al. (2010)</xref>, which performed well on our simulated data. In the MEG data, the weakly sparse HIPPO model performs similarly well, while the strongly sparse Bayesian model and SGGM are not able to beat the group GLASSO.</p>
        <p id="p0440">A paired <italic>t</italic>-test for a difference in mean performance between the hierarchical models (including SGGM) and the group GLASSO, conducted non-parametrically using 5000 sign flips of the difference between pairs, gave <inline-formula><mml:math id="M126" altimg="si106.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula>0.001 for each hierarchical model in the fMRI data, and <inline-formula><mml:math id="M127" altimg="si106.gif" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo></mml:mrow></mml:math></inline-formula>0.01 for the weakly sparse hierarchical model in the MEG data, without adjusting for the multiple tests. These results equate to a mean improvement in estimation compared to GLASSO, with standard deviation over subjects, of 18<inline-formula><mml:math id="M128" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>6% (0.7<inline-formula><mml:math id="M129" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>7%) for the sparse HIPPO model, 19<inline-formula><mml:math id="M130" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>5% (14<inline-formula><mml:math id="M131" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>6%) for the weakly sparse HIPPO model, 20<inline-formula><mml:math id="M132" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>4% (6<inline-formula><mml:math id="M133" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>7%) for sggm and 8<inline-formula><mml:math id="M134" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>2% (14<inline-formula><mml:math id="M135" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>6%) for the group GLASSO on the fMRI (MEG) data.</p>
        <p id="p0445">It is worth noting that the weakly sparse hierarchical model can beat the original GLASSO, in both modalities, even when the regularisation parameter for the latter is chosen <italic>with knowledge of the correct solution</italic>—in other words, when it is allowed to cheat. If <italic>λ</italic> is chosen for each subject so as to minimise the difference between the estimated network and the solution used here as truth, the proportional error with standard deviation over subjects for the GLASSO estimate is 82<inline-formula><mml:math id="M136" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>8% for fMRI and 83<inline-formula><mml:math id="M137" altimg="si107.gif" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>7% for MEG. The hierarchical Bayesian model can therefore reduce network estimation error to an extent that is better than GLASSO would ever be able to achieve.</p>
        <p id="p0450">The fused graphical LASSO of Danaher et al. and Peterson et al.'s MGGM approach, other models that performed well on simulations, did not approach convergence on these short datasets even after five days of computation on a MacBook Pro (with a 2.8 GHz processor and 16 GB of RAM). Cross-validation of the parameters and computation of a solution was therefore unachievable in a sensible time frame, and results from these methods are not available for comparison.</p>
      </sec>
      <sec id="sec4.3">
        <title>Heritability of MEG functional connectivity</title>
        <p id="p0455">To further illustrate the performance of the hierarchical models, we repeated a previous analysis of the heritability of functional connectivity with HCP data (<xref rid="bib17" ref-type="bibr">Colclough et al., 2017</xref>) using the best-performing models from our evaluations: the two Bayesian hierarchical models and sggm. The hcp dataset is a twin study, and the variability within the functional connectomes of the subjects is determined, in part, by genetic and shared environmental effects (<xref rid="bib17" ref-type="bibr">Colclough et al., 2017</xref>). Heritability, <inline-formula><mml:math id="M138" altimg="si103.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (A), is the proportion of variance in a phenotype that can be explained by additive genetic factors. It is estimated using linear decompositions of the variance into heritability, the environmental effect shared between twins, <inline-formula><mml:math id="M139" altimg="si104.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (C), and any other unmodelled variance sources and noise, <inline-formula><mml:math id="M140" altimg="si105.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (E). Improving the quality of network matrix estimation, therefore, with a hierarchical model that is blind to the twin structure of the data, should reduce estimates of <inline-formula><mml:math id="M141" altimg="si105.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and increase estimates of heritability.</p>
        <p id="p0460">We fitted ACE models on each edge, and analysed the average heritability over the edges, computing bootstrapped confidence intervals and permutation-based tests for significance (results shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>B). To allow easy comparison to the previous work, we fitted the models to correlation matrices: those estimated from the sample covariance matrix, from the SGGM model, and correlation matrices estimated from inversion of precision matrices, regularised using the hierarchical inference procedure. Using the hierarchical models, the estimates of heritability (with 95% confidence intervals in square brackets) increased from 16% [11%, 22%] (original estimate) to 22% [15%, 32%] (weakly sparse HIPPO), 23% [16%, 35%] (sparse HIPPO) and 24% [15%, 38%] (SGGM). (Uncorrected permutation-based <italic>p</italic>-values computed for each respective model are 0.01, 0.01, 0.003 and 0.02.) This increase in heritability is related to a reduction in the residual variance (noise and any other factors unexplained by genetics, shared environmental, maternal effects, motion, age, sex or brain size) of 7 percentage points for the Bayesian models and 10 percentage points for SGGM, from an original estimate of 76% for <inline-formula><mml:math id="M142" altimg="si105.gif" overflow="scroll"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. This corresponds to a noise reduction of about 10% by using these hierarchical models. These differences are not explained by any random variation in noise or sampling, as exactly the same data were passed into each of the covariance models. Group mean networks for the four models are given in the supplementary information.<fig id="fig5"><label>Fig. 5</label><caption><p>Performance of the hierarchical model. (A) <italic>Prediction of sex (top) and picture vocabulary scores (bottom) from fMRI functional connectomes.</italic> The correlation between scores on a picture vocabulary test and the predicted scores after training a regression model using resting-state fMRI functional connection strengths is shown, together with the accuracy in prediction of subjects' sex using a similar logistic regression model. Results are presented for standard Tikhonov-regularised network inference, and for networks inferred using the weakly sparse and strongly sparse versions of the hierarchical model (HIPPO), and for the hierarchically-structured SGGM. Black crosses indicate results from each of five cross-validation folds; the orange circle indicates the mean of these scores. (B) <italic>Estimation of the mean heritability of functional connections in the MEG beta band.</italic> Heritability estimates are compared between the two versions of the Bayesian hierarchical model, SGGM, and estimation using sample covariance matrices, as performed for <xref rid="bib17" ref-type="bibr">Colclough et al. (2017)</xref>. Bars give the estimated proportion of variability attributed to additive genetics, on average over all connections, and the error lines denote 95% bootstrapped confidence intervals.</p></caption><alt-text id="alttext0045">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
      </sec>
      <sec id="sec4.4">
        <title>Trait prediction using fMRI functional connectivity</title>
        <p id="p0465">The HCP has released an analysis of the ability of functional connections to predict a wide range of biological and behavioural traits,<xref rid="fn16" ref-type="fn">16</xref> using all 841 subjects with resting-state fMRI recordings, suggesting that in some cases there is discriminative information embedded within the functional connectome. To illustrate the application of hierarchical models to predictive modelling, we target two of the megatrawl's more successful traits, scores on a picture vocabulary test (a measure of crystallised intelligence<xref rid="fn17" ref-type="fn">17</xref>), and sex. <xref rid="fig5" ref-type="fig">Fig. 5</xref>A presents a comparison of predictive performance on these two measures using partial correlation networks estimated using Tikhonov regularisation (the algorithm employed in the HCP's disseminated networks), SGGM, and the (posterior mean) partial correlation networks inferred using the strongly sparse and weakly sparse hierarchical models. In all cases, the differences in predictive ability between the models is smaller than the error on the cross-validated estimate (although there is no difference in sampling or random variation between the methods). The correlation between scores on the picture vocabulary test and the predicted responses are slightly worse (by a few percent) for the hierarchical models, whereas there is a slight improvement in accuracy for the prediction of biological sex (although the accuracies are so close to 0.5 that it is difficult to have confidence in the performance of any model). We were principally interested in the differential prediction ability of the models, so permutation tests to look for significant classification have not been performed.</p>
        <p id="p0470">The group average functional connectivities inferred by the four models are very similar (too close to see major differences when connection strengths are displayed on a heat map). The (posterior mean) group-average partial correlation network for the sparse hierarchical model HIPPO is shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>.<xref rid="fn18" ref-type="fn">18</xref> The posterior for the edge inclusion variables gave very high probabilities (over 99%) for all connections, presumably because the quantity of data in an hour's total recording time is sufficient to provide evidence for connectivity between all nodes, even if this connectivity is small in some cases. This point is explored further in the supplementary information, section <xref rid="appsec2">C.3</xref>.<fig id="fig6"><label>Fig. 6</label><caption><p>Group average functional network for the HCP fMRI data. Posterior mean of the group average partial correlation network, computed using the hierarchical model (HIPPO). The results obtained using the Tikhonov-regularised, SGGM, or weakly sparse HIPPO models are visually identical. Numbers and brain slices indicate the ICA components which act as network nodes. The width and colour of the connections indicate the strength of the partial correlations (red for positive correlations, blue for negative).</p></caption><alt-text id="alttext0050">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
      </sec>
    </sec>
    <sec id="sec5">
      <title>Discussion</title>
      <p id="p0475">We have presented two hierarchical models for the functional connectivity measured with EEG, MEG or fMRI. One uses continuous priors to regularise the estimation of weak connectivities. The second explicitly promotes a sparse network structure, and provides posterior probabilities of a connection on each network edge. Both models characterise connectivity by the partial correlation between activations in ROIs, and jointly infer connection strengths for individual subjects and the population average. This ability to perform joint inference at both levels of the hierarchy, sharing information between subjects and regularising connection strengths towards the group mean, is an innovation in Bayesian covariance modelling, previously only possible for functions of linear effects (<xref rid="bib34" ref-type="bibr">Gelman et al., 2014</xref>).</p>
      <p id="p0480">Accurate estimation of precision and covariance matrices is difficult and noisy. Most techniques designed to address this problem regularise weak elements of the matrices towards zero with some sparsity-promoting scheme. The importance of estimating precision matrices with some form of regularisation is clear in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, where over many different datasets, even a simple approach like Tikhonov regularisation or GLASSO can reduce the reconstruction error by a third to a half. In simulations where we introduced some between-subject variability, the hierarchical Bayesian models outperform a wide range of methods that represent the state of the art in inverse covariance modelling. The non-Bayesian SGGM performs nearly as well. We note that this method has a very similar hierarchical structure to our ‘weakly sparse,’ continuous-prior Bayesian model, with terms designed to regularise subject connectivities towards the group and the group connectivities towards zero.</p>
      <p id="p0485">Hierarchical model structures and the partial pooling of information over subjects can be most useful when limited data are available within each subject (<xref rid="bib34" ref-type="bibr">Gelman et al., 2014</xref>). Compare, for example, the improvement in matrix reconstruction for simulations 2–4, in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, for which the number of subjects and network nodes remains constant, but the amount of data available within each subject increases. The hierarchical models do well in each case, but the differential improvement over more basic models is largest for the case with the least data. We observe the same effect in our studies of real data. Network estimation with a limited subset of both fMRI and MEG recordings can be greatly improved using the hierarchical models (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>). The key point is that the quality of single-subject connectivity inference is enhanced, using commonalities between subjects to reduce the noise within each. Thus, estimates of the heritability of functional connectivity using meg beta-band data are increased (<xref rid="fig5" ref-type="fig">Fig. 5</xref>B), because a portion of the noisy variability within the dataset is reduced.</p>
      <p id="p0490">There are two areas in our results where the hierarchical models are not the top performers. Despite their success in the MEG heritability analyses, SGGM and the strongly sparse HIPPO model give mediocre results when applied to the limited subset of MEG recordings (<xref rid="fig4" ref-type="fig">Fig. 4</xref>B). However, meg networks are very noisy to estimate in comparison to fMRI (<xref rid="bib16" ref-type="bibr">Colclough et al., 2016</xref>). The level of scan-to-scan variability may mean that the combination of three sessions that form our ‘ground truth’ is still not enough data to build a representative picture of each subject's functional connectivity, thereby skewing our results. The other area is in the quality of biological and behavioural trait prediction using functional networks estimated from fMRI data. It is possible that functional connectivity encodes very little information that can be extracted by a linear model about subjects' sex or their scores on picture vocabulary tests (<xref rid="bib4" ref-type="bibr">Bijsterbosch et al., 2018</xref>). Alternatively, it may simply be that the quantity of data may be dominating the prior, such that the hierarchical model provides little improvement over simple estimates. In the HCP dataset, there are vastly more data available for inference in fMRI than for meg: the total recording time for fMRI is 1 h, for MEG 18 min, and in our analyses we estimate networks with 25 nodes in the former case and 39 in the latter. (Sampling rates are comparable across the two modalities because we apply our network models to the down-sampled power envelopes of MEG recordings.) We discuss this issue, and illustrate it with a simulated example, in <xref rid="appsec2">supplementary section C.3</xref>.</p>
      <p id="p0495">Sparsity in functional connectivity matrices provides not only a mechanism to improve noisy estimates, but can also improve the interpretability of the networks. Our strongly sparse hierarchical model offers an analyst the ability to draw samples from the approximate posterior distribution of the graph representing the network structure of their dataset. They would then be able to construct posterior summaries of any function of that graph, <inline-formula><mml:math id="M143" altimg="si108.gif" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This idea was termed <italic>Bayesian connectomics</italic> when it was developed for structural connectivity by Janssen et al. in <xref rid="bib52" ref-type="bibr">2014</xref>. Using this fully probabilistic description of the network connections and their properties would be preferable to testing graph theory metrics (such as degree centrality or measures of ‘small world’ properties) over many binary network matrices created with a sliding scale of thresholds, as is currently common practice. However, while using MCMC chains to average over different models can provide effective regularisation of the parameter estimates, making inferences about graph theoretic functions of the network structure requires two conditions to be met. The first is basic, in that the analyst must be confident that they have run the sampling chains for long enough to have obtained a fair representation of the posterior. (George &amp; McCulloch caution that the parameter space is so enormous that a sampler can at best ‘search for promising models, rather than compute the whole posterior.’) The second condition is that they must believe that a network model with a shared sparsity structure across subjects is a good representation of the data. We turn to this second assumption now.</p>
      <p id="p0500">The sparse hierarchical model we present expressly shares the sparsity structure over all subjects: the network structure is therefore considered a property of the entire population, about which no subject is considered to deviate. This may be plausible, particularly if analyses are restricted to sub-populations in which this assumption holds (fitting the model separately to patients and healthy controls, for example). However, it is also not clear that any sparsity in functional networks is an accurate biophysical assumption. We might expect some level of measurable connectivity between all brain regions, even if this level is small. There is support for this view from a recent tract-tracing study (<xref rid="bib45" ref-type="bibr">Gămănut et al., 2018</xref>), and we note that inference using the large fMRI dataset gave evidence overwhelmingly in favour of the full model—that is, the model with all connections present. (This observation may however just be a consequence of the amount of data available, as discussed in <xref rid="appsec2">supplementary section C.3</xref> and <xref rid="bib91" ref-type="bibr">Smith and Nichols, 2018</xref>.) We must also be cautious in drawing strong conclusions from the estimated graph structure, as other failures of our assumptions, such as of undirected network influences, linearity of the system or of network stationarity, may lead to over-confident identification of some connections in the sparse network. As a result, we suggest that the sparse model structure can be used for effective regularisation of connectivity estimation, but that further interpretation of the network structure be performed with care.</p>
      <p id="p0505">Which models can we recommend for connectivity estimation? A number are ruled out on inference time alone. The fused GLASSO and MGGM were not practical to run on our real-world examples; and the best Bayesian sparse model for individual connectivity estimation, the <italic>G</italic>-Wishart distribution, is impractical for use with moderate numbers of subjects, or even for single functional networks with 50 or more nodes. Between the strongly and weakly sparse versions of our HIPPO model, there was not a clear differentiator in terms of performance, or even in their ability to detect network edges (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). However, in the weakly sparse model, the need for convergence of the posterior over the edge inclusion variables <inline-formula><mml:math id="M144" altimg="si43.gif" overflow="scroll"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula> (which exhibit highly autocorrelated behaviour) is removed, so the total run time need not be as long. For example, the fMRI and MEG results were produced using parallel sampling chains that took about 14 h to run (for each chain) for the weakly sparse model, and 20 h for the sparse model. In comparison, the best non-Bayesian solution, SGGM, is very fast to run, although the search for optimal hyper-parameters using cross-validation can extend inference times to several hours on our datasets. sggm performs nearly as well in our simulations, and just as well in our real-data examples, as the Bayesian hierarchical models. For inference of individual subjects' connectivities, therefore, we would recommend either SGGM or our own HIPPO approach.</p>
      <p id="p0510">Our models do, however, pave the way for further development of connectivity modelling, and not just in the flexibility of the sparsity-promoting priors that can be accommodated. It would be simple to incorporate uncertainty over ROI time course estimation into our algorithm, using a Bayesian description of the parcellation process. Our approach could be further integrated with a more complex model in order to simultaneously infer both the parcellations and sparse functional networks, for example by extending the work of <xref rid="bib46" ref-type="bibr">Harrison et al. (2015)</xref>. Outside of neuroimaging, graphical model determination and covariance modelling are important techniques in financial analyses, protein network determination and gene expression modelling. Our hierarchical inference structure could also be applied to improve network estimation in these fields.</p>
      <p id="p0515">In conclusion, we have presented an advance in functional connectivity and inverse covariance modelling, by designing hierarchical Bayesian models for the distribution of connection strengths in subjects set within a wider group or population. We have demonstrated that hierarchical models, both our Bayesian approach and Ng et al.'s SGGM, are the best available choices for partial correlation models of functional networks. These models improve the quality of single-subject network estimates, particularly in small or noisy datasets, with concomitant increases in sensitivity to properties of interest (such as heritability) in the functional connectomes. Our Bayesian inference program, HIPPO, is sufficiently scalable to allow it to be applied to conventional neuroimaging datasets. The models are applicable both to fMRI and to MEG data, and we hope they will enable improved inference for studies in both modalities.</p>
      <sec id="sec5.1">
        <title>Competing interests</title>
        <p id="p0520">S.S. is part-owner and shareholder of SBGneuro.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list id="cebib0010">
      <title>References</title>
      <ref id="bib1">
        <element-citation publication-type="journal" id="sref1">
          <person-group person-group-type="author">
            <name>
              <surname>Baker</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>Fast transient networks in spontaneous human brain activity</article-title>
          <source>eLife</source>
          <volume>3</volume>
          <year>2014</year>
          <comment>e01 867</comment>
        </element-citation>
      </ref>
      <ref id="bib4">
        <element-citation publication-type="journal" id="sref4">
          <person-group person-group-type="author">
            <name>
              <surname>Bijsterbosch</surname>
              <given-names>J.D.</given-names>
            </name>
          </person-group>
          <article-title>The relationship between spatial configuration and functional connectivity of brain regions</article-title>
          <source>eLife</source>
          <volume>7</volume>
          <year>2018</year>
          <comment>e32 992</comment>
        </element-citation>
      </ref>
      <ref id="bib5">
        <element-citation publication-type="book" id="sref5">
          <person-group person-group-type="author">
            <name>
              <surname>Boyd</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Vandenberghe</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <chapter-title>Convex Optimization</chapter-title>
          <year>2004</year>
          <publisher-name>Cambridge University Press</publisher-name>
          <publisher-loc>Cambridge, UK</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib6">
        <element-citation publication-type="journal" id="sref6">
          <person-group person-group-type="author">
            <name>
              <surname>Brookes</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Investigating the electrophysiological basis of resting state networks using magnetoencephalography</article-title>
          <source>Proc. Natl. Acad. Sci. U.S.A.</source>
          <volume>108</volume>
          <year>2011</year>
          <fpage>16 783</fpage>
          <lpage>16 788</lpage>
        </element-citation>
      </ref>
      <ref id="bib7">
        <element-citation publication-type="journal" id="sref7">
          <person-group person-group-type="author">
            <name>
              <surname>Brookes</surname>
              <given-names>M.J.</given-names>
            </name>
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
            <name>
              <surname>Barnes</surname>
              <given-names>G.R.</given-names>
            </name>
          </person-group>
          <article-title>Measuring functional connectivity in MEG: a multivariate approach insensitive to linear source leakage</article-title>
          <source>NeuroImage</source>
          <volume>63</volume>
          <issue>2</issue>
          <year>2012</year>
          <fpage>910</fpage>
          <lpage>920</lpage>
          <pub-id pub-id-type="pmid">22484306</pub-id>
        </element-citation>
      </ref>
      <ref id="bib8">
        <element-citation publication-type="journal" id="sref8">
          <person-group person-group-type="author">
            <name>
              <surname>Bullmore</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sporns</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title>
          <source>Nat. Rev. Neurosci.</source>
          <volume>10</volume>
          <issue>3</issue>
          <year>2009</year>
          <fpage>186</fpage>
          <lpage>198</lpage>
          <pub-id pub-id-type="pmid">19190637</pub-id>
        </element-citation>
      </ref>
      <ref id="bib10">
        <element-citation publication-type="journal" id="sref10">
          <person-group person-group-type="author">
            <name>
              <surname>Carvalho</surname>
              <given-names>C.M.</given-names>
            </name>
            <name>
              <surname>Polson</surname>
              <given-names>N.G.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>J.G.</given-names>
            </name>
          </person-group>
          <article-title>The horseshoe estimator for sparse signals</article-title>
          <source>Biometrika</source>
          <volume>97</volume>
          <issue>2</issue>
          <year>2010</year>
          <fpage>465</fpage>
          <lpage>480</lpage>
        </element-citation>
      </ref>
      <ref id="bib12">
        <element-citation publication-type="book" id="sref12">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <chapter-title>Accelerated Estimation and Inference for Heritability of FMRI Data</chapter-title>
          <comment>Ph.D. Thesis</comment>
          <year>2014</year>
          <publisher-name>University of Warwick</publisher-name>
          <publisher-loc>Warwick, UK</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib13">
        <element-citation publication-type="book" id="sref13">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <chapter-title>APACE: accelerated permutation inference for the ACE model</chapter-title>
          <source>The 20th Annual Meeting of the Organization for Human Brain Mapping</source>
          <year>2014</year>
          <publisher-name>OHBM</publisher-name>
          <publisher-loc>Hamburg, Germany</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib15">
        <element-citation publication-type="journal" id="sref15">
          <person-group person-group-type="author">
            <name>
              <surname>Colclough</surname>
              <given-names>G.L.</given-names>
            </name>
          </person-group>
          <article-title>A symmetric multivariate leakage correction for MEG connectomes</article-title>
          <source>NeuroImage</source>
          <volume>117</volume>
          <year>2015</year>
          <fpage>439</fpage>
          <lpage>448</lpage>
          <pub-id pub-id-type="pmid">25862259</pub-id>
        </element-citation>
      </ref>
      <ref id="bib16">
        <element-citation publication-type="journal" id="sref16">
          <person-group person-group-type="author">
            <name>
              <surname>Colclough</surname>
              <given-names>G.L.</given-names>
            </name>
          </person-group>
          <article-title>How reliable are MEG resting-state connectivity metrics?</article-title>
          <source>NeuroImage</source>
          <volume>138</volume>
          <year>2016</year>
          <fpage>284</fpage>
          <lpage>293</lpage>
          <pub-id pub-id-type="pmid">27262239</pub-id>
        </element-citation>
      </ref>
      <ref id="bib17">
        <element-citation publication-type="journal" id="sref17">
          <person-group person-group-type="author">
            <name>
              <surname>Colclough</surname>
              <given-names>G.L.</given-names>
            </name>
          </person-group>
          <article-title>The heritability of multi-modal connectivity in human brain activity</article-title>
          <source>eLife</source>
          <volume>6</volume>
          <year>2017</year>
          <comment>e20 178</comment>
        </element-citation>
      </ref>
      <ref id="bib18">
        <element-citation publication-type="journal" id="sref18">
          <person-group person-group-type="author">
            <name>
              <surname>Danaher</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Witten</surname>
              <given-names>D.M.</given-names>
            </name>
          </person-group>
          <article-title>The joint graphical lasso for inverse covariance estimation across multiple classes</article-title>
          <source>J. R. Stat. Soc. B</source>
          <volume>76</volume>
          <issue>2</issue>
          <year>2015</year>
          <fpage>373</fpage>
          <lpage>397</lpage>
        </element-citation>
      </ref>
      <ref id="bib19">
        <element-citation publication-type="journal" id="sref19">
          <person-group person-group-type="author">
            <name>
              <surname>Dempster</surname>
              <given-names>A.P.</given-names>
            </name>
          </person-group>
          <article-title>Covariance selection</article-title>
          <source>Biometrics</source>
          <volume>28</volume>
          <issue>1</issue>
          <year>1972</year>
          <fpage>157</fpage>
          <lpage>175</lpage>
        </element-citation>
      </ref>
      <ref id="bib20">
        <element-citation publication-type="book" id="sref20">
          <person-group person-group-type="author">
            <name>
              <surname>Duff</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <chapter-title>Utility of partial correlation for characterising brain dynamics: MVPA-based assessment of regularisation and network selection</chapter-title>
          <source>3rd International Workshop on Pattern Recognition in Neuroimaging</source>
          <year>2013</year>
          <publisher-name>PRNI</publisher-name>
          <publisher-loc>Philadelphia, PA</publisher-loc>
          <fpage>58</fpage>
          <lpage>61</lpage>
        </element-citation>
      </ref>
      <ref id="bib22">
        <element-citation publication-type="journal" id="sref22">
          <person-group person-group-type="author">
            <name>
              <surname>Van Essen</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>The WU-Minn human connetome project: an overview</article-title>
          <source>NeuroImage</source>
          <volume>80</volume>
          <year>2013</year>
          <fpage>62</fpage>
          <lpage>97</lpage>
          <pub-id pub-id-type="pmid">23684880</pub-id>
        </element-citation>
      </ref>
      <ref id="bib23">
        <element-citation publication-type="journal" id="sref23">
          <person-group person-group-type="author">
            <name>
              <surname>Felleman</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Van Essen</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>
          <source>Cereb. Cortex</source>
          <volume>1</volume>
          <issue>1</issue>
          <year>1991</year>
          <fpage>1</fpage>
          <lpage>47</lpage>
          <pub-id pub-id-type="pmid">1822724</pub-id>
        </element-citation>
      </ref>
      <ref id="bib25">
        <element-citation publication-type="journal" id="sref25">
          <person-group person-group-type="author">
            <name>
              <surname>Finn</surname>
              <given-names>E.S.</given-names>
            </name>
          </person-group>
          <article-title>Functional connectome fingerprinting: identifying individuals using patterns of brain connectivity</article-title>
          <source>Nat. Neurosci.</source>
          <volume>18</volume>
          <year>2015</year>
          <fpage>1664</fpage>
          <lpage>1671</lpage>
          <pub-id pub-id-type="pmid">26457551</pub-id>
        </element-citation>
      </ref>
      <ref id="bib26">
        <element-citation publication-type="journal" id="sref26">
          <person-group person-group-type="author">
            <name>
              <surname>Fisher</surname>
              <given-names>R.A.</given-names>
            </name>
          </person-group>
          <article-title>The distribution of the partial correlation coefficient</article-title>
          <source>Metron</source>
          <volume>3</volume>
          <year>1924</year>
          <fpage>329</fpage>
          <lpage>332</lpage>
        </element-citation>
      </ref>
      <ref id="bib27">
        <element-citation publication-type="journal" id="sref27">
          <person-group person-group-type="author">
            <name>
              <surname>Friedman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hastie</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Tibshirani</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Sparse inverse covariance estimation with the graphical lasso</article-title>
          <source>Biostatistics</source>
          <volume>9</volume>
          <issue>3</issue>
          <year>2008</year>
          <fpage>432</fpage>
          <lpage>441</lpage>
          <pub-id pub-id-type="pmid">18079126</pub-id>
        </element-citation>
      </ref>
      <ref id="bib28">
        <element-citation publication-type="journal" id="sref28">
          <person-group person-group-type="author">
            <name>
              <surname>Friedman</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hastie</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Tibshirani</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>
          <source>J. Stat. Softw.</source>
          <volume>33</volume>
          <issue>1</issue>
          <year>2010</year>
        </element-citation>
      </ref>
      <ref id="bib29">
        <element-citation publication-type="journal" id="sref29">
          <person-group person-group-type="author">
            <name>
              <surname>Friston</surname>
              <given-names>K.J.</given-names>
            </name>
          </person-group>
          <article-title>Functional and effective connectivity: a review</article-title>
          <source>Brain Connect.</source>
          <volume>1</volume>
          <issue>1</issue>
          <year>2011</year>
          <fpage>13</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="pmid">22432952</pub-id>
        </element-citation>
      </ref>
      <ref id="bib30">
        <element-citation publication-type="journal" id="sref30">
          <person-group person-group-type="author">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Prior distributions for variance parameters in hierarchical models</article-title>
          <source>Bayesian Anal.</source>
          <volume>1</volume>
          <issue>3</issue>
          <year>2006</year>
          <fpage>515</fpage>
          <lpage>533</lpage>
        </element-citation>
      </ref>
      <ref id="bib33">
        <element-citation publication-type="journal" id="sref33">
          <person-group person-group-type="author">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Using redundant parameterizations to fit hierarchical models</article-title>
          <source>J. Comput. Graph. Stat.</source>
          <volume>17</volume>
          <issue>1</issue>
          <year>2008</year>
          <fpage>95</fpage>
          <lpage>122</lpage>
        </element-citation>
      </ref>
      <ref id="bib34">
        <element-citation publication-type="book" id="sref34">
          <person-group person-group-type="author">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <chapter-title>Bayesian Data Analysis</chapter-title>
          <edition>third ed.</edition>
          <year>2014</year>
          <publisher-name>CRC Press, Taylor &amp; Francis Group</publisher-name>
          <publisher-loc>Boca Raton, FL</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib37">
        <element-citation publication-type="journal" id="sref37">
          <person-group person-group-type="author">
            <name>
              <surname>Glahn</surname>
              <given-names>D.C.</given-names>
            </name>
          </person-group>
          <article-title>Genetic control over the resting brain</article-title>
          <source>Proc. Natl. Acad. Sci. U.S.A.</source>
          <volume>107</volume>
          <issue>3</issue>
          <year>2010</year>
          <fpage>1223</fpage>
          <lpage>1228</lpage>
          <pub-id pub-id-type="pmid">20133824</pub-id>
        </element-citation>
      </ref>
      <ref id="bib38">
        <element-citation publication-type="journal" id="sref38">
          <person-group person-group-type="author">
            <name>
              <surname>Glasser</surname>
              <given-names>M.F.</given-names>
            </name>
          </person-group>
          <article-title>The minimal preprocesing pipelines for the Human Connectome Project</article-title>
          <source>NeuroImage</source>
          <volume>80</volume>
          <year>2013</year>
          <fpage>105</fpage>
          <lpage>124</lpage>
          <pub-id pub-id-type="pmid">23668970</pub-id>
        </element-citation>
      </ref>
      <ref id="bib39">
        <element-citation publication-type="journal" id="sref39">
          <person-group person-group-type="author">
            <name>
              <surname>Glasser</surname>
              <given-names>M.F.</given-names>
            </name>
          </person-group>
          <article-title>A multi-modal parcellation of human cerebral cortex</article-title>
          <source>Nature</source>
          <volume>536</volume>
          <year>2016</year>
          <fpage>171</fpage>
          <lpage>178</lpage>
          <pub-id pub-id-type="pmid">27437579</pub-id>
        </element-citation>
      </ref>
      <ref id="bib41">
        <element-citation publication-type="journal" id="sref41">
          <person-group person-group-type="author">
            <name>
              <surname>Greicius</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>Resting-state functional connectivity in neuropsychiatric disorders</article-title>
          <source>Curr. Opin. Neurol.</source>
          <volume>21</volume>
          <year>2008</year>
          <fpage>424</fpage>
          <lpage>430</lpage>
          <pub-id pub-id-type="pmid">18607202</pub-id>
        </element-citation>
      </ref>
      <ref id="bib44">
        <element-citation publication-type="journal" id="sref44">
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Joint estimation of multiple graphical models</article-title>
          <source>Biometrika</source>
          <volume>98</volume>
          <issue>1</issue>
          <year>2011</year>
          <fpage>1</fpage>
          <lpage>15</lpage>
          <pub-id pub-id-type="pmid">23049124</pub-id>
        </element-citation>
      </ref>
      <ref id="bib45">
        <element-citation publication-type="journal" id="sref45">
          <person-group person-group-type="author">
            <name>
              <surname>Gămănut</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>The mouse cortical connectome, characterized by an ultra-dense cortical graph, maintains specificity by distinct connectivity profiles</article-title>
          <source>Neuron</source>
          <volume>97</volume>
          <issue>2</issue>
          <year>2018</year>
          <fpage>698</fpage>
          <lpage>715</lpage>
          <pub-id pub-id-type="pmid">29420935</pub-id>
        </element-citation>
      </ref>
      <ref id="bib46">
        <element-citation publication-type="journal" id="sref46">
          <person-group person-group-type="author">
            <name>
              <surname>Harrison</surname>
              <given-names>S.J.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale probabilistic functional modes from resting-state fMRI</article-title>
          <source>NeuroImage</source>
          <volume>109</volume>
          <year>2015</year>
          <fpage>217</fpage>
          <lpage>231</lpage>
          <pub-id pub-id-type="pmid">25598050</pub-id>
        </element-citation>
      </ref>
      <ref id="bib47">
        <element-citation publication-type="journal" id="sref47">
          <person-group person-group-type="author">
            <name>
              <surname>Higham</surname>
              <given-names>N.J.</given-names>
            </name>
          </person-group>
          <article-title>Computing a nearest symmtetric positive semidefinite matrix</article-title>
          <source>Linear Algebra its Appl.</source>
          <volume>103</volume>
          <issue>103–118</issue>
          <year>1988</year>
        </element-citation>
      </ref>
      <ref id="bib48">
        <element-citation publication-type="journal" id="sref48">
          <person-group person-group-type="author">
            <name>
              <surname>Hinne</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian inference of structural brain networks</article-title>
          <source>NeuroImage</source>
          <volume>66</volume>
          <year>2013</year>
          <fpage>543</fpage>
          <lpage>552</lpage>
          <pub-id pub-id-type="pmid">23041334</pub-id>
        </element-citation>
      </ref>
      <ref id="bib49">
        <element-citation publication-type="journal" id="sref49">
          <person-group person-group-type="author">
            <name>
              <surname>Hinne</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Efficient sampling of Gaussian graphical models using conditional Bayes factors</article-title>
          <source>Stat</source>
          <volume>3</volume>
          <year>2014</year>
          <fpage>326</fpage>
          <lpage>336</lpage>
        </element-citation>
      </ref>
      <ref id="bib50">
        <element-citation publication-type="journal" id="sref50">
          <person-group person-group-type="author">
            <name>
              <surname>Hinne</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian estimation of conditional independence graphs improves functional connectivity estimates</article-title>
          <source>PLoS Comput. Biol.</source>
          <year>2015</year>
        </element-citation>
      </ref>
      <ref id="bib51">
        <element-citation publication-type="journal" id="sref51">
          <person-group person-group-type="author">
            <name>
              <surname>Hipp</surname>
              <given-names>J.F.</given-names>
            </name>
          </person-group>
          <article-title>Large-scale cortical correlation structure of spontaneous oscillatory activity</article-title>
          <source>Nat. Neurosci.</source>
          <volume>15</volume>
          <issue>6</issue>
          <year>2012</year>
          <fpage>884</fpage>
          <lpage>890</lpage>
          <pub-id pub-id-type="pmid">22561454</pub-id>
        </element-citation>
      </ref>
      <ref id="bib52">
        <element-citation publication-type="journal" id="sref52">
          <person-group person-group-type="author">
            <name>
              <surname>Janssen</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <article-title>Quantifying uncertainty in brain network measures using Bayesian connectomics</article-title>
          <source>Front. Comput. Neurosci.</source>
          <volume>8</volume>
          <issue>126</issue>
          <year>2014</year>
        </element-citation>
      </ref>
      <ref id="bib53">
        <element-citation publication-type="journal" id="sref53">
          <person-group person-group-type="author">
            <name>
              <surname>Kerman</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Neutral noninformative and informative conjugate beta and gamma prior distributions</article-title>
          <source>Electron. J. Stat.</source>
          <volume>5</volume>
          <year>2011</year>
          <fpage>1450</fpage>
          <lpage>1470</lpage>
        </element-citation>
      </ref>
      <ref id="bib54">
        <element-citation publication-type="journal" id="sref54">
          <person-group person-group-type="author">
            <name>
              <surname>Larson-Prior</surname>
              <given-names>L.J.</given-names>
            </name>
          </person-group>
          <article-title>Adding dynamics to the human connectome project with MEG</article-title>
          <source>NeuroImage</source>
          <volume>80</volume>
          <year>2013</year>
          <fpage>190</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="pmid">23702419</pub-id>
        </element-citation>
      </ref>
      <ref id="bib55">
        <element-citation publication-type="journal" id="sref55">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Joint estimation of multiple precision matrices with common structures</article-title>
          <source>J. Mach. Learn. Res.</source>
          <volume>16</volume>
          <year>2015</year>
          <fpage>1035</fpage>
          <lpage>1062</lpage>
          <pub-id pub-id-type="pmid">26568704</pub-id>
        </element-citation>
      </ref>
      <ref id="bib56">
        <element-citation publication-type="journal" id="sref56">
          <person-group person-group-type="author">
            <name>
              <surname>Lenkoski</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>A direct sampler for G-Wishart variates</article-title>
          <source>Stat</source>
          <volume>2</volume>
          <year>2013</year>
          <fpage>119</fpage>
          <lpage>128</lpage>
        </element-citation>
      </ref>
      <ref id="bib57">
        <element-citation publication-type="journal" id="sref57">
          <person-group person-group-type="author">
            <name>
              <surname>Letac</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Massam</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Wishart distributions for decomposable graphs</article-title>
          <source>Ann. Stat.</source>
          <volume>35</volume>
          <issue>3</issue>
          <year>2007</year>
          <fpage>1278</fpage>
          <lpage>1323</lpage>
        </element-citation>
      </ref>
      <ref id="bib58">
        <element-citation publication-type="journal" id="sref58">
          <person-group person-group-type="author">
            <name>
              <surname>Liang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Connelly</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Calamante</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>A novel joint sparse partial correlation method for estimating group functional networks</article-title>
          <source>Hum. Brain Mapp.</source>
          <volume>37</volume>
          <year>2016</year>
          <fpage>1162</fpage>
          <lpage>1177</lpage>
          <pub-id pub-id-type="pmid">26859311</pub-id>
        </element-citation>
      </ref>
      <ref id="bib60">
        <element-citation publication-type="journal" id="sref60">
          <person-group person-group-type="author">
            <name>
              <surname>Luckhoo</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Inferring task-related networks using independent component analysis in magnetoencephalography</article-title>
          <source>NeuroImage</source>
          <volume>62</volume>
          <year>2012</year>
          <fpage>530</fpage>
          <lpage>541</lpage>
          <pub-id pub-id-type="pmid">22569064</pub-id>
        </element-citation>
      </ref>
      <ref id="bib62">
        <element-citation publication-type="journal" id="sref62">
          <person-group person-group-type="author">
            <name>
              <surname>Mantini</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Electrophysiological signatures of resting state networks in the human brain</article-title>
          <source>Proc. Natl. Acad. Sci. U.S.A.</source>
          <volume>104</volume>
          <issue>32</issue>
          <year>2007</year>
          <fpage>13 170</fpage>
          <lpage>13 175</lpage>
        </element-citation>
      </ref>
      <ref id="bib63">
        <element-citation publication-type="journal" id="sref63">
          <person-group person-group-type="author">
            <name>
              <surname>Marrelec</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Partial correlation for functional brain interactivity investigation in functional MRI</article-title>
          <source>NeuroImage</source>
          <volume>32</volume>
          <issue>1</issue>
          <year>2006</year>
          <fpage>228</fpage>
          <lpage>237</lpage>
          <pub-id pub-id-type="pmid">16777436</pub-id>
        </element-citation>
      </ref>
      <ref id="bib64">
        <element-citation publication-type="journal" id="sref64">
          <person-group person-group-type="author">
            <name>
              <surname>Mazumder</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hastie</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Exact covariance thresholding into connected components for large-scale graphical lasso</article-title>
          <source>J. Mach. Learn. Res.</source>
          <volume>13</volume>
          <year>2012</year>
          <fpage>723</fpage>
          <lpage>726</lpage>
        </element-citation>
      </ref>
      <ref id="bib65">
        <element-citation publication-type="journal" id="sref65">
          <person-group person-group-type="author">
            <name>
              <surname>Mazumder</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hastie</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>The graphical lasso: new insights and alternatives</article-title>
          <source>Electron. J. Stat.</source>
          <volume>6</volume>
          <year>2012</year>
          <fpage>2125</fpage>
          <lpage>2149</lpage>
          <pub-id pub-id-type="pmid">25558297</pub-id>
        </element-citation>
      </ref>
      <ref id="bib66">
        <element-citation publication-type="journal" id="sref66">
          <person-group person-group-type="author">
            <name>
              <surname>Mejia</surname>
              <given-names>A.F.</given-names>
            </name>
          </person-group>
          <article-title>Improved estimation of subject-level functional connectivity using full and partial correlation with empirical Bayes shrinkage</article-title>
          <source>NeuroImage</source>
          <volume>172</volume>
          <year>2018</year>
          <fpage>478</fpage>
          <lpage>491</lpage>
          <pub-id pub-id-type="pmid">29391241</pub-id>
        </element-citation>
      </ref>
      <ref id="bib68">
        <element-citation publication-type="journal" id="sref68">
          <person-group person-group-type="author">
            <name>
              <surname>Mitchell</surname>
              <given-names>T.J.</given-names>
            </name>
            <name>
              <surname>Beauchamp</surname>
              <given-names>J.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian variable selection in linear regression</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>83</volume>
          <issue>404</issue>
          <year>1988</year>
          <fpage>1023</fpage>
          <lpage>1032</lpage>
        </element-citation>
      </ref>
      <ref id="bib69">
        <element-citation publication-type="journal" id="sref69">
          <person-group person-group-type="author">
            <name>
              <surname>Mohammadi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wit</surname>
              <given-names>E.C.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian structure learning in sparse Gaussian graphical models</article-title>
          <source>Bayesian Anal.</source>
          <volume>10</volume>
          <issue>1</issue>
          <year>2015</year>
          <fpage>109</fpage>
          <lpage>138</lpage>
        </element-citation>
      </ref>
      <ref id="bib70">
        <element-citation publication-type="book" id="sref70">
          <person-group person-group-type="author">
            <name>
              <surname>Nadkarni</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <chapter-title>Sparse plus low-rank graphical models of time series for functional connectivity in MEG. Submitted to <italic>Uncertainty in Artificial Intelligence</italic></chapter-title>
          <source>Presented at the 2nd KDD Workshop on Mining and Learning from Time Series in 2016</source>
          <year>2017</year>
          <comment>Available at:</comment>
          <ext-link ext-link-type="uri" xlink:href="http://www-bcf.usc.edu/%7Eliu32/milets16/paper/MiLeTS_2016_paper_22.pdf" id="intref0015">www-bcf.usc.edu/%7Eliu32/milets16/paper/MiLeTS_2016_paper_22.pdf</ext-link>
          <comment>[accessed 24th June 2017]</comment>
        </element-citation>
      </ref>
      <ref id="bib71">
        <element-citation publication-type="book" id="sref71">
          <person-group person-group-type="author">
            <name>
              <surname>Ng</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <chapter-title>A novel sparse group Gaussian graphical model for functional connectivity estimation</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Gee</surname>
              <given-names>J.C.</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Pohl</surname>
              <given-names>K.M.</given-names>
            </name>
            <name>
              <surname>Wells</surname>
              <given-names>W.M.</given-names>
            </name>
            <name>
              <surname>Zöllei</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <source>Information Processing in Medical Imaging, IPMI 2013, Volume 7917 of Lecture Notes in Computer Science</source>
          <year>2013</year>
          <publisher-name>Springer</publisher-name>
          <publisher-loc>Berlin, Heidelberg</publisher-loc>
          <fpage>256</fpage>
          <lpage>267</lpage>
        </element-citation>
      </ref>
      <ref id="bib73">
        <element-citation publication-type="journal" id="sref73">
          <person-group person-group-type="author">
            <name>
              <surname>de Pasquale</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>A cortical core for dynamic integration of functional networks in the resting human brain</article-title>
          <source>Neuron</source>
          <volume>74</volume>
          <issue>4</issue>
          <year>2012</year>
          <fpage>753</fpage>
          <lpage>764</lpage>
          <pub-id pub-id-type="pmid">22632732</pub-id>
        </element-citation>
      </ref>
      <ref id="bib74">
        <element-citation publication-type="journal" id="sref74">
          <person-group person-group-type="author">
            <name>
              <surname>de Pasquale</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>A dynamic core network and global efficiency in the resting human brain</article-title>
          <source>Cereberal Cortex</source>
          <volume>26</volume>
          <issue>10</issue>
          <year>2015</year>
          <fpage>4015</fpage>
          <lpage>4033</lpage>
        </element-citation>
      </ref>
      <ref id="bib75">
        <element-citation publication-type="journal" id="sref75">
          <person-group person-group-type="author">
            <name>
              <surname>Peltola</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Marttinen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Vehtari</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Metropolis-Hastings algorithm for variable selection in genome-wide association analysis</article-title>
          <source>PLoS One</source>
          <volume>7</volume>
          <issue>11</issue>
          <year>2012</year>
          <comment>e49 445</comment>
        </element-citation>
      </ref>
      <ref id="bib76">
        <element-citation publication-type="journal" id="sref76">
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Partial correlation estimation by joint sparse regression models</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>104</volume>
          <issue>486</issue>
          <year>2009</year>
          <fpage>735</fpage>
          <lpage>746</lpage>
          <pub-id pub-id-type="pmid">19881892</pub-id>
        </element-citation>
      </ref>
      <ref id="bib78">
        <element-citation publication-type="journal" id="sref78">
          <person-group person-group-type="author">
            <name>
              <surname>Peterson</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Stingo</surname>
              <given-names>F.C.</given-names>
            </name>
            <name>
              <surname>Vannucci</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian inference of multiple Gaussian graphical models</article-title>
          <source>J. Am. Stat. Assoc.</source>
          <volume>110</volume>
          <issue>509</issue>
          <year>2015</year>
          <fpage>159</fpage>
          <lpage>174</lpage>
          <pub-id pub-id-type="pmid">26078481</pub-id>
        </element-citation>
      </ref>
      <ref id="bib79">
        <element-citation publication-type="journal" id="sref79">
          <person-group person-group-type="author">
            <name>
              <surname>Polson</surname>
              <given-names>N.G.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>J.G.</given-names>
            </name>
          </person-group>
          <article-title>On the half-Cauchy prior for a global scale parameter</article-title>
          <source>Bayesian Anal.</source>
          <volume>7</volume>
          <issue>2</issue>
          <year>2012</year>
          <fpage>1</fpage>
          <lpage>16</lpage>
        </element-citation>
      </ref>
      <ref id="bib80">
        <element-citation publication-type="journal" id="sref80">
          <person-group person-group-type="author">
            <name>
              <surname>Qiu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Joint estimation of multiple graphical models from high dimensional time series</article-title>
          <source>J. R. Stat. Soc. B</source>
          <volume>78</volume>
          <issue>2</issue>
          <year>2015</year>
          <fpage>487</fpage>
          <lpage>504</lpage>
        </element-citation>
      </ref>
      <ref id="bib82">
        <element-citation publication-type="journal" id="sref82">
          <person-group person-group-type="author">
            <name>
              <surname>Ramsey</surname>
              <given-names>J.D.</given-names>
            </name>
            <name>
              <surname>Sanchez-Romero</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Glymour</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Non-Gaussian methods and high-pass filters in the estimation of effective connections</article-title>
          <source>NeuroImage</source>
          <volume>84</volume>
          <year>2014</year>
          <fpage>986</fpage>
          <lpage>1006</lpage>
          <pub-id pub-id-type="pmid">24099845</pub-id>
        </element-citation>
      </ref>
      <ref id="bib84">
        <element-citation publication-type="book" id="sref84">
          <person-group person-group-type="author">
            <name>
              <surname>Robinson</surname>
              <given-names>S.E.</given-names>
            </name>
            <name>
              <surname>Vrba</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <chapter-title>Functional neuroimaging by synthetic aperture magnetometry (SAM)</chapter-title>
          <person-group person-group-type="editor">
            <name>
              <surname>Yoshimoto</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Kotani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kuriki</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Karibe</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Nakasato</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <source>Recent Advances in Biomagnetism</source>
          <year>1999</year>
          <publisher-name>Tohoku University Press</publisher-name>
          <publisher-loc>Sendai, Japan</publisher-loc>
          <fpage>302</fpage>
          <lpage>305</lpage>
        </element-citation>
      </ref>
      <ref id="bib85">
        <element-citation publication-type="journal" id="sref85">
          <person-group person-group-type="author">
            <name>
              <surname>Robinson</surname>
              <given-names>E.C.</given-names>
            </name>
          </person-group>
          <article-title>MSM: a new flexible framework for Multimodal Surface Matching</article-title>
          <source>NeuroImage</source>
          <volume>100</volume>
          <year>2014</year>
          <fpage>414</fpage>
          <lpage>426</lpage>
          <pub-id pub-id-type="pmid">24939340</pub-id>
        </element-citation>
      </ref>
      <ref id="bib86">
        <element-citation publication-type="journal" id="sref86">
          <person-group person-group-type="author">
            <name>
              <surname>Rosenberg</surname>
              <given-names>M.D.</given-names>
            </name>
          </person-group>
          <article-title>A neuromarker of sustained attention from whole-brain functional connectivity</article-title>
          <source>Nat. Neurosci.</source>
          <volume>19</volume>
          <issue>1</issue>
          <year>2016</year>
          <fpage>165</fpage>
          <lpage>171</lpage>
          <pub-id pub-id-type="pmid">26595653</pub-id>
        </element-citation>
      </ref>
      <ref id="bib87">
        <element-citation publication-type="journal" id="sref87">
          <person-group person-group-type="author">
            <name>
              <surname>Ryali</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Estimation of functional connectivity in fMRI data using stability selection-based sparse correlation with elastic net penalty</article-title>
          <source>NeuroImage</source>
          <volume>59</volume>
          <issue>4</issue>
          <year>2012</year>
          <fpage>3852</fpage>
          <lpage>3861</lpage>
          <pub-id pub-id-type="pmid">22155039</pub-id>
        </element-citation>
      </ref>
      <ref id="bib88">
        <element-citation publication-type="journal" id="sref88">
          <person-group person-group-type="author">
            <name>
              <surname>Salimi-Khorshidi</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Automatic denoising of functional MRI data: combining independent component analysis and hierarchical fusion of classifiers</article-title>
          <source>NeuroImage</source>
          <volume>90</volume>
          <year>2014</year>
          <fpage>449</fpage>
          <lpage>468</lpage>
          <pub-id pub-id-type="pmid">24389422</pub-id>
        </element-citation>
      </ref>
      <ref id="bib89">
        <element-citation publication-type="journal" id="sref89">
          <person-group person-group-type="author">
            <name>
              <surname>Scannell</surname>
              <given-names>J.W.</given-names>
            </name>
          </person-group>
          <article-title>The connectional organization of the cortico-thralamic system of the cat</article-title>
          <source>Cereberal Cortex</source>
          <volume>9</volume>
          <issue>3</issue>
          <year>1999</year>
          <fpage>277</fpage>
          <lpage>299</lpage>
        </element-citation>
      </ref>
      <ref id="bib91">
        <element-citation publication-type="journal" id="sref91">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
            <name>
              <surname>Nichols</surname>
              <given-names>T.E.</given-names>
            </name>
          </person-group>
          <article-title>Statistical challenges in “big data” human neuroimaging</article-title>
          <source>Neuron</source>
          <volume>97</volume>
          <issue>2</issue>
          <year>2018</year>
          <fpage>263</fpage>
          <lpage>268</lpage>
          <pub-id pub-id-type="pmid">29346749</pub-id>
        </element-citation>
      </ref>
      <ref id="bib92">
        <element-citation publication-type="journal" id="sref92">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>Network modelling methods for fMRI</article-title>
          <source>Neuroimage</source>
          <volume>54</volume>
          <year>2011</year>
          <fpage>875</fpage>
          <lpage>891</lpage>
          <pub-id pub-id-type="pmid">20817103</pub-id>
        </element-citation>
      </ref>
      <ref id="bib93">
        <element-citation publication-type="journal" id="sref93">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>Functional connectomics from resting-state fMRI</article-title>
          <source>Trends Cognit. Sci.</source>
          <volume>17</volume>
          <issue>12</issue>
          <year>2013</year>
          <fpage>666</fpage>
          <lpage>682</lpage>
          <pub-id pub-id-type="pmid">24238796</pub-id>
        </element-citation>
      </ref>
      <ref id="bib94">
        <element-citation publication-type="journal" id="sref94">
          <person-group person-group-type="author">
            <name>
              <surname>Smith</surname>
              <given-names>S.M.</given-names>
            </name>
          </person-group>
          <article-title>A positive-negative mode of population covariation links brain connectivity, demographics and behavior</article-title>
          <source>Nat. Neurosci.</source>
          <volume>18</volume>
          <year>2015</year>
          <fpage>1565</fpage>
          <lpage>1567</lpage>
          <pub-id pub-id-type="pmid">26414616</pub-id>
        </element-citation>
      </ref>
      <ref id="bib95">
        <element-citation publication-type="journal" id="sref95">
          <person-group person-group-type="author">
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>Modern network science of neurological disorders</article-title>
          <source>Nat. Rev. Neuro- Sci.</source>
          <volume>15</volume>
          <year>2014</year>
          <fpage>683</fpage>
          <lpage>695</lpage>
        </element-citation>
      </ref>
      <ref id="bib96">
        <element-citation publication-type="journal" id="sref96">
          <person-group person-group-type="author">
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
            <name>
              <surname>van Straaten</surname>
              <given-names>E.C.W.</given-names>
            </name>
          </person-group>
          <article-title>The organization of physiological brain networks</article-title>
          <source>Clin. Neurophysiol.</source>
          <volume>123</volume>
          <year>2012</year>
          <fpage>1067</fpage>
          <lpage>1087</lpage>
          <pub-id pub-id-type="pmid">22356937</pub-id>
        </element-citation>
      </ref>
      <ref id="bib98">
        <element-citation publication-type="journal" id="sref98">
          <person-group person-group-type="author">
            <name>
              <surname>van Straaten</surname>
              <given-names>E.C.W.</given-names>
            </name>
            <name>
              <surname>Stam</surname>
              <given-names>C.J.</given-names>
            </name>
          </person-group>
          <article-title>Structure out of chaos: functional brain network analysis with EEG, MEG and functional MRI</article-title>
          <source>Eur. Neuropsychopharmacol.</source>
          <volume>23</volume>
          <year>2013</year>
          <fpage>7</fpage>
          <lpage>18</lpage>
          <pub-id pub-id-type="pmid">23158686</pub-id>
        </element-citation>
      </ref>
      <ref id="bib99">
        <element-citation publication-type="journal" id="sref99">
          <person-group person-group-type="author">
            <name>
              <surname>Tavor</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Task-free mri predicts individual differences in brain activity during task performance</article-title>
          <source>Science</source>
          <volume>352</volume>
          <issue>6282</issue>
          <year>2016</year>
          <fpage>216</fpage>
          <lpage>220</lpage>
          <pub-id pub-id-type="pmid">27124457</pub-id>
        </element-citation>
      </ref>
      <ref id="bib101">
        <element-citation publication-type="journal" id="sref101">
          <person-group person-group-type="author">
            <name>
              <surname>Uhler</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lenkoski</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Richards</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Exact formulas for the normalizing constants of Wishart distributions for graphical models</article-title>
          <source>Ann. Stat.</source>
          <volume>46</volume>
          <issue>1</issue>
          <year>2018</year>
          <fpage>90</fpage>
          <lpage>118</lpage>
        </element-citation>
      </ref>
      <ref id="bib102">
        <element-citation publication-type="journal" id="sref102">
          <person-group person-group-type="author">
            <name>
              <surname>Varoquaux</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Craddock</surname>
              <given-names>R.C.</given-names>
            </name>
          </person-group>
          <article-title>Learning and comparing functional connectomes across subjects</article-title>
          <source>NeuroImage</source>
          <volume>80</volume>
          <year>2013</year>
          <fpage>405</fpage>
          <lpage>415</lpage>
          <pub-id pub-id-type="pmid">23583357</pub-id>
        </element-citation>
      </ref>
      <ref id="bib103">
        <element-citation publication-type="book" id="sref103">
          <person-group person-group-type="author">
            <name>
              <surname>Varoquaux</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <chapter-title>Brain covariance selection: better individual functional connectivity models using population prior</chapter-title>
          <series>Advances in Neural Information Processing Systems</series>
          <volume>vol. 23</volume>
          <year>2010</year>
          <publisher-name>NIPS</publisher-name>
          <publisher-loc>Vancouver, Canada</publisher-loc>
        </element-citation>
      </ref>
      <ref id="bib104">
        <element-citation publication-type="journal" id="sref104">
          <person-group person-group-type="author">
            <name>
              <surname>Van Veen</surname>
              <given-names>B.D.</given-names>
            </name>
          </person-group>
          <article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title>
          <source>IEEE Trans. Biomed. Imaging</source>
          <volume>44</volume>
          <year>1997</year>
          <fpage>867</fpage>
          <lpage>880</lpage>
        </element-citation>
      </ref>
      <ref id="bib105">
        <element-citation publication-type="journal" id="sref105">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian graphical lasso models and efficient posterior computation</article-title>
          <source>Bayesian Anal.</source>
          <volume>7</volume>
          <issue>2</issue>
          <year>2012</year>
          <fpage>771</fpage>
          <lpage>790</lpage>
          <pub-id pub-id-type="pmid">27375829</pub-id>
        </element-citation>
      </ref>
      <ref id="bib106">
        <element-citation publication-type="journal" id="sref106">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Efficient Gaussian graphical model determination under G-Wishart distributions</article-title>
          <source>Electron. J. Stat.</source>
          <volume>6</volume>
          <year>2012</year>
          <fpage>168</fpage>
          <lpage>198</lpage>
        </element-citation>
      </ref>
      <ref id="bib107">
        <element-citation publication-type="journal" id="sref107">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Scaling it up: stochastic search structure learning in graphical models</article-title>
          <source>Bayesian Anal.</source>
          <volume>10</volume>
          <issue>2</issue>
          <year>2015</year>
          <fpage>351</fpage>
          <lpage>377</lpage>
        </element-citation>
      </ref>
      <ref id="bib108">
        <element-citation publication-type="journal" id="sref108">
          <person-group person-group-type="author">
            <name>
              <surname>Winkler</surname>
              <given-names>A.M.</given-names>
            </name>
          </person-group>
          <article-title>Multi-level block permutation</article-title>
          <source>NeuroImage</source>
          <volume>123</volume>
          <year>2015</year>
          <fpage>253</fpage>
          <lpage>268</lpage>
          <pub-id pub-id-type="pmid">26074200</pub-id>
        </element-citation>
      </ref>
      <ref id="bib109">
        <element-citation publication-type="journal" id="sref109">
          <person-group person-group-type="author">
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>Robust group analysis using outlier inference</article-title>
          <source>NeuroImage</source>
          <volume>41</volume>
          <year>2008</year>
          <fpage>286</fpage>
          <lpage>301</lpage>
          <pub-id pub-id-type="pmid">18407525</pub-id>
        </element-citation>
      </ref>
      <ref id="bib110">
        <element-citation publication-type="journal" id="sref110">
          <person-group person-group-type="author">
            <name>
              <surname>Woolrich</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>Meg beamforming using Bayesian PCA for adaptive data covariance matrix regularization</article-title>
          <source>NeuroImage</source>
          <volume>57</volume>
          <year>2011</year>
          <fpage>1466</fpage>
          <lpage>1479</lpage>
          <pub-id pub-id-type="pmid">21620977</pub-id>
        </element-citation>
      </ref>
      <ref id="bib112">
        <element-citation publication-type="journal" id="sref112">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Fused multiple graphical lasso</article-title>
          <source>Soc. Ind. Appl. Math. J. Optim.</source>
          <volume>25</volume>
          <issue>2</issue>
          <year>2015</year>
          <fpage>916</fpage>
          <lpage>943</lpage>
        </element-citation>
      </ref>
      <ref id="bib113">
        <element-citation publication-type="journal" id="sref113">
          <person-group person-group-type="author">
            <name>
              <surname>Zou</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hastie</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Regularization and variable selection via the elastic net</article-title>
          <source>J. R. Stat. Soc. B</source>
          <volume>67</volume>
          <issue>2</issue>
          <year>2005</year>
          <fpage>301</fpage>
          <lpage>320</lpage>
        </element-citation>
      </ref>
    </ref-list>
    <sec id="appsec1" sec-type="supplementary-material">
      <label>Appendix A</label>
      <title>Supplementary data</title>
      <p id="p0530">The following are the supplementary data related to this article:<supplementary-material content-type="local-data" id="mmc1"><caption><title>Supplementary data</title></caption><media xlink:href="mmc1.pdf"><alt-text>Supplementary data</alt-text></media></supplementary-material></p>
    </sec>
    <ack id="ack0010">
      <title>Acknowledgements</title>
      <p>Thanks to Paul McCarthy for help creating figures, to Diego Vidaurre for sharing his elastic net regression tools, and to Bernard Ng for providing his SGGGM software. Functional MRI and MEG data were collected by the Human Connectome Project, WU-Minn Consortium (principal investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH institutes and centres that support the <funding-source id="gs1">NIH Blueprint for Neuroscience Research</funding-source>; and by the <funding-source id="gs2">McDonnell Center for Systems Neuroscience at Washington University</funding-source>. The <funding-source id="gs3">Wellcome Centre for Integrative Neuroimaging</funding-source> is supported by core funding from the Wellcome Trust (203139/Z/16/Z). G.L.C. was funded by the <funding-source id="gs4">Research Councils UK Digital Economy Programme</funding-source> (EP/G036861/1, Centre for Doctoral Training in Healthcare Innovation). M.W.W. was funded by the Wellcome Trust (092753), and was supported by the <funding-source id="gs5">National Institute for Health Research</funding-source> (NIHR) <funding-source id="gs6">Oxford Biomedical Research Centre</funding-source> (IS-BRC-1215-20005) based at the Oxford University Hospitals Trust, Oxford University. S.J.H. was supported by the Wellcome Trust and by the <funding-source id="gs7">UK Engineering and Physical Sciences Research Council</funding-source> (EP/F500394/1, Life Sciences Interface Doctoral Training Centre). P.A.V.S. was funded by grants from the <funding-source id="gs8">National Nature Science Foundation of China</funding-source> (61673090 and 81330032). S.M.S. was funded by a <funding-source id="gs9">Wellcome Trust Strategic Award</funding-source> (098369/Z/12/Z). This work was also supported by an MRC UK MEG Partnership Grant (MR/K005464/1). The views expressed in this paper are those of the authors and not necessarily those of the NHS, the NIHR, the Department of Health, or of the other funding bodies.</p>
    </ack>
    <fn-group>
      <fn id="fn1">
        <label>1</label>
        <p id="ntpara0010">OSL, the OHBA (Oxford Centre for Human Brain Activity) Software Library, and a Matlab implementation of our HIPPO algorithm are both available from <ext-link ext-link-type="uri" xlink:href="http://www.github.com/ohba-analysis/" id="interef0020">www.github.com/ohba-analysis/</ext-link>.</p>
      </fn>
      <fn id="fn2">
        <label>2</label>
        <p id="ntpara0015">The code used to perform this adjustment, nearestSPD.m was written by John D'Errico and is available from <ext-link ext-link-type="uri" xlink:href="http://uk.mathworks.com/matlabcentral/fileexchange/42885-nearestspd" id="interref0065">uk.mathworks.com/matlabcentral/fileexchange/42885-nearestspd</ext-link>.</p>
      </fn>
      <fn id="fn3">
        <label>3</label>
        <p id="ntpara0020">Both of these matrices are available from the Brain Connectivity Toolbox, at <ext-link ext-link-type="uri" xlink:href="http://sites.google.com/site/bctnet/datasets" id="interref0070">sites.google.com/site/bctnet/datasets</ext-link>.</p>
      </fn>
      <fn id="fn4">
        <label>4</label>
        <p id="ntpara0025">Python routines for solving this problem are available as part of nilearn. See <ext-link ext-link-type="uri" xlink:href="http://nilearn.github.io/connectivity" id="interref0060">nilearn.github.io/connectivity</ext-link> for more details.</p>
      </fn>
      <fn id="fn5">
        <label>5</label>
        <p id="ntpara0030">R code for solving Danaher et al.'s group GLASSO and fused GLASSO using the alternating direction method of multipliers algorithm is available as the package JGL, from <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/JGL/index.html" id="interref0015">cran.r-project.org/web/packages/JGL/index.html</ext-link>.</p>
      </fn>
      <fn id="fn6">
        <label>6</label>
        <p id="ntpara0035">Matlab code for solving the model was obtained from the authors.</p>
      </fn>
      <fn id="fn7">
        <label>7</label>
        <p id="ntpara0040">If <inline-formula><mml:math id="M145" altimg="si85.gif" overflow="scroll"><mml:mi mathvariant="bold">Ω</mml:mi></mml:math></inline-formula> is (<italic>G</italic>-) Wishart distributed, <inline-formula><mml:math id="M146" altimg="si109.gif" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi mathvariant="script">G</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then its expectation is <inline-formula><mml:math id="M147" altimg="si111.gif" overflow="scroll"><mml:mrow><mml:mo>〈</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>〉</mml:mo><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mi mathvariant="bold-italic">V</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      </fn>
      <fn id="fn8">
        <label>8</label>
        <p id="ntpara0045">BayesGlasso is available from <ext-link ext-link-type="uri" xlink:href="http://msu.edu/~haowang/RESEARCH/Bglasso/bglasso.html" id="interref0020">msu.edu/∼haowang/RESEARCH/Bglasso/bglasso.html</ext-link>.</p>
      </fn>
      <fn id="fn9">
        <label>9</label>
        <p id="ntpara0050">BayesGGM_SSVS is available from <ext-link ext-link-type="uri" xlink:href="http://msu.edu/~haowang" id="interref0025">msu.edu/∼haowang</ext-link>/.</p>
      </fn>
      <fn id="fn10">
        <label>10</label>
        <p id="ntpara0055">ggm_gwish_cbf_direct.m is available from <ext-link ext-link-type="uri" xlink:href="http://github.com/ccnlab/BaCon/tree/master/ggm" id="interref0030">github.com/ccnlab/BaCon/tree/master/ggm</ext-link>.</p>
      </fn>
      <fn id="fn11">
        <label>11</label>
        <p id="ntpara0060">Software implementing the method is available from <ext-link ext-link-type="uri" xlink:href="http://odin.mdacc.tmc.edu/~cbpeterson/software.html" id="interref0035">odin.mdacc.tmc.edu/∼cbpeterson/software.html</ext-link>.</p>
      </fn>
      <fn id="fn12">
        <label>12</label>
        <p id="ntpara0065"><ext-link ext-link-type="uri" xlink:href="http://db.humanconnectome.org" id="interref0040">db.humanconnectome.org</ext-link>.</p>
      </fn>
      <fn id="fn13">
        <label>13</label>
        <p id="ntpara0070">These two confounds are excluded when sex is the predicted variable.</p>
      </fn>
      <fn id="fn14">
        <label>14</label>
        <p id="ntpara0075">Junyang Qian's Matlab implementation of Friedman et al.'s Glmnet algorithms is available from <ext-link ext-link-type="uri" xlink:href="http://web.stanford.edu/~hastie/glmnet_matlab/" id="interref0045">web.stanford.edu/∼hastie/glmnet_matlab/</ext-link>.</p>
      </fn>
      <fn id="fn15">
        <label>15</label>
        <p id="ntpara0080">The Advanced Permutation inference for ACE models software is available from <ext-link ext-link-type="uri" xlink:href="http://warwick.ac.uk/tenichols/apace" id="interref0050">warwick.ac.uk/tenichols/apace</ext-link>.</p>
      </fn>
      <fn id="fn16">
        <label>16</label>
        <p id="ntpara0085">Available at <ext-link ext-link-type="uri" xlink:href="http://db.humanconnectome.org/megatrawl/" id="interref0055">db.humanconnectome.org/megatrawl/</ext-link>.</p>
      </fn>
      <fn id="fn17">
        <label>17</label>
        <p id="ntpara0090"><italic>Crystallised intelligence</italic> is defined by Wikipedia (on the 21<sup>st</sup> August 2016) as ‘the ability to use skills, knowledge, and experience. It does not equate to memory, but it does rely on accessing information from long-term memory.’ This is in contrast to <italic>fluid intelligence,</italic> which characterises an ability to reason, deduce and to solve novel problems.</p>
      </fn>
      <fn id="fn18">
        <label>18</label>
        <p id="ntpara0095">Figure produced using Paul McCarthy's visualisation tool in FSLNets.</p>
      </fn>
      <fn id="appsec2" fn-type="supplementary-material">
        <label>Appendix A</label>
        <p id="p0535">Supplementary data related to this article can be found at <ext-link ext-link-type="doi" xlink:href="10.1016/j.neuroimage.2018.04.077" id="intref0010">https://doi.org/10.1016/j.neuroimage.2018.04.077</ext-link>.</p>
      </fn>
    </fn-group>
  </back>
</article>
