<?xml version='1.0' encoding='UTF-8'?>
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Front Hum Neurosci</journal-id>
      <journal-id journal-id-type="iso-abbrev">Front Hum Neurosci</journal-id>
      <journal-id journal-id-type="publisher-id">Front. Hum. Neurosci.</journal-id>
      <journal-title-group>
        <journal-title>Frontiers in Human Neuroscience</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1662-5161</issn>
      <publisher>
        <publisher-name>Frontiers Media S.A.</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">37497043</article-id>
      <article-id pub-id-type="pmc">10366614</article-id>
      <article-id pub-id-type="doi">10.3389/fnhum.2023.1134012</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Original Research</subject>
          </subj-group>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Manifold learning for fMRI time-varying functional connectivity</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Gonzalez-Castillo</surname>
            <given-names>Javier</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref>
          <uri xlink:href="http://loop.frontiersin.org/people/67207/overview"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Fernandez</surname>
            <given-names>Isabel S.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Lam</surname>
            <given-names>Ka Chun</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref>
          <uri xlink:href="http://loop.frontiersin.org/people/2300225/overview"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Handwerker</surname>
            <given-names>Daniel A.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref>
          <uri xlink:href="http://loop.frontiersin.org/people/6829/overview"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>Francisco</given-names>
          </name>
          <xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref>
          <uri xlink:href="http://loop.frontiersin.org/people/29406/overview"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Bandettini</surname>
            <given-names>Peter A.</given-names>
          </name>
          <xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref>
          <uri xlink:href="http://loop.frontiersin.org/people/13390/overview"/>
        </contrib>
      </contrib-group>
      <aff id="aff1"><sup>1</sup><institution>Section on Functional Imaging Methods, National Institute of Mental Health</institution>, <addr-line>Bethesda, MD</addr-line>, <country>United States</country></aff>
      <aff id="aff2"><sup>2</sup><institution>Machine Learning Group, National Institute of Mental Health</institution>, <addr-line>Bethesda, MD</addr-line>, <country>United States</country></aff>
      <aff id="aff3"><sup>3</sup><institution>Functional Magnetic Resonance Imaging (FMRI) Core, National Institute of Mental Health</institution>, <addr-line>Bethesda, MD</addr-line>, <country>United States</country></aff>
      <author-notes>
        <fn fn-type="edited-by">
          <p>Edited by: Georgios D. Mitsis, McGill University, Canada</p>
        </fn>
        <fn fn-type="edited-by">
          <p>Reviewed by: Anees Abrol, Georgia State University, United States; Deirel Paz-Linares, University of Electronic Science and Technology of China, China; Mohsen Bahrami, Virginia Tech – Wake Forest University, United States</p>
        </fn>
        <corresp id="c001">*Correspondence: Javier Gonzalez-Castillo, <email>javier.gonzalez-castillo@nih.gov</email></corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>11</day>
        <month>7</month>
        <year>2023</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2023</year>
      </pub-date>
      <volume>17</volume>
      <elocation-id>1134012</elocation-id>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>12</month>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>6</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Copyright © 2023 Gonzalez-Castillo, Fernandez, Lam, Handwerker, Pereira and Bandettini.</copyright-statement>
        <copyright-year>2023</copyright-year>
        <copyright-holder>Gonzalez-Castillo, Fernandez, Lam, Handwerker, Pereira and Bandettini</copyright-holder>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Whole-brain functional connectivity (<italic>FC</italic>) measured with functional MRI (fMRI) evolves over time in meaningful ways at temporal scales going from years (e.g., development) to seconds [e.g., within-scan time-varying <italic>FC</italic> (<italic>tvFC</italic>)]. Yet, our ability to explore <italic>tvFC</italic> is severely constrained by its large dimensionality (several thousands). To overcome this difficulty, researchers often seek to generate low dimensional representations (e.g., <italic>2D</italic> and <italic>3D</italic> scatter plots) hoping those will retain important aspects of the data (e.g., relationships to behavior and disease progression). Limited prior empirical work suggests that manifold learning techniques (<italic>MLTs</italic>)—namely those seeking to infer a low dimensional non-linear surface (i.e., the manifold) where most of the data lies—are good candidates for accomplishing this task. Here we explore this possibility in detail. First, we discuss why one should expect tv<italic>FC</italic> data to lie on a low dimensional manifold. Second, we estimate what is the intrinsic dimension (<italic>ID</italic>; i.e., minimum number of latent dimensions) of <italic>tvFC</italic> data manifolds. Third, we describe the inner workings of three state-of-the-art <italic>MLTs</italic>: Laplacian Eigenmaps (<italic>LEs</italic>), T-distributed Stochastic Neighbor Embedding (<italic>T-SNE</italic>), and Uniform Manifold Approximation and Projection (<italic>UMAP</italic>). For each method, we empirically evaluate its ability to generate neuro-biologically meaningful representations of <italic>tvFC</italic> data, as well as their robustness against hyper-parameter selection. Our results show that <italic>tvFC</italic> data has an <italic>ID</italic> that ranges between 4 and 26, and that <italic>ID</italic> varies significantly between rest and task states. We also show how all three methods can effectively capture subject identity and task being performed: <italic>UMAP</italic> and <italic>T-SNE</italic> can capture these two levels of detail concurrently, but <italic>LE</italic> could only capture one at a time. We observed substantial variability in embedding quality across <italic>MLTs</italic>, and within-<italic>MLT</italic> as a function of hyper-parameter selection. To help alleviate this issue, we provide heuristics that can inform future studies. Finally, we also demonstrate the importance of feature normalization when combining data across subjects and the role that temporal autocorrelation plays in the application of <italic>MLTs</italic> to <italic>tvFC</italic> data. Overall, we conclude that while <italic>MLTs</italic> can be useful to generate summary views of labeled <italic>tvFC</italic> data, their application to unlabeled data such as resting-state remains challenging.</p>
      </abstract>
      <kwd-group>
        <kwd>manifold learning</kwd>
        <kwd>fMRI</kwd>
        <kwd>time-varying functional connectivity</kwd>
        <kwd>data visualization</kwd>
        <kwd>
          <italic>T-SNE</italic>
        </kwd>
        <kwd>Uniform Manifold Approximation and Projection (<italic>UMAP</italic>)</kwd>
        <kwd>Laplacian Eigenmaps (<italic>LE</italic>)</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source id="cn001">
            <institution-wrap>
              <institution>National Institute of Mental Health</institution>
              <institution-id institution-id-type="doi">10.13039/100000025</institution-id>
            </institution-wrap>
          </funding-source>
          <award-id award-type="contract" rid="cn001">ZIAMH002783</award-id>
          <award-id award-type="contract" rid="cn001">ZICMH002968</award-id>
        </award-group>
        <funding-statement>This research was the supported by the National Institute of Mental Health Intramural Research Programs (ZIAMH002783 and ZICMH002968).</funding-statement>
      </funding-group>
      <counts>
        <fig-count count="10"/>
        <table-count count="2"/>
        <equation-count count="16"/>
        <ref-count count="91"/>
        <page-count count="22"/>
        <word-count count="19135"/>
      </counts>
      <custom-meta-group>
        <custom-meta>
          <meta-name>section-at-acceptance</meta-name>
          <meta-value>Brain Imaging and Stimulation</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro" id="S1">
      <title>Introduction</title>
      <p>From a data-science perspective, a functional MRI (fMRI) scan is a four-dimensional tensor <italic>T</italic> [<italic>x</italic>, <italic>y</italic>, <italic>z</italic>, <italic>t</italic>] with the first three dimensions encoding position in space (<italic>x, y, z</italic>) and the fourth dimension referring to time (<italic>t</italic>). Yet, for operational purposes, it is often reasonable to merge the three spatial dimensions into one and conceptualize this data as a matrix of space vs. time. With current technology (e.g., voxel size ∼ 2 mm × 2 mm × 2 mm, TR ∼ 1.5 s), a representative 10-min fMRI scan with full brain coverage will generate a matrix with approximately 400 temporal samples (number of acquisitions) in each of over 40,000 gray matter (GM) locations (number of voxels). Before this data is ready for interpretation, it must undergo several transformations that address three key needs: (1) removal of signal variance unrelated to neuronal activity; (2) spatial normalization into a common space to enable comparisons across subjects and studies; and (3) generation of intuitive visualizations for explorative or reporting purposes. These three needs and how to address them will depend on the nature of the study [e.g., bandpass filtering may be appropriate for resting-state data and not task, the <italic>MNI152</italic> template (<xref rid="B35" ref-type="bibr">Fonov et al., 2009</xref>) will be a good common space to report adult data yet not for a study conducted on a pediatric population]. This work focuses on how to address the third need—the generation of interpretable visualizations—particularly for studies that explore the temporal dynamics of the human functional connectome.</p>
      <p>Most human functional connectome studies use the concept of a functional connectivity matrix (<italic>FC</italic> matrix) or its graph equivalent. Two of the most common types of <italic>FC</italic> matrices in the fMRI literature are: (1) static <italic>FC</italic> (<italic>sFC</italic> [<italic>i</italic>, <italic>j</italic>]) matrices designed to capture average levels of inter-regional activity synchronization across the duration of an entire scan, and (2) time-varying FC (<italic>tvFC</italic> [(<italic>i</italic>, <italic>j</italic>), <italic>t</italic>]) matrices meant to retain temporal information about how connectivity strength fluctuates as scanning progresses [see <xref rid="B72" ref-type="bibr">Mokhtari et al. (2018a</xref>,<xref rid="B73" ref-type="bibr">b</xref>) for alternative approaches]. These two matrix types not only differ on their representational goal, but also in their structure and dimensionality. In a <italic>sFC</italic> matrix, rows (<italic>i</italic>) and columns (<italic>j</italic>) represent spatial locations [e.g., voxels and regions of interest (ROIs)], and the value of a given cell <italic>(i, j)</italic> is a measure of similarity (e.g., Pearson’s correlation, partial correlation, and mutual information) between the complete time series recorded at these two locations. When <italic>FC</italic> is expressed in terms of Pearson’s correlation (the most common approach in the fMRI literature), <italic>sFC</italic> matrices are symmetric with a unit diagonal. Moreover, they can be transformed from their original <italic>2D</italic> form (<italic>N × N</italic>; <italic>N</italic> = number of spatial locations) into a <italic>1D</italic> vector of dimensionality Eq. 1 with <italic>N</italic><sub><italic>cons</italic></sub> being the number of unique pair-wise connections.</p>
      <disp-formula id="S1.E1">
<label>(1)</label>
<mml:math id="M1" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mpadded lspace="5pt" width="+5pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="10.8pt">=</mml:mo><mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math>
</disp-formula>
      <p>Conversely, a <italic>tvFC</italic> matrix is a much larger data structure where rows <italic>(i,j)</italic> represent connections between regions <italic>i</italic> and <italic>j</italic>, and columns <italic>(t)</italic> represent time (<xref rid="F1" ref-type="fig">Figure 1A</xref>). The size of a <italic>tvFC</italic> matrix is <italic>N<sub><italic>cons</italic></sub> × N<sub><italic>wins</italic></sub></italic>; and it is determined as follows. The number of rows (<italic>N</italic><sub><italic>cons</italic></sub>) is given by the pairs of spatial locations contributing to the matrix (Eq. 1). The number of columns (<italic>N</italic><sub><italic>wins</italic></sub>) is a function of the duration of the scan (<italic>N</italic><sub><italic>acq</italic></sub> = number of temporal samples or acquisitions) and the mechanism used to construct the matrix, which in fMRI, is often some form of sliding window technique that proceeds as follows. First, a temporal window of duration shorter than the scan is chosen (e.g., <italic>W</italic><sub><italic>duration</italic></sub> = 20 samples &lt;&lt; <italic>N</italic><sub><italic>acq</italic></sub>). Second, a <italic>sFC</italic> matrix is computed using only the data within that temporal window. The resulting <italic>sFC</italic> matrix is then transformed into its <italic>1D</italic> vector representation, which becomes the first column of the <italic>tvFC</italic> matrix. Next, the temporal window slides forward a given amount determined by the windowing step (e.g., <italic>W<sub><italic>step</italic></sub></italic> = 3 samples), a new <italic>sFC</italic> matrix is computed for the new window, transformed into its <italic>1D</italic> form, and inserted as the second column of the <italic>tvFC</italic> matrix. This process is repeated until a full window can no longer be fit to the data. This results in <italic>N</italic><sub><italic>wins</italic></sub> columns, with <italic>N</italic><sub><italic>wins</italic></sub> given by:</p>
      <disp-formula id="S1.E2">
<label>(2)</label>
<mml:math id="M2" overflow="scroll"><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mpadded lspace="5pt" width="+5pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo rspace="7.5pt">-</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math>
</disp-formula>
      <fig position="float" id="F1">
        <label>FIGURE 1</label>
        <caption>
          <p><bold>(A)</bold> Time-varying <italic>FC</italic> matrix at scale to illustrate the disproportionate larger dimensionality of the connectivity axis (<italic>y</italic>-axis) relative to the time axis (<italic>x</italic>-axis). <bold>(B)</bold> Same <italic>tvFC</italic> matrix as in panel <bold>(A)</bold> but no longer at scale. The <italic>x</italic>-axis has now been stretched to better observe how connectivity evolves over time. Connections are sorted in terms of their average strength. Task-homogenous windows are clearly marked above the <italic>tvFC</italic> matrix with color-coded rectangles. <bold>(C)</bold> Same <italic>tvFC</italic> matrix with connections sorted in terms of their volatility (as indexes by the coefficient of variance). <bold>(D)</bold> Same <italic>tvFC</italic> matrix with connections sorted according to hemispheric membership. Intra-hemispheric connections appear at the top of the matrix and inter-hemispheric at the bottom. Laplacian Eigenmap (correlation distance, <italic>k</italic> = 90) for the <italic>tvFC</italic> matrix in panels <bold>(A–D)</bold> with no color annotation <bold>(E)</bold>, annotated by time <bold>(F)</bold>, and annotated by task <bold>(G)</bold>. <bold>(H)</bold> Laplacian Eigenmap for the <italic>tvFC</italic> matrix in panels <bold>(A–D)</bold> using correlation distance and <italic>k</italic> = 20. <bold>(I)</bold> Euclidean distance of each point in the embedding from the origin. Dashed lines indicate automatically detected peaks in the distance trace. Shaded regions around those locations indicate the temporal segments (30 windows) used to compute the FC matrices represented below. <bold>(J)</bold> FC matrices associated with each scan interval indicated in panel <bold>(I)</bold>. <bold>(K)</bold> Same information as in panel <bold>(J)</bold> but shown over a brain surface. Only connections with | <italic>r</italic>| &gt; 0.4 are shown in these brain maps.</p>
        </caption>
        <graphic xlink:href="fnhum-17-1134012-g001" position="float"/>
      </fig>
      <p>For those interested in a more mathematically oriented description of these two key data structures (<italic>sFC</italic> and <italic>tvFC</italic>) for FC analyses (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Note 1</xref>).</p>
      <p><xref rid="F1" ref-type="fig">Figure 1A</xref> shows a <italic>tvFC</italic> matrix for a 25-min-long fMRI scan (<italic>N</italic><sub><italic>acqs</italic></sub> = 1,017, TR = 1.5 s) acquired continuously as a subject performed and transitioned between four different tasks [i.e., rest, working memory (WM), arithmetic calculations, and visual attention (VA) (<xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>)]. Each task was performed continuously for two separate 3 min periods. The <italic>tvFC</italic> matrix was generated using a brain parcellation with 157 ROIs and a sliding window approach (<italic>W</italic><sub><italic>duration</italic></sub> = 30 samples, <italic>W</italic><sub><italic>step</italic></sub> = 1 sample). As such, the dimensions of the <italic>tvFC</italic> matrix are <italic>12,246 connections × 988 temporal windows</italic>. <xref rid="F1" ref-type="fig">Figure 1A</xref> shows the matrix at scale (each datapoint represented by a square) so we can appreciate the disproportionate ratio between number of connections (<italic>y</italic>-axis) and number of temporal samples (<italic>x</italic>-axis). <xref rid="F1" ref-type="fig">Figure 1B</xref> shows the same matrix as <xref rid="F1" ref-type="fig">Figure 1A</xref>, but this time the temporal axis has been stretched so that we can better observe the temporal evolution of <italic>FC</italic>. In this view of the data, connections are sorted according to average strength across time. The colored segments on top of the matrix indicate the task being performed at a given window [gray, rest; blue, WM; yellow, VA; green, arithmetic (Math)]. Colors are only shown for task-homogenous windows, meaning those that span scan periods when the subject was always performing the same task (i.e., no transitions or two different tasks). Transition windows, namely those that include more than one task and/or instruction periods, are marked as empty spaces between the colored boxes. <xref rid="F1" ref-type="fig">Figures 1C, D</xref> show the same <italic>tvFC</italic> matrix as <xref rid="F1" ref-type="fig">Figure 1A</xref>, but with connections sorted by temporal volatility (i.e., coefficient of variation) and hemispheric membership, respectively. In all instances, irrespective of sorting, it is quite difficult to directly infer from these matrices basic characteristics of how <italic>FC</italic> varies over time and/or relates to behavior. This is in large part due to the high dimensionality of the data.</p>
      <p>When an initial exploration of high dimensional data is needed, it is common practice to generate a low dimensional representation (e.g., two or three dimensions) that can be easily visualized yet preserves important information about the structure of the data (e.g., groups of similar samples and presence of outliers) in the original space. <xref rid="F1" ref-type="fig">Figure 1E</xref> shows one such representation of our <italic>tvFC</italic> matrix generated using a manifold learning method called <italic>Laplacian Eigenmaps</italic> (<italic>LEs</italic>; <xref rid="B8" ref-type="bibr">Belkin and Niyogi, 2003</xref>). In this representation, each column from the <italic>tvFC</italic> matrix becomes a point in <italic>3D</italic> space. In other words, each point represents the brain <italic>FC</italic> during a portion of the scan (in this case a 30 samples window). Points that are closer in this lower dimensional space are supposed to correspond to whole brain <italic>FC</italic> patterns that are similar. A first look at <xref rid="F1" ref-type="fig">Figure 1E</xref> reveals that there are four different recurrent <italic>FC</italic> configurations (corners marked with red arrows). If we annotate points with colors that represent time (<xref rid="F1" ref-type="fig">Figure 1F</xref>), we can also learn that each of those configurations were visited twice, once during the first half of the scan (blue tones) and a second time during the second half (red tones). Similarly, if we compute the <italic>Euclidean</italic> distance of each point in the embedding to the origin, we can easily observe the temporal profile of the experiment with eight distinct blocks (<xref rid="F1" ref-type="fig">Figure 1I</xref>). One could then use this information to temporally fragment scans into segments of interest and explore the whole brain FC patterns associated with each of them (<xref rid="F1" ref-type="fig">Figures 1J, K</xref>). Similar approaches that rely on <italic>tvFC</italic> embeddings as an initial step toward making biological inferences about how FC self-organizes during rest (<xref rid="B78" ref-type="bibr">Saggar et al., 2018</xref>) or relates to covert cognitive processes (<xref rid="B42" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>) have been previously reported and we refer readers to these works for additional details on how dimensionality reduction can inform subsequent analyses aimed at making biological inferences regarding the dynamics of FC.</p>
      <p>Finally, if we annotate the points with information about the task being performed at each window, we can clearly observe that the four corners correspond to <italic>FC</italic> patterns associated with each of the tasks—with temporally separated occurrences of the task appearing close to each other—and that transitional windows tend to form trajectories going between two corners corresponding to the tasks at both ends of each transitional period.</p>
      <p>To achieve the meaningful representations presented in <xref rid="F1" ref-type="fig">Figures 1E–G</xref>, one ought to make several analytical decisions beyond those related to fMRI data preprocessing and brain parcellation selection. These include the selection of a dimensionality reduction method, a dissimilarity function and a set of additional method-specific hyper-parameters (e.g., number of neighbors, perplexity, learning rate, etc.). In the same way to how using the wrong bandpass filter while pre-processing resting-state data can eliminate all neuronally relevant information, choosing incorrect hyper-parameters for <italic>LE</italic> can produce less meaningful low dimensional representations of <italic>tvFC</italic> matrices. <xref rid="F1" ref-type="fig">Figure 1H</xref> shows one such instance, where using an excessively small neighborhood (<italic>K</italic><sub><italic>nn</italic></sub>) resulted on a <italic>3D</italic> scatter that only captures temporal autocorrelation (e.g., temporally successive windows appear next to each other in a “spaghetti-like” structure) and misses the other important data characteristics discussed in the previous paragraph (e.g., the repetitive task structure of the experimental paradigm).</p>
      <p>In this manuscript we will explore the usability of three prominent manifold learning methods—namely <italic>LE</italic>, T-distributed Stochastic Neighbor Embedding (<italic>T-SNE</italic>; <xref rid="B86" ref-type="bibr">van der Maaten and Hinton, 2008</xref>), and Uniform Manifold Approximation and Projection (<italic>UMAP</italic>; <xref rid="B67" ref-type="bibr">McInnes et al., 2018</xref>)—to generate low-dimensional representations of <italic>tvFC</italic> matrices that retain neurobiological and behavioral information. These three methods were selected because of their success across scientific disciplines, including many biomedical applications (<xref rid="B91" ref-type="bibr">Zeisel et al., 2018</xref>; <xref rid="B26" ref-type="bibr">Diaz-Papkovich et al., 2019</xref>; <xref rid="B57" ref-type="bibr">Kollmorgen et al., 2020</xref>). First, in section “Theory,” we will introduce the manifold hypothesis and the concept of intrinsic dimension (<italic>ID</italic>) of a dataset. We will also provide a programmatic level description (as opposed to purely mathematical) of each of these methods. Next, we will evaluate each method’s ability to generate meaningful low dimensional representations of <italic>tvFC</italic> matrices using a clustering framework and a predictive framework for both individual scans and the complete dataset. Readers interested solely on the empirical evaluation of the methods are invited to skip the section “Theory” and proceed directly to the section “Materials and methods,” “Results,” and “Discussion.” The section “Theory” is intended to provide a detailed introductory background to manifold learning and the methods under scrutiny here to members of the neuroimaging research community, without assuming a machine learning background.</p>
      <p>Our results demonstrate the <italic>tvFC</italic> data reside in low dimensional manifolds that can be effectively estimated by the three methods under evaluation, yet also highlight the importance of correctly choosing key hyper-parameters as well as that of considering the effects of temporal autocorrelation when designing experiments and interpreting the final embeddings. In this regard, we provide a set of heuristics that can guide their application in future studies. In parallel, we also demonstrate the value of <italic>ID</italic> for deciding how many dimensions ought to be explored or included in additional analytical steps (e.g., spatial transformations and classification), and demonstrate its value as an index of how <italic>tvFC</italic> complexity varies between resting and task states.</p>
    </sec>
    <sec id="S2">
      <title>Theory</title>
      <sec id="S2.SS1">
        <title>Manifold hypothesis</title>
        <p>The manifold hypothesis sustains that many high dimensional datasets that occur in the real world (e.g., real images, speech, etc.) lie along or near a low-dimensional manifold (e.g., the equivalent of a curve or surface beyond three dimensions) embedded in the original high dimensional space (often referred to as the ambient space). This is because the generative process for real world data usually has a limited number of degrees of freedom constrained by laws (e.g., physical, biological, linguistic, etc.) specific to the process. For example, images of human faces lie along a low dimensional manifold within the higher dimensional ambient pixel space because most human faces have a quite regular structure (one nose between two eyes sitting above a mouth, etc.) and symmetry. This makes the space of pixel combinations that lead to images of human faces a very limited space compared to that of all possible pixel combinations. Similarly, speech lies in a low dimensional manifold within the higher dimensional ambient space of sound pressure timeseries because speech sounds are restricted both by the physical laws that limit the type of sounds the human vocal tract can generate and by the phonetic principles of a given language. Now, does an equivalent argument apply to the generation of <italic>tvFC</italic> data? In other words, is there evidence to presume that <italic>tvFC</italic> data lies along or near a low dimensional manifold embedded within the high dimensional ambient space of all possible pair-wise <italic>FC</italic> configurations? The answer is yes. Given our current understanding of the functional connectome and the laws governing fMRI signals it is reasonable to assume that the manifold hypothesis applies to fMRI-based <italic>tvFC</italic> data.</p>
        <p>First, the topological structure of the human functional connectome is not random but falls within a small subset of possible topological configurations known as small-world networks, which are characterized by high clustering and short path lengths (<xref rid="B81" ref-type="bibr">Sporns and Honey, 2006</xref>). This type of network structure allows the co-existence of functionally segregated modules (e.g., visual cortex and auditory cortex) yet also provide efficient ways for their integration when needed. Second, <italic>FC</italic> is constrained by anatomical connectivity (<xref rid="B40" ref-type="bibr">Goñi et al., 2014</xref>); which is also highly organized and far from random. Third, when the brain engages in different cognitive functions, <italic>FC</italic> changes accordingly (<xref rid="B41" ref-type="bibr">Gonzalez-Castillo and Bandettini, 2018</xref>); yet those changes are limited (<xref rid="B21" ref-type="bibr">Cole et al., 2014</xref>; <xref rid="B58" ref-type="bibr">Krienen et al., 2014</xref>), and global properties of the functional connectome, such as its small-worldness, are preserved as the brain transitions between task and rest states (<xref rid="B7" ref-type="bibr">Bassett et al., 2006</xref>). Forth, <italic>tvFC</italic> matrices have structure in both the connectivity and time dimensions. On the connectivity axis, pair-wise connections tend to organize into networks (i.e., sets of regions with higher connectivity among themselves than to the rest of the brain) that are reproducible across scans and across participants. On the temporal axis, connectivity time-series show temporal autocorrelation due to the sluggishness of the hemodynamic response and the use of overlapping sliding windows. Fifth, previous attempts at applying manifold learning to <italic>tvFC</italic> data have proven successful at generating meaningful low dimensional representations that capture differences in <italic>FC</italic> across mental states (<xref rid="B6" ref-type="bibr">Bahrami et al., 2019</xref>; <xref rid="B42" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>; <xref rid="B39" ref-type="bibr">Gao et al., 2021</xref>), sleep stages (<xref rid="B77" ref-type="bibr">Rué-Queralt et al., 2021</xref>), and populations (<xref rid="B6" ref-type="bibr">Bahrami et al., 2019</xref>; <xref rid="B71" ref-type="bibr">Miller et al., 2022</xref>). The same is true for static FC data (<xref rid="B16" ref-type="bibr">Casanova et al., 2021</xref>).</p>
        <p>Methods aimed at finding non-linear surfaces or manifolds are commonly referred to as manifold learning methods. It is important to note that the word “<italic>learning</italic>” does not denote the need for labels or that these methods should be regarded as “<italic>supervised</italic>.” The use of the word “<italic>learning</italic>” here is aimed at signaling that the goal of these manifold learning methods—and others not discussed here—is to discover (i.e., learn) an intrinsic low-dimensional non-linear structure—namely the manifold—where the data lies. It is through that process that manifold learning methods accomplish the goal of reducing the dimensionality of data and are able to map the data from the high dimensional input space (the ambient space) to a lower dimensional space (that of the embedding) in a way that preserves important geometric relationships between datapoints.</p>
      </sec>
      <sec id="S2.SS2">
        <title>Intrinsic dimension</title>
        <p>One important property of data is their <italic>ID</italic> (<xref rid="B14" ref-type="bibr">Campadelli et al., 2015</xref>), namely the minimum number of variables (i.e., dimensions) required to describe the manifold where the data lie with little loss of information. Given the above-mentioned constrains that apply to the generative process of <italic>tvFC</italic> data, it is reasonable to expect that the <italic>ID</italic> of <italic>tvFC</italic> data will be significantly smaller than that of the original ambient space, yet it may still be a number well above three. Because <italic>ID</italic> informs us about the minimum number of variables needed to faithfully represent the data, having an estimate of what is the <italic>ID</italic> of <italic>tvFC</italic> data is key. For example, if the <italic>ID</italic> is greater than three, one should not restrict visual exploration of low dimensional representations of <italic>tvFC</italic> data to the first three dimensions and should also explore dimensions above those. Similarly, if manifold estimation is used to compress the data or extract features for a subsequent classification step, knowing the <italic>ID</italic> can help us decide how many dimensions (i.e., features) to keep. Finally, <italic>ID</italic> of a dataset can also be thought of as a measure of the complexity of the data (<xref rid="B4" ref-type="bibr">Ansuini et al., 2019</xref>). In the context of <italic>tvFC</italic> data, such a metric might have clinical and behavioral relevance.</p>
        <p>Intrinsic dimension estimation is currently an intense area of research (<xref rid="B29" ref-type="bibr">Facco et al., 2017</xref>; <xref rid="B1" ref-type="bibr">Albergante et al., 2019</xref>), with <italic>ID</italic> estimation methods in continuous evolution to address open issues such as computational complexity, under-sampled distributions, and dealing with datasets that reside in multiple manifolds [see <xref rid="B14" ref-type="bibr">Campadelli et al. (2015)</xref> for an in-depth review of these issues]. Because no consensus exists on how to optimally select an <italic>ID</italic> estimator, here we will compare estimates from three state-of-the-art methods, namely local PCA (<italic>lPCA</italic>; <xref rid="B31" ref-type="bibr">Fan et al., 2010</xref>), two nearest neighbors (<italic>twoNN</italic>; <xref rid="B29" ref-type="bibr">Facco et al., 2017</xref>), and Fisher separability (<italic>FisherS</italic>; <xref rid="B1" ref-type="bibr">Albergante et al., 2019</xref>). These three <italic>ID</italic> estimators were selected because of their complementary nature on how they estimate <italic>ID</italic>, robustness against data redundancy and overall performance (<xref rid="B5" ref-type="bibr">Bac et al., 2021</xref>). In all instances, we will report both global and local <italic>ID</italic> (<italic>ID</italic><sub><italic>local</italic></sub>) estimates. The global <italic>ID</italic> (<italic>ID</italic><sub><italic>global</italic></sub>) is a single <italic>ID</italic> estimate per dataset generated using all data samples. It works under the assumption that the whole dataset has the same <italic>ID</italic> (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 1C</xref> for a counter example). Conversely, <italic>ID</italic><sub><italic>local</italic></sub> estimates are computed on a sample-by-sample basis using small vicinities of size determined by the number of neighbors (<italic>k</italic><sub><italic>nn</italic></sub>). In that way, <italic>ID</italic><sub><italic>local</italic></sub> estimates can help identify regions with different <italic>IDs</italic>; yet its accuracy is more dependent on noise levels and the relative size of the data curvature with respect to <italic>k</italic><sub><italic>nn</italic></sub>. See <xref rid="DS1" ref-type="supplementary-material">Supplementary Note 2</xref> for additional details on the relationship between <italic>ID</italic><sub><italic>global</italic></sub> and <italic>ID</italic><sub><italic>local</italic></sub>, and about how <italic>k</italic><sub><italic>nn</italic></sub> can affect <italic>ID</italic><sub><italic>local</italic></sub> estimation.</p>
      </sec>
      <sec id="S2.SS3">
        <title>Laplacian Eigenmaps</title>
        <p>The first method that we evaluate is the <italic>LE</italic> algorithm originally described by <xref rid="B8" ref-type="bibr">Belkin and Niyogi (2003)</xref> and publicly available as part of the <italic>scikit-learn</italic> library (<xref rid="B74" ref-type="bibr">Pedregosa et al., 2011</xref>). In contrast to linear dimensionality reduction methods (e.g., <italic>PCA</italic>) that seek to preserve the global structure of the data, <italic>LE</italic> attempts to preserve its local structure. Importantly, this bias toward preservation of local over global structure facilitates the discovery of natural clusters in the data.</p>
        <p>The <italic>LE</italic> algorithm starts by constructing an undirected graph (<italic>G</italic>) from the data (<xref rid="F2" ref-type="fig">Figure 2D</xref>). In <italic>G</italic>, each node represents a sample (i.e., a column of the <italic>tvFC</italic> matrix; <xref rid="F2" ref-type="fig">Figure 2A</xref>), and edges are drawn only between nodes associated with samples that are “<italic>close</italic>” in original space. The construction of <italic>G</italic> proceeds in two steps. First, a dissimilarity matrix (<italic>DS</italic>) is computed (<xref rid="F2" ref-type="fig">Figure 2B</xref>). For this, one must choose a distance function (<italic>d</italic>). Common choices include the <italic>Euclidean</italic>, <italic>Correlation</italic>, and <italic>Cosine</italic> distances (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Note 3</xref> for additional details about these distance metrics). Next, this <italic>DS</italic> matrix is transformed into an affinity matrix (<italic>W</italic>) using the <italic>N-nearest neighbors</italic> algorithm (<xref rid="F2" ref-type="fig">Figure 2C</xref>). In <italic>W</italic>, the entry for <italic>i</italic> and <italic>j</italic> (<italic>W</italic><sub><italic>ij</italic></sub>) is equal to 1 (signaling the presence of an edge) if and only if node <italic>i</italic> is among the <italic>K<sub><italic>nn</italic></sub></italic> nearest neighbors of node <italic>j</italic> (<italic>i</italic> → <italic>j</italic>) or <italic>j</italic> is among the <italic>K</italic><sub><italic>nn</italic></sub> nearest neighbors of node <italic>i (j</italic> → <italic>i)</italic>. Otherwise <italic>W</italic><sub><italic>ij</italic></sub> equals zero. An affinity matrix constructed this way is equivalent to an undirected, unweighted graph (<xref rid="F2" ref-type="fig">Figure 2D</xref>). According to <xref rid="B8" ref-type="bibr">Belkin and Niyogi (2003)</xref>, it is also possible to work with a weighted version of the graph, for example:</p>
        <disp-formula id="S2.E3">
<label>(3)</label>
<mml:math id="M3" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mtext>if</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mi>j</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:mtext>and</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0.5</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:mpadded width="+5pt"><mml:mtext>if</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mrow><mml:mpadded width="+5pt"><mml:mi>j</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:mtext>or</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <fig position="float" id="F2">
          <label>FIGURE 2</label>
          <caption>
            <p>The Laplacian Eigenmap algorithm. <bold>(A)</bold> Representative <italic>tvFC</italic> matrix for a multi-task run from <xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref>, which consists of a scan acquired continuously as participants engage and transition between four different mental tasks (2-back, math, visual attention (VA), and rest). Additional details in the Dataset portion of the section “Materials and methods.” Columns indicate windows and rows indicate connections. <bold>(B)</bold> Dissimilarity matrix for the <italic>tvFC</italic> matrix in panel <bold>(A)</bold> computed using the Euclidean distance function. <bold>(C)</bold> Affinity matrix computed for panel <bold>(B)</bold> using <italic>K</italic><sub><italic>nn</italic></sub> = 90 neighbors. Black cells indicate 1 (i.e., an edge exists) and white indicate zero (no edge). <bold>(D)</bold> Graph visualization of the affinity matrix in panel <bold>(C)</bold>. In the top graph all nodes are colored in white to highlight that the <italic>LE</italic> algorithm used no information about the tasks at any moment. The bottom graph is the same as the one above, but now nodes are colored by task to make apparent how the graph captures the structure of the data (e.g., clusters together windows that correspond to the same experimental task). <bold>(E)</bold> Final embedding for <italic>m</italic> = 3. This embedding faithfully presents the task structure of the data. <bold>(F)</bold> Final embedding for <italic>m</italic> = 2. In this case, the windows for rest and memory overlap. Red arrows and text indicate decision points in the algorithm. Step-by-step code describing the <italic>LE</italic> algorithm and used to create the different panels of this figure can be found in the code repository that accompanies this publication (<italic>Notebook N03_Figure02_Theory_LE.ipynb</italic>).</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g002" position="float"/>
        </fig>
        <p>This alternative version is the one used in the implementation of the <italic>scikit-learn</italic> library used in this work.</p>
        <p>Once the graph is built, the next step is to obtain the Laplacian matrix (<italic>L</italic>)<sup><xref rid="footnote1" ref-type="fn">1</xref></sup> of the graph, which is defined as</p>
        <disp-formula id="S2.E4">
<label>(4)</label>
<mml:math id="M4" overflow="scroll"><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>L</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>-</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math>
</disp-formula>
        <p>In Eq. 4, <italic>W</italic> is the affinity matrix (Eq. 3), and <italic>D</italic> is a matrix that holds information about the degree (i.e., number of connections) of each node on the diagonal and zeros elsewhere. The last step of the <italic>LE</italic> algorithm is to extract eigenvalues (λ<sub>0</sub> ≤ λ<sub>1</sub> ≤ … ≤ λ<sub><italic>k</italic>−1</sub>) and eigenvectors (<italic>f</italic><sub>0</sub>, …, <italic>f</italic><sub><italic>k</italic>−1</sub>) by solving</p>
        <disp-formula id="S2.E5">
<label>(5)</label>
<mml:math id="M5" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+3.3pt"><mml:mi>f</mml:mi></mml:mpadded></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi mathvariant="normal">λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math>
</disp-formula>
        <p>Once those are available, the embedding of a given sample <italic>x</italic> in a lower dimensional space with <italic>m</italic> ≪ <italic>k</italic> dimensions is given by:</p>
        <disp-formula id="S2.E6">
<label>(6)</label>
<mml:math id="M6" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>x</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <p>The first eigenvector <italic>f</italic><sub>0</sub> is ignored because its associated eigenvalue λ<sub>0</sub> is always zero. For those interested in a step-by-step mathematical justification of why the spectral decomposition of <italic>L</italic> renders a representation of the data that preserves local information, read <italic>Section 3</italic> of the original work by <xref rid="B8" ref-type="bibr">Belkin and Niyogi (2003)</xref>. Intuitively, the <italic>Laplacian</italic> matrix is a linear operator that holds information about all between-sample relationships in the manifold and the eigenvectors obtained via its spectral decomposition provide a set of orthonormal bases.</p>
        <p>In summary, the <italic>LE</italic> algorithm requires, at a minimum, the selection of a distance function (<italic>d</italic>), and a neighborhood size (<italic>k</italic><sub><italic>nn</italic></sub>). These two hyper-parameters (marked in red in <xref rid="F2" ref-type="fig">Figure 2</xref>) determine the construction of <italic>G</italic> because they mathematically specify what it means for two <italic>tvFC</italic> patterns to be similar (or graph nodes to be connected). Because the <italic>LE</italic> algorithm does not look back at the input data once <italic>G</italic> is constructed, and all algorithmic steps past the construction of <italic>G</italic> are fixed, appropriately selecting <italic>d</italic> and <italic>k</italic><sub><italic>nn</italic></sub> is key for the generation of biologically meaningful embeddings of <italic>tvFC</italic> data. Finally, as with any dimensionality reduction technique, one must also select how many dimensions to explore (<italic>m;</italic>
<xref rid="F2" ref-type="fig">Figures 2E, F</xref>), but in the case of <italic>LE</italic> such decision does not affect the inner workings of the algorithm.</p>
      </sec>
      <sec id="S2.SS4">
        <title>T-distributed Stochastic Neighbor Embedding (T-SNE)</title>
        <p>The second technique evaluated here is <italic>T-SNE</italic> (<xref rid="B86" ref-type="bibr">van der Maaten and Hinton, 2008</xref>), which is a commonly used method for visualizing high dimensional biomedical data in two or three dimensions. Like <italic>LE</italic>, <italic>T-SNE</italic>’s goal is to generate representations that give priority to the preservation of local structure. These two methods are also similar in that their initial step requires the selection of a distance function used to construct a <italic>DS</italic> (<xref rid="F3" ref-type="fig">Figure 3B</xref>) that will subsequently be transformed into an affinity matrix (<italic>P</italic>; <xref rid="F3" ref-type="fig">Figure 3C</xref>). Yet, <italic>T-SNE</italic> uses a very different approach to go from <italic>DS</italic> to <italic>P</italic>. Instead of relying on the <italic>N</italic>-nearest <italic>neighbor</italic> algorithm, <italic>T-SNE</italic> models pair-wise similarities in terms of probability densities. Namely, the affinity between two points <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub></italic> in original space is given by the following conditional <italic>Gaussian</italic> distribution:</p>
        <disp-formula id="S2.E7">
<label>(7)</label>
<mml:math id="M7" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mpadded width="+0.3pt"><mml:mi>k</mml:mi></mml:mpadded><mml:mo rspace="1.8pt">≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula>
        <fig position="float" id="F3">
          <label>FIGURE 3</label>
          <caption>
            <p>The <italic>T-SNE</italic> algorithm. <bold>(A)</bold> Representative <italic>tvFC</italic> matrix for a multi-task run (same as in <xref rid="F2" ref-type="fig">Figure 2</xref>). Columns indicate windows and rows indicate connections. <bold>(B)</bold> Dissimilarity matrix for the <italic>tvFC</italic> matrix in panel <bold>(A)</bold> computed using the Euclidean distance function. <bold>(C)</bold> Affinity matrix generated using Eqs 3, 4 and a perplexity value of 100. <bold>(D)</bold> Random lower-dimensional representation of the data. Each dot represents one column of the data in panel <bold>(A)</bold>. <bold>(E)</bold> Dissimilarity matrix for the data in panel <bold>(D)</bold> computed using the <italic>Euclidean</italic> distance function. <bold>(F)</bold> Affinity matrix for the data in panel <bold>(D)</bold> computed using a T-distribution function as in Eq. 5. <bold>(G)</bold> Evolution of the cost function with the number of gradient descent iterations. In this execution, the early exaggeration factor was set to 4 for the initial 100 iterations [as originally described by <xref rid="B86" ref-type="bibr">van der Maaten and Hinton (2008)</xref>]. A dashed vertical line marks the iteration when the early exaggeration factor was removed (early exaggeration phase highlighted in light blue). Below the cost function evolution curve, we show embeddings and affinity matrices for a set of representative iterations. Iterations corresponding to the early exaggeration periods are shown on the left, while iterations for the post early exaggeration period are shown on the right. In the embeddings, points are colored according to the task being performed on each window. Windows that contain more than one task are marked in pink with the label “XXXX.” Step-by-step code describing a basic implementation of the <italic>T-SNE</italic> algorithm and used to create the different panels of this figure can be found in the code repository that accompanies this publication (<italic>Notebook N04_Figure03_Theory_TSNE.ipynb</italic>).</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g003" position="float"/>
        </fig>
        <p>As such, <italic>T-SNE</italic> conceptually defines the affinity between <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub></italic> as the likelihood that <italic>x<sub>i</sub></italic> would pick <italic>x<sub>j</sub></italic> as its neighbor, with the definition of neighborhood given by a <italic>Gaussian</italic> kernel of width (<inline-formula><mml:math id="INEQ11" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>) centered at <italic>x<sub>i</sub></italic>. The width of the kernel (<inline-formula><mml:math id="INEQ12" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>) is sample-dependent to accommodate datasets with varying densities (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 2</xref> for an example) and ensure all neighborhoods are equivalent in terms of how many samples they encompass. In fact, <italic>T-SNE</italic> users do not set <inline-formula><mml:math id="INEQ13" overflow="scroll"><mml:msubsup><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> directly but select neighborhood size via the <italic>perplexity (PP)</italic> parameter, which can be thought of as an equivalent to <italic>k</italic><sub><italic>nn</italic></sub> in the <italic>LE</italic> algorithm. See <xref rid="DS1" ref-type="supplementary-material">Supplementary Note 4</xref> for more details on how perplexity relates to neighborhood size.</p>
        <p>Because conditional probabilities are not symmetric, entries in the affinity matrix <italic>P</italic> are defined as follows in terms of the conditional probabilities defined in Eq. 7:</p>
        <disp-formula id="S2.E8">
<label>(8)</label>
<mml:math id="M8" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math>
</disp-formula>
        <p>T-distributed Stochastic Neighbor Embedding proceeds next by generating an initial set of random coordinates for all samples in the lower <italic>m</italic> dimensional space (Υ<sub><italic>init</italic></sub>; <xref rid="F3" ref-type="fig">Figure 3D</xref>). Once Υ<sub><italic>init</italic></sub> is available, a dissimilarity (<xref rid="F3" ref-type="fig">Figure 3E</xref>) and an affinity matrix (<italic>Q;</italic>
<xref rid="F3" ref-type="fig">Figure 3F</xref>) are also computed for this random initial low dimensional representation of the data. To transform <italic>DS</italic> into <italic>Q</italic>, <italic>T-SNE</italic> uses a <italic>T-Student</italic> distribution (Eq. 9) instead of a <italic>Gaussian</italic> distribution. The reason for this is that <italic>T-Student</italic> distributions have heavier tails than <italic>Gaussian</italic> distributions, which in the context of <italic>T-SNE</italic>, translates into higher affinities for distant points. This gives <italic>T-SNE</italic> the ability to place distant points further apart in the lower dimensional representation and use more space to model the local structure of the data.</p>
        <disp-formula id="S2.E9">
<label>(9)</label>
<mml:math id="M9" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mpadded width="+0.3pt"><mml:mi>k</mml:mi></mml:mpadded><mml:mo rspace="1.8pt">≠</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</disp-formula>
        <p>The <italic>T-SNE</italic> steps presented so far constitute the setup phase of the algorithm (<xref rid="F3" ref-type="fig">Figures 3A–F</xref>). From this point onward (<xref rid="F3" ref-type="fig">Figure 3G</xref>), the <italic>T-SNE</italic> algorithm proceeds as an optimization problem where the goal is to update Υ (e.g., the locations of the points in lower <italic>m</italic> dimensional space) in a way that makes <italic>P</italic> and <italic>Q</italic> most similar (e.g., match pair-wise distances). <italic>T-SNE</italic> solves this optimization problem using gradient descent to minimize the <italic>Kullback–Leibler</italic> (<italic>KL</italic>) divergence between both affinity matrices (Eq. 10).</p>
        <disp-formula id="S2.E10">
<label>(10)</label>
<mml:math id="M10" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>C</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mi>K</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mpadded width="+0.3pt"><mml:mo stretchy="false">|</mml:mo></mml:mpadded><mml:mo stretchy="false">|</mml:mo><mml:mi>Q</mml:mi><mml:mo rspace="2.8pt">)</mml:mo></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math>
</disp-formula>
        <p><xref rid="F3" ref-type="fig">Figure 3G</xref> shows how <italic>C</italic> evolves with the number of gradient descent iterations. Below the <italic>C</italic> curve we show intermediate low dimensional representations (Υ) and affinity matrices (<italic>Q</italic>) at representative iterations. In this execution of <italic>T-SNE</italic>, it takes approximately 20–50 iterations for Υ to present some meaningful structure: rest windows (gray dots) being together on the top left corner of the embedding and VA windows (yellow dots) being on the bottom right corner. As the number of iterations grows the profile becomes more distinct, and windows associated with the math (green) and WM tasks (blue) also start to separate. Because <italic>T-SNE</italic>’s optimization problem is non-convex, modifications to the basic gradient descent algorithm are needed to increase the chances of obtaining meaningful low dimensional representations. These include, among others, early compression (i.e., the addition of an L2-penalty term to the cost function during initial iterations), early exaggeration (i.e., a multiplicative term on <italic>P</italic> during initial iterations), and an adaptative learning rate procedure. For example, <xref rid="F3" ref-type="fig">Figure 3G</xref> shows the effects of early exaggeration. At iteration 100, when early exaggeration is removed, <italic>C</italic> sharply decreases as <italic>P</italic> suddenly becomes much closer in value to <italic>Q</italic>. As the optimization procedures continues beyond that point, we can observe how the temporal autocorrelation inherent to <italic>tvFC</italic> data dominates the structure of the embedding (e.g., continuous spaghetti-like structure), yet windows corresponding to the two different periods of each task still appear close to each other showing the ability of the embedding to also preserve behaviorally relevant information.</p>
        <p>These optimization “<italic>tricks</italic>” result into additional hyper-parameters that can affect the behavior of <italic>T-SNE</italic>, yet not all of them are always accessible to the user. For example, in the <italic>scikit-learn</italic> library (the one used in this work), one can set the early exaggeration factor, but not the number of gradient descent iterations to which it applies. Given optimization of gradient descent is beyond the scope of this work, here we focus our investigation only on the effects of distance metric (<italic>d</italic>), perplexity (<italic>PP</italic>), and learning rate (α). Similarly, it is also worth noting that the description of <italic>T-SNE</italic> provided in this section corresponds roughly to that originally proposed by <xref rid="B86" ref-type="bibr">van der Maaten and Hinton (2008)</xref>. Since then, several modifications have been proposed, such as the use of the <italic>Barnes–Hut</italic> approximation to reduce computational complexity (<xref rid="B64" ref-type="bibr">Maaten, 2014</xref>), and the use of <italic>PCA</italic> initialization to introduce information about the global structure of the data during the initialization process (<xref rid="B56" ref-type="bibr">Kobak and Berens, 2019</xref>). These two modifications are available in <italic>scikit-learn</italic> and will be used as defaults in the analyses described below.</p>
        <p>In summary, although <italic>T-SNE</italic> and <italic>LE</italic> share the goal of generating low dimensional representations that preserve local structure and rely on topological representations (i.e., graphs) to accomplish that goal, the two methods differ in important aspects. First, the number of hyper-parameters is much higher for <italic>T-SNE</italic> than <italic>LE</italic>. This is because the <italic>T-SNE</italic> algorithm contains a highly parametrized optimization problem with options for the learning rate, early exaggeration, early compression, Barnes-Hut radius, and initialization method to use. Second, in <italic>LE</italic> the number of desired dimensions (<italic>m</italic>) is used to select the number of eigenvectors to explore, and as such, it does not affect the outcome of the algorithm in any other way. That is not the case for <italic>T-SNE</italic>, where <italic>m</italic> determines the dimensionality of Υ at all iterations, and therefore the space that the optimization problem can explore in search of a solution. In other words, if one were to run LE for <italic>m = 3</italic> and later decide to only explore the first two dimensions, that would be a valid approach as the first two dimensions of the solution for <italic>m = 3</italic> are equivalent to the two dimensions of the solution for <italic>m = 2</italic>. The same is not true for <italic>T-SNE</italic>, which would require separate executions for each scenario (<italic>m = 2</italic> and <italic>m = 3</italic>).</p>
      </sec>
      <sec id="S2.SS5">
        <title>Uniform Manifold Approximation and Projection (UMAP)</title>
        <p>The last method considered here is <italic>UMAP</italic> (<xref rid="B67" ref-type="bibr">McInnes et al., 2018</xref>). <italic>UMAP</italic> is, as of this writing, the latest addition to the family of dimensionality reduction techniques based on neighboring graphs. Here we will describe <italic>UMAP</italic> only from a computational perspective that allow us to gain an intuitive understanding of the technique and its key hyperparameters. Yet, it is worth noting that <italic>UMAP</italic> builds on strong mathematical foundations from the field of topological data analysis (<italic>TDA</italic>; <xref rid="B15" ref-type="bibr">Carlsson, 2009</xref>), and that, it is those foundations [as described in Section 2 of <xref rid="B67" ref-type="bibr">McInnes et al. (2018)</xref>], that justify each procedural step of the <italic>UMAP</italic> algorithm. For those interested in gaining a basic understanding of <italic>TDA</italic> we recommend these two works: (<xref rid="B18" ref-type="bibr">Chazal and Michel, 2021</xref>) which is written for data scientists as the target audience, and (<xref rid="B79" ref-type="bibr">Sizemore et al., 2019</xref>) which is more specific to applications in neuroscience.</p>
        <p>Uniform Manifold Approximation and Projection can be described as having two phases: (1) the construction of an undirected weighted graph for the data, and (2) the computation of a low dimensional layout for the graph. Phase one proceeds as follows. First, a <italic>DS</italic> (<xref rid="F4" ref-type="fig">Figure 4B</xref>) is computed using the user-selected distance function <italic>d</italic>. This <italic>DS</italic> matrix is then transformed into a binary adjacency matrix (<italic>A;</italic>
<xref rid="F4" ref-type="fig">Figure 4C</xref>) using the <italic>k-nearest neighbor</italic> algorithm and a user-selected number of neighbors (<italic>k</italic><sub><italic>nn</italic></sub>). This is similar to the first steps in <italic>LE</italic>, except that here matrix <italic>A</italic> defines a directed (as opposed to undirected in <italic>LE</italic>) unweighted graph <italic>G</italic><sub><italic>a</italic></sub> = (<italic>V</italic>, <italic>E</italic>, <italic>w</italic><sub><italic>a</italic></sub>) (<xref rid="F4" ref-type="fig">Figure 4D</xref>) where <italic>V</italic> is a set of nodes/vertices representing each data sample (i.e., a window of connectivity), <italic>E</italic> is the set of edges signaling detected neighboring relationships, and <italic>w<sub>a</sub></italic> equals 1 for all edges in <italic>E</italic>. Third, <italic>UMAP</italic> computes an undirected weighted graph <italic>G</italic><sub><italic>b</italic></sub> = (<italic>V</italic>, <italic>E</italic>, <italic>w</italic><sub><italic>b</italic></sub>) (<xref rid="F4" ref-type="fig">Figure 4F</xref>) with the same set of nodes and edges of <italic>G<sub>a</sub></italic>, but with weights <italic>w<sub>b</sub></italic> given by</p>
        <disp-formula id="S2.E11">
<label>(11)</label>
<mml:math id="M11" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <fig position="float" id="F4">
          <label>FIGURE 4</label>
          <caption>
            <p><bold>(A)</bold> Input <italic>tvFC</italic> data (same as in <xref rid="F2" ref-type="fig">Figure 2</xref>). <bold>(B)</bold> Dissimilarity matrix obtained using the Euclidean distance as a distance function. <bold>(C)</bold> Binary non-symmetric affinity matrix for <italic>K</italic><sub><italic>nn</italic></sub> = 90. <bold>(D)</bold> Graph equivalent of the affinity matrix in panel <bold>(C)</bold>. <bold>(E)</bold> Effect of the distance normalization step on the original dissimilarities between neighboring nodes. <bold>(F)</bold> Graph associated with the normalized affinity matrix. <bold>(G)</bold> Final undirected graph after application of Eq. 15. This is the input to the optimization phase. <bold>(H)</bold> Representative embeddings at different epochs of the stochastic gradient descent algorithm for an initial learning rate equal to 1. <bold>(I)</bold> Same as panel <bold>(H)</bold> but when the initial learning rate is set to 0.01. <bold>(J)</bold> Difference between embeddings at consecutive epochs measured as the average Euclidean distance across all node locations for an initial learning rate of 1 (dark gray) and 0.01 (light gray). Step-by-step code describing a basic implementation of the UMAP algorithm and used to create the different panels of this figure can be found in the code repository that accompanies this publication (<italic>Notebook N05_Figure04_Thoery_UMAP.ipynb</italic>).</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g004" position="float"/>
        </fig>
        <p>where <italic>x</italic><sub><italic>i<sub>j</sub></italic></sub> refers to the <italic>j</italic>-th nearest neighbor of node <italic>x<sub>i</sub></italic> with <italic>j</italic> = {1..<italic>k</italic><sub><italic>nn</italic></sub>}, <italic>d</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>i</italic><sub><italic>j</italic></sub></sub>) is their dissimilarity as provided by the distance function <italic>d</italic>, and ρ<sub><italic>i</italic></sub> and σ<sub><italic>i</italic></sub> are node-specific normalization factors given by Eqs 12, 13 below.</p>
        <disp-formula id="S2.E12">
<label>(12)</label>
<mml:math id="M12" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi mathvariant="normal">ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>i</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">≤</mml:mo><mml:mpadded width="+3.3pt"><mml:mi>j</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">≤</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">&gt;</mml:mo><mml:mpadded width="+5pt"><mml:mn>0</mml:mn></mml:mpadded></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <disp-formula id="S2.E13">
<label>(13)</label>
<mml:math id="M13" overflow="scroll"><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mpadded width="+1.3pt"><mml:mi>j</mml:mi></mml:mpadded><mml:mo rspace="2.8pt">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="normal">ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msub><mml:mi mathvariant="normal">σ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <p>By constructing <italic>G<sub>b</sub></italic> this way, <italic>UMAP</italic> ensures that in <italic>G<sub>b</sub></italic> all nodes are connected to at least one other node with a weight of one, and that the data is represented as if it were uniformly distributed on the manifold in ambient space. Practically speaking, Eqs 11 through 13 transform original dissimilarities between neighboring nodes into exponentially decaying curves in the range [0,1] (<xref rid="F4" ref-type="fig">Figure 4E</xref>).</p>
        <p>Finally, if we describe <italic>G<sub>b</sub></italic> in terms of an affinity matrix <italic>B</italic></p>
        <disp-formula id="S2.E14">
<label>(14)</label>
<mml:math id="M14" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo rspace="7.5pt">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mpadded width="+5pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mpadded width="+5pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mi/></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <p>Then, we can generate a symmetrized version of matrix <italic>B</italic>, called <italic>C</italic>, as follows:</p>
        <disp-formula id="S2.E15">
<label>(15)</label>
<mml:math id="M15" overflow="scroll"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mi>C</mml:mi></mml:mpadded><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mpadded width="+0.3pt"><mml:mi>B</mml:mi></mml:mpadded><mml:mo rspace="1.8pt">∘</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <p>This matrix <italic>C</italic> represents graph <italic>G</italic><sub><italic>c</italic></sub> = (<italic>V</italic>, <italic>E</italic>, <italic>w</italic><sub><italic>c</italic></sub>) (<xref rid="F4" ref-type="fig">Figure 4G</xref>), which is the undirected weighted graph whose layout is optimized during phase 2 of the UMAP algorithm as described next.</p>
        <p>Once <italic>G</italic><sub><italic>c</italic></sub> is available, the second phase of the <italic>UMAP</italic> algorithm is concerned with finding a set of positions for all nodes ({<italic>Y</italic><sub><italic>i</italic></sub>}<sub><italic>i</italic> = 1..<italic>N</italic></sub>) in ℝ<sup><italic>m</italic></sup>, with <italic>m</italic> being the desired number of user-selected final dimensions. For this, <italic>UMAP</italic> uses a force-directed graph-layout algorithm that positions nodes using a set of attractive and repulsive forces proportional to the graph weights. An equivalent way to think about this second phase of <italic>UMAP</italic>—that renders a more direct comparison with <italic>T-SNE</italic>—is in terms of an optimization problem attempting to minimize the edge-wise cross-entropy (Eq. 16) between <italic>G<sub>c</sub></italic> and an equivalent weighted graph <italic>H</italic> = (<italic>V</italic>, <italic>E</italic>, <italic>w</italic><sub><italic>h</italic></sub>) with node layout given by {<italic>Y</italic><sub><italic>i</italic></sub>}<sub><italic>i</italic> = 1..<italic>N</italic></sub> ∈ ℝ<sup><italic>m</italic></sup>. The goal being to find a layout for <italic>H</italic> that makes <italic>H</italic> and <italic>G<sub>c</sub></italic> are as similar as possible as dictated by the edge-wise cross-entropy function.</p>
        <disp-formula id="S2.E16">
<label>(16)</label>
<mml:math id="M16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo rspace="5.8pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.8pt">=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mmultiscripts><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:none/></mml:mmultiscripts><mml:mmultiscripts><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:none/></mml:mmultiscripts></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <disp-formula id="S2.Ex1">
<mml:math id="M17" overflow="scroll"><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mmultiscripts><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:none/></mml:mmultiscripts></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mmultiscripts><mml:mi>w</mml:mi><mml:mi>c</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:none/></mml:mmultiscripts></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mmultiscripts><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:none/><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:none/></mml:mmultiscripts></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math>
</disp-formula>
        <p>If we compare T-SNE’s (Eq. 10) and UMAP’s (Eq. 16) optimization cost functions, we can see that Eq. 10 is equivalent to the left term of Eq. 16. This left term represents the set of attractive forces along the edges that is responsible for positioning together similar nodes in ℝ<sup><italic>m</italic></sup>. Conversely, the right term of Eq. 16 represents a set of repulsive forces between nodes that are responsible for enforcing gaps between dissimilar nodes. This additional term helps UMAP preserve some of the global structure of the data while still capturing local structure.</p>
        <p><xref rid="F4" ref-type="fig">Figures 4H, I</xref> exemplify how UMAP phase 2 proceeds for two different learning rates (a key hyper-parameter of the optimization algorithm). These two panels in <xref rid="F4" ref-type="fig">Figure 4</xref> show embeddings at representative epochs (e.g., 2, 5, 11, 100, 350, 499, and 500). Additionally, <xref rid="F4" ref-type="fig">Figure 4J</xref> shows the average <italic>Euclidean</italic> distance between all nodes at two consecutive epochs. This allows us to evaluate if optimization proceeds smoothly with small changes in the embedding from one step to the next, or abruptly. An abrupt optimization process is not desirable because, if so, a small change in the number of epochs to run can lead to substantially different results. <xref rid="F4" ref-type="fig">Figure 4I</xref> shows how when the learning rate is set to 0.01, optimization proceeds abruptly only at the initial stages (<italic>N</italic><sub><italic>epoch</italic></sub> &lt; 100) and then stabilizes. In this case a small change in the maximum number of epochs to run will not affect the results. Moreover, the embedding for <italic>N</italic><sub><italic>epoch</italic></sub> = 500 in <xref rid="F4" ref-type="fig">Figure 4I</xref> has correctly captured the structure of the data (i.e., the tasks). Next, in <xref rid="F4" ref-type="fig">Figure 4H</xref> we observe that the same is not true for a learning rate of 1 (the default value in the <italic>umap-learn</italic> library). In this case, embeddings substantially change from one epoch to the next all the way to <italic>N</italic><sub><italic>epoch</italic></sub> = 500. These results highlight the strong effects that hyper-parameters associated with optimization phase of <italic>UMAP</italic> can have when working with <italic>tvFC</italic> data.</p>
        <p>In summary, although <italic>UMAP</italic> shares many conceptual and practical aspects with <italic>LE</italic> and <italic>T-SNE</italic>, it differs in important ways on the specifics of how a graph is generated from the data and how this graph is translated into a low dimensional embedding. Similarly to <italic>LE</italic> and T-SNE, <italic>UMAP</italic> requires careful selection of distance metric (<italic>d</italic>), neighborhood size (<italic>k</italic><sub><italic>nn</italic></sub>) and the dimensionality of the final space (<italic>m</italic>). In addition, like <italic>T-SNE</italic>, <italic>UMAP</italic> exposes many additional parameters associated with its optimization phase. Here we have only discussed the learning rate and maximum number of epochs, but many other are available. For a complete list please check the <italic>umap-learn</italic> documentation. For this work, unless expressed otherwise, we will use default values for all other hyper-parameters. Finally, there is one more hyper-parameter specific to <italic>UMAP</italic> that we have not yet discussed called minimum distance (<italic>min_dist</italic>). Its value determines the minimum separation between closest samples in the embedding. In that way, <italic>min_dist</italic> controls how tightly together similar samples appear in the embedding (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Note 5</xref> for additional details).</p>
      </sec>
    </sec>
    <sec sec-type="materials|methods" id="S3">
      <title>Materials and methods</title>
      <sec id="S3.SS1">
        <title>Dataset</title>
        <p>This work is conducted using a multi-task dataset previously described in detail in <xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al. (2015</xref>, <xref rid="B42" ref-type="bibr">2019</xref>). In summary, it contains data from 22 subjects (13 females; age 27 ± 5) who gave informed consent in compliance with a protocol approved by the Institutional Review Board of the National Institute of Mental Health in Bethesda, MD, USA. The data from two subjects were discarded from the analysis due to excessive spatial distortions in the functional time series.</p>
        <p>Subjects were scanned continuously for 25 min and 24 s while performing four different tasks: rest with eyes open (<italic>REST</italic>), simple mathematical computations (<italic>MATH</italic>), 2-back <italic>WM</italic>, and <italic>VA</italic>/recognition. Each task occupied two separate 180-s blocks, preceded by a 12 s instruction period. Task blocks were arranged so that each task was always preceded and followed by a different task. Additional details can be found on the <xref rid="DS1" ref-type="supplementary-material">Supplementary material</xref> accompanying <xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref>.</p>
      </sec>
      <sec id="S3.SS2">
        <title>Data acquisition</title>
        <p>Imaging was performed on a Siemens 7 T MRI scanner equipped with a 32-element receive coil (Nova Medical). Functional runs were obtained using a gradient recalled, single shot, echo planar imaging (gre-EPI) sequence: (TR = 1.5 s; TE = 25 ms; FA = 50°; 36 interleaved slices; slice thickness = 2 mm; in-plane resolution = 2 × 2 mm; GRAPPA = 2). Each multi-task scan consists of 1,017 volumes acquired continuously as subjects engage and transition between the different tasks. In addition, high resolution (1 mm<sup>3</sup>) T1-weighted magnetization-prepared rapid gradient-echo and proton density sequences were acquired for presentation and alignment purposes.</p>
      </sec>
      <sec id="S3.SS3">
        <title>Data pre-processing</title>
        <p>Data pre-processing was conducted with AFNI (<xref rid="B22" ref-type="bibr">Cox, 1996</xref>). Preprocessing steps match those described in <xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref>, and include: (i) despiking; (ii) physiological noise correction (in all but four subjects, due to the insufficient quality of physiological recordings for these subjects); (iii) slice time correction; and (iv) head motion correction. In addition, mean, slow signal trends modeled with Legendre polynomials up to seventh order, signal from eroded local white matter, signal from the lateral ventricles (cerebrospinal fluid), motion estimates, and the first derivatives of motion were regressed out in a single regression step to account for potential hardware instabilities and remaining physiological noise (<italic>ANATICOR</italic>; <xref rid="B52" ref-type="bibr">Jo et al., 2010</xref>). Finally, time series were converted to signal percent change, bandpass filtered (0.03–0.18 Hz), and spatially smoothed (<italic>FWHM</italic> = 4 mm). The cutoff frequency of the high pass filter was chosen to match the inverse of window length (<italic>WL</italic> = 45 s); following recommendations from <xref rid="B61" ref-type="bibr">Leonardi and Ville (2014)</xref>.</p>
        <p>In addition, spatial transformation matrices to go from <italic>EPI</italic> native space to Montreal Neurological Institute (<italic>MNI</italic>) space were computed for all subjects following procedures previously described in <xref rid="B43" ref-type="bibr">Gonzalez-Castillo et al. (2013)</xref>. These matrices were then used to bring publicly available <italic>ROI</italic> definitions from MNI space into each subject’s EPI native space.</p>
      </sec>
      <sec id="S3.SS4">
        <title>Brain parcellation</title>
        <p>We used 157 regions of interest from the publicly available version of the 200 ROI version of the Craddock Atlas (<xref rid="B23" ref-type="bibr">Craddock et al., 2012</xref>). The missing 43 ROIs were excluded because they did not contain at least 10 voxels in all subjects’ imaging field of view. Those were ROIs primarily located in cerebellar, inferior temporal, and orbitofrontal regions.</p>
      </sec>
      <sec id="S3.SS5">
        <title>Time-varying functional connectivity</title>
        <p>First, for each scan we obtained representative timeseries for all ROIs as the spatial average across all voxels part of the ROI using AFNI program <italic>3dNetCorr</italic> (<xref rid="B82" ref-type="bibr">Taylor and Saad, 2013</xref>). Next, we computed time-varying FC for all scans separately using 45 s (30 samples) long rectangular windows with an overlap of one sample (1.5 s) in <italic>Python</italic>. The connectivity metric was the <italic>Fisher’s</italic> transform of the <italic>Pearson’s</italic> correlation between time-windowed ROI timeseries. Windowed FC matrices were converted to <italic>1D</italic> vectors by taking only unique connectivity values above the matrix diagonal. These were subsequently concatenated into scan-wise <italic>tvFC</italic> matrices of size 12,246 connections × 988 temporal windows.</p>
        <p>Time-varying <italic>FC</italic> matrices computed this way are referred through the manuscript as non-normalized or as-is. Additionally, we also computed normalized versions of those <italic>tvFC</italic> matrices in which all rows have been forced to have a mean of zero and a standard deviation of one across the time dimension. We refer to those matrices as normalized or <italic>Z</italic>-<italic>scored</italic> matrices.</p>
      </sec>
      <sec id="S3.SS6">
        <title>Intrinsic dimension</title>
        <p>Three different <italic>ID</italic> estimators were used here: <italic>lPCA</italic>, <italic>twoNN</italic>, and <italic>FisherS</italic>. For each method, we computed both <italic>ID</italic><sub><italic>local</italic></sub> estimates at each <italic>tvFC</italic> window and <italic>ID</italic><sub><italic>global</italic></sub> estimates at the scan level. We then report on the distribution of these two quantities across the whole sample.</p>
      </sec>
      <sec id="S3.SS7">
        <title>Dimensionality reduction</title>
        <p>We computed low dimensional representations of the data at two different scales: scan-level and group-level. Scan-level embeddings were generated separately for each scan providing as input their respective <italic>tvFC</italic> matrices.</p>
        <p>To generate group-level embeddings, meaning embeddings that contain all windows from all scans in the dataset, we used two different approaches:</p>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>“<italic>Concatenate + Embed</italic>”: in this case, we first concatenate all scan-level <italic>tvFC</italic> matrices into a single larger matrix for the whole dataset. We then provide this matrix as input to the dimensionality reduction step.</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>“<italic>Embed + Procrustes</italic>”: here, we first compute scan-level embeddings separately for each scan and then apply the <italic>Procrustes</italic> transformation to bring all of them into a common space.</p>
          </list-item>
        </list>
        <p><xref rid="T1" ref-type="table">Table 1</xref> summarizes the different configurations being explored for each manifold learning method.</p>
        <table-wrap position="float" id="T1">
          <label>TABLE 1</label>
          <caption>
            <p>Hyper-parameter exploration space.</p>
          </caption>
          <table frame="box" rules="all" cellspacing="5" cellpadding="5">
            <thead>
              <tr>
                <td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Hyper-parameters</td>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">LE</td>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">T-SNE</td>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">UMAP</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Distance function</td>
                <td valign="top" align="center" rowspan="1" colspan="1">Euclidean, correlation, cosine</td>
                <td valign="top" align="center" rowspan="1" colspan="1">Euclidean, correlation, cosine</td>
                <td valign="top" align="center" rowspan="1" colspan="1">Euclidean, correlation, cosine</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Neighborhood size</td>
                <td valign="top" align="center" rowspan="1" colspan="1"><italic>K</italic><sub><italic>nn</italic></sub>: [5…200, step = 5]</td>
                <td valign="top" align="center" rowspan="1" colspan="1"><italic>PP</italic>: [5…100, step = 5] + [150, 175, 200]</td>
                <td valign="top" align="center" rowspan="1" colspan="1"><italic>K</italic><sub><italic>nn</italic></sub>: [5…200, step = 5]</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1"># Dimensions</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2, 3, 5, 10, 15, 20, 25, 30</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2, 3, 10, 15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2, 3, 5, 10, 15, 20, 25, 30</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Learning rate</td>
                <td valign="top" align="center" rowspan="1" colspan="1">N/A</td>
                <td valign="top" align="center" rowspan="1" colspan="1">10, 50, 75, 100, 200, 500, 1,000</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.01, 0.1, 1.0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Minimum distance</td>
                <td valign="top" align="center" rowspan="1" colspan="1">N/A</td>
                <td valign="top" align="center" rowspan="1" colspan="1">N/A</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.8</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="S3.SS8">
        <title>Embedding evaluation</title>
        <p>Two different frameworks—clustering and predictive—were used to quantitatively evaluate the quality of the resulting low dimensional representations. The clustering framework looks at the ability of those representations to show groupings of <italic>FC</italic> configurations that match labels of interest (e.g., task being performed). The use of this framework is primarily motivated by the concept of <italic>FC</italic> states (<xref rid="B2" ref-type="bibr">Allen et al., 2014</xref>; <xref rid="B44" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>)—namely short-term recurrent <italic>FC</italic> configurations—and the fact that external cognitive demands modulate <italic>FC</italic> (<xref rid="B41" ref-type="bibr">Gonzalez-Castillo and Bandettini, 2018</xref>). As such, a meaningful low dimensional representation of the multi-task dataset should show cluster structure that relates to the different tasks. A common way to measure the cluster consistency in machine learning is the <italic>Silhouette index</italic> (<italic>SI</italic>; <xref rid="B76" ref-type="bibr">Rousseeuw, 1987</xref>), which is a measure of cluster cohesion (how similar members of a cluster are to each other) against cluster separation (the minimum distance between samples from two different clusters). <italic>SI</italic> ranges from −1 to 1, with higher <italic>SI</italic> values indicating more clearly delineated clusters. <italic>SI</italic> was computed using the <italic>Python scikit</italic>-<italic>learn</italic> library. Only task-homogenous windows—namely those that do not include instruction periods or more than one task—are used for the computation of the <italic>SI</italic>. For scan-level results we computed <italic>SI</italic> based on tasks. For group-level results we computed <italic>SI</italic> based on tasks (<italic>SI</italic><sub><italic>task</italic></sub>) and subject identity (<italic>SI</italic><sub><italic>subject</italic></sub>). For comparison purposes, <italic>SI</italic> was also computed using the <italic>tvFC</italic> matrices as input to the <italic>SI</italic> calculation.</p>
        <p>We also evaluate embeddings using a predictive framework. In this case, the goal is to quantify how well low dimensional representations of <italic>tvFC</italic> data performs as inputs to subsequent regression/classification machinery. This framework is motivated by the wide-spread use of <italic>FC</italic> (both static and time-varying) patterns as input features for the prediction of clinical conditions (<xref rid="B75" ref-type="bibr">Rashid et al., 2016</xref>), clinical outcomes (<xref rid="B27" ref-type="bibr">Dini et al., 2021</xref>), personality traits (<xref rid="B49" ref-type="bibr">Hsu et al., 2018</xref>), behavioral performance (<xref rid="B51" ref-type="bibr">Jangraw et al., 2018</xref>), and cognitive abilities (<xref rid="B33" ref-type="bibr">Finn et al., 2014</xref>). Our quality measure under this second framework is the <italic>F1</italic> accuracy of classification at predicting task name for task-homogenous windows using as input group-level embeddings. We restricted analyses to <italic>UMAP</italic> and <italic>LE</italic> group-level embeddings obtained using the “<italic>Embed + Procrustes</italic>” approach because those have good task separability scores and are computationally efficient at estimating embeddings beyond three dimensions. The classification engine used is a logistic regression machine with an <italic>L1</italic> regularization term as implemented in the <italic>scikit-learn</italic> library. We split the data into training and testing sets using two separate approaches:</p>
        <list list-type="simple">
          <list-item>
            <p>Split-half cross-validation: first, we trained the classifier using all windows within the first half of all scans and test on the remaining of the data. We then switched training and testing sets. Reported accuracy values are the average of the results across the two halves, when they were the test set. It is worth noting that, by splitting the data this way, we achieve two goals: (1) training on data from all tasks in all subjects, and (2) testing using data from windows that are fully temporally independent from the ones used for training.</p>
          </list-item>
          <list-item>
            <p>Leave-one-subject-out cross validation: in this case, we generated 20 different splits of the data. In each split, data from all subjects but one was used for training; and the data from the left-out subject was used for testing. As above, we report average values across all data splits. By splitting the data this way we avoid potential overfitting issues resulting from including data from the same subject in both the training and testing datasets.</p>
          </list-item>
        </list>
      </sec>
      <sec id="S3.SS9">
        <title>Stability analysis</title>
        <p>Two of the three methods under scrutiny are non-deterministic: <italic>UMAP</italic> and <italic>T-SNE</italic>. To evaluate the stability of these two embedding techniques, we decided to compute scan-level embeddings 1,000 times on each subject using optimal hyper-parameters (<italic>T-SNE</italic>: <italic>PP</italic> = 65, <italic>d</italic> = Correlation, <italic>alpha</italic> = 10, <italic>m</italic> = 2 dimensions | <italic>UMAP</italic>: <italic>K</italic><sub><italic>nn</italic></sub> = 70, <italic>d</italic> = Euclidean, <italic>alpha</italic> = 10, <italic>m</italic> = 3 dimensions). We then looked at the distribution of SI<sub><italic>task</italic></sub> values across the 1,000 iterations run on each subject’s data.</p>
      </sec>
      <sec id="S3.SS10">
        <title>Null models</title>
        <p>Two null models were used as control conditions in this study. The first null model (labeled “<italic>randomized connectivity</italic>”) proceeds by randomizing row ordering separately for each column of the <italic>tvFC</italic> matrix. By doing so, the row-to-connection relationship inherent to <italic>tvFC</italic> matrices is destroyed and a given row of the <italic>tvFC</italic> matrix no longer represents the true temporal evolution of <italic>FC</italic> between a given pair of ROIs.</p>
        <p>The second model (labeled “<italic>phase randomization</italic>”) proceeds by randomizing the phase of the <italic>ROI</italic> timeseries prior to the computation of the <italic>tvFC</italic> matrix (<xref rid="B45" ref-type="bibr">Handwerker et al., 2012</xref>). More specifically, for each ROI we computed its <italic>Fourier</italic> transform, kept the magnitude of the transform but substituted the phase by uniformly distributed phase spectra, and finally applied the inverse <italic>Fourier</italic> transform to get the surrogate ROI representative timeseries. This procedure ensures the surrogate data will retain the autoregressive properties of the original time series, yet the precise timing of signal fluctuations is destroyed.</p>
        <p>All code associated with these analyses can be found at the following github repo.<sup><xref rid="footnote2" ref-type="fn">2</xref></sup></p>
      </sec>
    </sec>
    <sec sec-type="results" id="S4">
      <title>Results</title>
      <sec id="S4.SS1">
        <title>Intrinsic dimension</title>
        <p>Average estimates of <italic>ID</italic><sub><italic>global</italic></sub> and <italic>ID</italic><sub><italic>local</italic></sub> across all scans are presented in <xref rid="F5" ref-type="fig">Figures 5A, B</xref>. We show estimates based on three <italic>ID</italic> estimators: <italic>Local PCA</italic>, <italic>Two Nearest Neighbors</italic>, and <italic>Fisher Separability</italic>. Average <italic>ID</italic><sub><italic>global</italic></sub> ranged from 26.25 dimensions (<italic>Local PCA</italic>, normalized <italic>tvFC</italic> matrices) to 4.10 dimensions (<italic>Fisher Separability</italic>, no normalization). <italic>ID</italic><sub><italic>global</italic></sub> estimates were significantly larger for <italic>Local PC</italic>A than for the two other methods (<italic>p<sub><italic>Bonf</italic></sub> &lt; 1e<sup>–4</sup></italic>). Normalization of <italic>tvFC</italic> matrices had a negligible effect of <italic>ID</italic><sub><italic>global</italic></sub> estimates. Despite the differences across estimation techniques, in all instances the <italic>ID</italic><sub><italic>global</italic></sub> of these data is shown to be several orders of magnitude below that of ambient space (i.e., 12,246 Connections).</p>
        <fig position="float" id="F5">
          <label>FIGURE 5</label>
          <caption>
            <p><bold>(A)</bold> Summary view of global ID estimates segregated by estimation method (local PCA, two nearest neighbors, and Fisher separability) and normalization approach (none or <italic>Z</italic>-scoring). Bar height corresponds to average across scans, bars indicate 95% confidence intervals. <bold>(B)</bold> Summary view of local ID estimates segregated by estimation method and neighborhood size (<italic>NN</italic> = number of neighbors) for the non-normalized data. <bold>(C)</bold> Statistical differences in global ID across tasks (*<italic>p</italic><sub><italic>FDR</italic></sub> &lt; 0.05) (gray, rest; yellow, visual attention; green, math; and blue, 2-back).</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g005" position="float"/>
        </fig>
        <p>Estimating <italic>ID</italic><sub><italic>local</italic></sub> requires the selection of a neighborhood size defined in terms of the number of neighbors (<italic>NN</italic>). <xref rid="F5" ref-type="fig">Figure 5B</xref> shows average <italic>ID</italic><sub><italic>local</italic></sub> estimates for non-normalized data across all scans as a function of both estimation technique and <italic>NN</italic>. Overall, <italic>ID</italic><sub><italic>local</italic></sub> ranges between 2 (<italic>Fisher Separability, NN</italic> = 50) and 21 (<italic>Local PCA, NN</italic> = 200). As is the case with <italic>ID</italic><sub><italic>global</italic></sub>, <italic>ID</italic><sub><italic>local</italic></sub> estimates vary significantly between estimators. In general, “<italic>Local PCA”</italic> leads to the largest estimates. Also, there is a general trend for <italic>ID</italic><sub><italic>local</italic></sub> estimates to increase monotonically with neighborhood size. Exceptions to these two trends only occur for the “<italic>Two NN</italic>” estimator when <italic>NN</italic> ≤ 75. It is important to note that <italic>ID</italic><sub><italic>local</italic></sub> estimates are always below their counterpart <italic>ID</italic><sub><italic>global</italic></sub> estimates.</p>
        <p>We also computed <italic>ID</italic><sub><italic>global</italic></sub> separately for each task. When using the <italic>twoNN</italic> estimator, we found that rest has a significantly higher <italic>ID</italic><sub><italic>global</italic></sub> than all other tasks (<xref rid="F5" ref-type="fig">Figure 5C</xref>). A significant difference was not detected with the other two methods.</p>
      </sec>
      <sec id="S4.SS2">
        <title>Evaluation for visualization/exploration purposes</title>
        <p><xref rid="F6" ref-type="fig">Figures 6A–C</xref> show the distribution of <italic>SI</italic><sub><italic>task</italic></sub> for <italic>2D</italic> and <italic>3D</italic> single-scan embeddings for both original data and the two null models. Each panel shows results for a different manifold learning technique (<italic>MLT</italic>). <italic>SI</italic><sub><italic>task</italic></sub> of original data often reached values above 0.4 (black arrows). That is not the case for either null model. Yet, while the “<italic>Connectivity Randomization</italic>” model always resulted in <italic>SI</italic><sub><italic>task</italic></sub> near or below zero, the “<italic>Phase Rand</italic>omization” model shows substantial overlap with the lower end of the distribution for original data, especially for <italic>LE</italic> and <italic>UMAP</italic>.</p>
        <fig position="float" id="F6">
          <label>FIGURE 6</label>
          <caption>
            <p>Task separability for single-scan level embeddings. <bold>(A)</bold> Distribution of <italic>SI</italic><sub><italic>task</italic></sub> values for original data and null models across all hyperparameters for <italic>LE</italic> for 2D and 3D embeddings <italic>(Total Number of Cases: 20 Subjects × 3 Models × 2 Norm Methods × 3 Distances × 40 Knn values × 2 Dimensions).</italic>
<bold>(B)</bold> Same as panel <bold>(A)</bold> but for <italic>T-SNE</italic>. <bold>(C)</bold> Same as panel <bold>(A)</bold> but for <italic>UMAP</italic>. <bold>(D)</bold>
<italic>SI</italic><sub><italic>task</italic></sub> for <italic>LE</italic> as a function on distance metric and number of final dimensions for the original data. Bold lines indicate mean values across all scans and hyperparameters. Shaded regions indicate 95% confidence interval. <bold>(E)</bold> Effects of normalization scheme on <italic>SI</italic><sub><italic>task</italic></sub> at <italic>K</italic><sub><italic>nn</italic></sub> = 75 for original data and the three distances. <bold>(F)</bold> Same as panel <bold>(D)</bold> but for <italic>T-SNE</italic>. <bold>(G)</bold> Same as panel <bold>(E)</bold> but for <italic>T-SNE</italic> and <italic>PP</italic> = 75. <bold>(H)</bold>
<italic>SI</italic><sub><italic>task</italic></sub> dependence on learning rate for <italic>T-SNE</italic> at <italic>PP</italic> = 75. <bold>(I)</bold> Same as panel <bold>(D)</bold> but for <italic>UMAP</italic>. <bold>(J)</bold> Same as panel <bold>(E)</bold> but for <italic>UMAP</italic>. <bold>(K)</bold> Same as panel <bold>(H)</bold> but for <italic>UMAP</italic>. In panels <bold>(E,G,H,J,K)</bold> bar height indicate mean value and error bars indicate 95% confidence interval. Statistical annotations: <italic><sup>ns</sup></italic>non-significant, *<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.05, **<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.01, ***<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.001, ****<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.0001.</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g006" position="float"/>
        </fig>
        <p><xref rid="F6" ref-type="fig">Figure 6D</xref> shows how <italic>SI</italic><sub><italic>task</italic></sub> changes with distance function and <italic>K</italic><sub><italic>nn</italic></sub> for <italic>LE</italic> embeddings. Overall, best performance is achieved when using the <italic>Correlation</italic> distance and keeping three dimensions. Additionally, <italic>K</italic><sub><italic>nn</italic></sub> can also have substantial influence on task separability. <italic>SI</italic><sub><italic>task</italic></sub> is low for <italic>k</italic><sub><italic>nn</italic></sub> values below 50, then starts to increase until it reaches a peak around <italic>k</italic><sub><italic>nn</italic></sub> = 75 and then decreases again monotonically with <italic>k</italic><sub><italic>nn</italic></sub>. <xref rid="F6" ref-type="fig">Figure 6E</xref> shows that whether <italic>tvFC</italic> matrices are normalized or not prior to entering <italic>LE</italic> has little effect on task separability.</p>
        <p><xref rid="F6" ref-type="fig">Figures 6F–H</xref> summarize how task separability varies with distance, perplexity, normalization scheme, and learning rate when using <italic>T-SNE</italic>. As it was the case with <italic>LE</italic>, best results were obtained with the <italic>Correlation</italic> distance. We can also observe high dependence of embedding quality with neighborhood size (i.e., perplexity), and almost no dependence with normalization scheme. Regarding learning rate, <italic>SI</italic><sub><italic>task</italic></sub> monotonically decreases as learning rate increased. <xref rid="F6" ref-type="fig">Figures 6I–K</xref> show results for equivalent analyses when using <italic>UMAP</italic>. In this case, best performance is achieved with the <italic>Euclidean</italic> distance. Again, we observe high dependence of <italic>SI</italic><sub><italic>task</italic></sub> with neighborhood size, no dependence on normalization scheme, and a monotonic decrease with increasing learning rate.</p>
        <p><xref rid="F7" ref-type="fig">Figure 7</xref> shows representative single-scan <italic>2D</italic> embeddings (see <xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 4</xref> for <italic>3D</italic> results). First, <xref rid="F7" ref-type="fig">Figure 7A</xref> shows the best and worse <italic>LE</italic> embeddings obtained using the <italic>Correlation</italic> distance and <italic>K</italic><sub><italic>nn</italic></sub> = 25, 75, 125, and 175. <xref rid="F7" ref-type="fig">Figures 7B, C</xref> show <italic>2D</italic> embeddings for the same scans obtained using <italic>T-SNE</italic> with the <italic>Correlation</italic> distance and <italic>UMAP</italic> with the <italic>Euclidean</italic> distance, respectively. Embedding shape significantly differed across <italic>MLTs</italic> and as function of hyperparameters. For <italic>K</italic><sub><italic>nn</italic></sub> = 25, all <italic>MLTs</italic> placed next to each other are temporally contiguous windows in a “<italic>spaghetti-like</italic>” configuration. No other structure of interest is captured by those embeddings. For <italic>K</italic><sub><italic>nn</italic></sub> ≥ 75 (embeddings marked with a green box), those “<italic>spaghetti</italic>” start to break and bend in ways that bring together windows corresponding to the same task independently of whether or not such windows are contiguous in time. If we focus our attention on high quality exemplars (green boxes), we observe clear differences in shape across <italic>MLTs</italic>. For example, <italic>LE</italic> places windows from different tasks in distal corners of the <italic>2D</italic> (and <italic>3D</italic>) space; and the presence of two distinct task blocks is no longer clear. Conversely, <italic>T-SNE</italic> and <italic>UMAP</italic> still preserve a resemblance of the “spaghetti-like” structure previously mentioned, and although windows from both task blocks now appear together, one can still appreciate that there were two blocks of each task. Finally, <xref rid="F7" ref-type="fig">Figures 7D, E</xref> show embeddings for the null models at <italic>K</italic><sub><italic>nn</italic></sub> = 75 for the best scan. When connections are randomized, embeddings look like random point clouds. When the phase of ROI timeseries is randomized prior to generating <italic>tvFC</italic> matrices, embeddings look similar to those generated with real data at low <italic>K</italic><sub><italic>nn</italic></sub>, meaning they have a “<italic>spaghetti-like</italic>” structure where time contiguous windows appear together, but windows corresponding to the two different blocks of the same task do not.</p>
        <fig position="float" id="F7">
          <label>FIGURE 7</label>
          <caption>
            <p>Representative single-scan embeddings. <bold>(A)</bold> LE embeddings for <italic>K</italic><sub><italic>nn</italic></sub> = 25,75,125 and 175 generated with the Correlation distance. Left column shows embeddings for the scan that reached the maximum <italic>SI</italic><sub><italic>task</italic></sub> value across all possible <italic>K</italic><sub><italic>nn</italic></sub> values. Right column shows embeddings for the scan that reached worse <italic>SI</italic><sub><italic>task</italic></sub>. In each scatter plot, a dot represents a window of <italic>tvFC</italic> data (i.e., a column in the <italic>tvFC</italic> matrix). Dots are annotated by task being performed during that window. <bold>(B)</bold>
<italic>T-SNE</italic> embeddings computed using the Correlation distance, learning rate = 10 and <italic>PP</italic> = 25, 75, 125, and 175. These embeddings correspond to the same scans depicted in panel <bold>(A)</bold>. <bold>(C)</bold>
<italic>UMAP</italic> embeddings computed using the <italic>Euclidean</italic> distance, learning rate = 0.01 and <italic>K</italic><sub><italic>nn</italic></sub> = 25, 75, 125, and 175. These also correspond to the same scans depicted in panels <bold>(A,C)</bold>. <bold>(D)</bold>
<italic>LE</italic>, <italic>T-SNE</italic>, and <italic>UMAP</italic> embeddings computed using as input the connectivity randomized version of the same scan corresponding to “best” in panels <bold>(A–C)</bold>. <bold>(E)</bold>
<italic>LE</italic>, <italic>T-SNE</italic>, and <italic>UMAP</italic> embeddings computed using as input the phase randomized version of the same scan corresponding to “best” in panels <bold>(A–C)</bold>.</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g007" position="float"/>
        </fig>
        <p>In terms of stability of scan level embeddings, <italic>UMAP</italic> performed better than <italic>T-SNE</italic> (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 5</xref>). While for <italic>T-SNE</italic> we can often observe outliers and wide distributions for <italic>SI</italic><sub><italic>task</italic></sub> values, that is not the case for <italic>UMAP</italic>, which had very consistent <italic>SI</italic><sub><italic>task</italic></sub> values across the 1,000 iterations conducted on the data of each participant.</p>
        <p><xref rid="F8" ref-type="fig">Figure 8</xref> shows clustering evaluation results for group-level embeddings generated with <italic>LE</italic> for the original data. Regarding task separability (<xref rid="F8" ref-type="fig">Figure 8A</xref>), the “<italic>Embed + Procrustes</italic>” approach (orange) outperforms the “<italic>Concatenate + Embed</italic>” approach (blue). Importantly, the higher gains for the “<italic>Embed + Procrustes”</italic> approach occur when the transformation is calculated using dimensions beyond three (portion of the orange distribution outlined in dark red in <xref rid="F8" ref-type="fig">Figure 8A</xref>). <xref rid="F8" ref-type="fig">Figure 8C</xref> shows embeddings in which the <italic>Procrustes</italic> transformation was computed with an increasing number of dimensions (from left to right). As the number of dimensions increases toward the data’s <italic>ID</italic>, task separability improves. For example, when all 30 dimensions are used during the <italic>Procrustes</italic> transformation the group embedding show four distinct clusters (one per task), and all subject specific information has been removed (orange histogram in <xref rid="F8" ref-type="fig">Figure 8B</xref>). <xref rid="F8" ref-type="fig">Figure 8D</xref> shows one example of high <italic>SI</italic><sub><italic>subject</italic></sub> for the “Concatenation + Embed” approach. This occurs on a few instances (long right tail of the blue distribution in <xref rid="F8" ref-type="fig">Figure 8B</xref>) that corresponds to scenarios where an excessively low <italic>K</italic><sub><italic>nn</italic></sub> results in a disconnected graph.</p>
        <fig position="float" id="F8">
          <label>FIGURE 8</label>
          <caption>
            <p>Summary of clustering evaluation for <italic>LE</italic> group embeddings. <bold>(A)</bold> Histograms of <italic>SI</italic><sub><italic>task</italic></sub> values across all hyperparameters when using the <italic>Correlation</italic> distance with real data. Distributions segregated by grouping method: <italic>“Embed + Procrustes”</italic> in orange and <italic>“Concatenation + Embed”</italic> in blue. Dark red outline highlights the section of the distribution for “<italic>Embed + Procrustes</italic>” that corresponds to instances where more than 3 dimensions were used to compute the Procrustes transformation. <bold>(B)</bold> Same as panel <bold>(A)</bold> but this time we report <italic>SI</italic><sub><italic>subject</italic></sub> instead of <italic>SI</italic><sub><italic>task</italic></sub>. <bold>(C)</bold> Group level <italic>LE</italic> embeddings obtained via “<italic>Embed + Procrustes</italic>” with an increasing number of dimensions entering the Procrustes transformation step. In all instances we show embeddings annotated by task, including windows that span more than one task and/or instruction periods. For <italic>m</italic> = 30 we show two additional versions of the embedding, one in which task inhomogeneous windows have been removed, so that task separability becomes clear, and one when windows are annotated by subject identity to demonstrate how inter-individual differences are not captured in this instance. <bold>(D)</bold> Representative group embedding with high <italic>SI</italic><sub><italic>subject</italic></sub> obtained via “<italic>Concatenation + Embed</italic>” approach.</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g008" position="float"/>
        </fig>
        <p><xref rid="F9" ref-type="fig">Figure 9</xref> shows clustering evaluation results for <italic>UMAP</italic>. <xref rid="F9" ref-type="fig">Figure 9A</xref> shows the distribution of <italic>SI</italic><sub><italic>task</italic></sub> values across all hyperparameters when working with the real data and the <italic>Euclidean</italic> distance. High <italic>SI</italic><sub><italic>task</italic></sub> values were only obtained when combining scans via the “<italic>Embed + Procrustes</italic>” approach and using more than three dimensions during the <italic>Procrustes</italic> transformation (highlighted portion of the orange distribution in <xref rid="F9" ref-type="fig">Figure 9A</xref>). <xref rid="F9" ref-type="fig">Figure 9C</xref> shows one example of an embedding with high <italic>SI</italic><sub><italic>task</italic></sub> computed this way. Clear task separability is observed when annotating the embedding by task (<xref rid="F9" ref-type="fig">Figure 9C</xref>, left). If we annotate by subject identity (<xref rid="F9" ref-type="fig">Figure 9C</xref>, right), we can observe how individual differences have been removed by this procedure. <xref rid="F9" ref-type="fig">Figure 9B</xref> shows the distribution of <italic>SI</italic><sub><italic>subject</italic></sub> values. High <italic>SI</italic><sub><italic>subject</italic></sub> values were only obtained when using the “<italic>Concatenation + Embed</italic>” approach in data that has not been normalized (dark blue outline). <italic>Z</italic>-scoring scan-level <italic>tvFC</italic> matrices prior to concatenation removes <italic>UMAP</italic> ability to capture subject identity (light blue highlight). <xref rid="F9" ref-type="fig">Figure 9D</xref> shows a <italic>UMAP</italic> embedding with high <italic>SI</italic><sub><italic>subject</italic></sub> annotated by task (left) and subject identity (right). The embedding shows meaningful structure at two different levels. First, windowed <italic>tvFC</italic> are clearly group by subject. In addition, for most subjects, the embedding also captures the task structure of the experiment. Results for <italic>T-SNE</italic> group-level embeddings are shown in <xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 6</xref>. <italic>T-SNE</italic> was also able to generate embeddings that simultaneously capture task and subject information using the “<italic>Concatenation + Embed”</italic> and no normalization. Similarly, it could generate group embeddings that bypass subject identity and only capture subject structure by using the “<italic>Embed + Procrustes</italic>” approach, yet their quality was inferior to those obtained with <italic>UMAP</italic>.</p>
        <fig position="float" id="F9">
          <label>FIGURE 9</label>
          <caption>
            <p>Summary of clustering evaluation for <italic>UMAP</italic> group embeddings. <bold>(A)</bold> Histograms of <italic>SI</italic><sub><italic>task</italic></sub> values across all hyperparameters when using <italic>Euclidean</italic> distance on real data. Distributions are segregated by grouping method: “<italic>Embed + Procrustes</italic>” in orange and “<italic>Concatenation + Embed</italic>” in blue. Dark orange outline highlights the section of the “<italic>Embed + Procrustes</italic>” distribution that corresponds to instances where more than 3 dimensions were used to compute the Procrustes transformation. <bold>(B)</bold> Histograms of <italic>SI</italic><sub><italic>subject</italic></sub> values across all hyperparameters when using <italic>Euclidean</italic> distance on real data. Distributions are segregated by grouping method in the same way as in panel <bold>(A)</bold>. Light blue contour highlights the part of the distribution for “<italic>Concatenation + Embed</italic>” computed on data that has been normalized (e.g., <italic>Z</italic>-scored), while the dark blue contour highlights the portion corresponding to data that has not been normalized. <bold>(C)</bold> High quality group-level <italic>“Embed + Procrustes”</italic> embedding annotated by task (left) and subject identity (right). <bold>(D)</bold> High quality group-level “<italic>Concatenation + Embed</italic>” annotated by task (left) and subject identity (right).</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g009" position="float"/>
        </fig>
        <p>Independently of the method used, optimal hyper-parameter selection always resulted in better separation of the data in terms of task or subjects (see <xref rid="T2" ref-type="table">Table 2</xref> for SI values calculated using directly as input the <italic>tvFC</italic> matrices).</p>
        <table-wrap position="float" id="T2">
          <label>TABLE 2</label>
          <caption>
            <p>Silhouette index computed using <italic>tvFC</italic> data in original ambient space.</p>
          </caption>
          <table frame="box" rules="all" cellspacing="5" cellpadding="5">
            <thead>
              <tr>
                <td valign="top" align="left" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1"/>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Non-normalized/“as-is”</td>
                <td valign="top" align="center" style="color:#ffffff;background-color: #7f8080;" rowspan="1" colspan="1">Normalized/“Z-scored”</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Scan-level</td>
                <td valign="top" align="center" rowspan="1" colspan="1">SI<sub><italic>task</italic></sub></td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.13 ± 0.02</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.12 ± 0.01</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Group level</td>
                <td valign="top" align="center" rowspan="1" colspan="1">SI<sub><italic>task</italic></sub></td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.01</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.01</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td valign="top" align="center" rowspan="1" colspan="1">SI<sub><italic>subject</italic></sub></td>
                <td valign="top" align="center" rowspan="1" colspan="1">0.05</td>
                <td valign="top" align="center" rowspan="1" colspan="1">−0.02</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="S4.SS3">
        <title>Evaluation for predictive/classification purposes</title>
        <p><xref rid="F10" ref-type="fig">Figure 10</xref> shows results for the predictive framework evaluation using the split-half cross-validation approach (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 9</xref> shows equivalent results using leave-on-subject-out cross validation). This evaluation was performed using only embeddings that performed well on the task separability evaluation. For <italic>UMAP</italic>, this includes embeddings computed using the <italic>Euclidean</italic> distance, learning rate = 0.01 and <italic>K</italic><sub><italic>nn</italic></sub> &gt; 50. For <italic>LE</italic>, this includes embeddings computed using the <italic>Correlation</italic> distance and <italic>K</italic><sub><italic>nn</italic></sub> &gt; 50. In both instances, we used as input group level embeddings computed using the “<italic>Embed + Procrustes</italic>” aggregation method. We did not perform this evaluation on T-<italic>SNE</italic> embeddings because computational demands increase significantly with the number of dimensions.</p>
        <fig position="float" id="F10">
          <label>FIGURE 10</label>
          <caption>
            <p>Summary of predictive framework evaluation for LE <bold>(A,C)</bold> and <italic>UMAP</italic>
<bold>(B,D)</bold> group embeddings using the split-half cross validation scheme. <bold>(A)</bold> Classification accuracy as a function of the number of LE dimensions used as input features to the logistic regression classifier. Classification improves as m increases up to <italic>m</italic> = 20. <bold>(B)</bold> Classification accuracy as a function of the number of UMAP dimensions used as input features to the logistic regression classifier. Classification improves as <italic>m</italic> increases up to <italic>m</italic> = 5. Statistical annotations for panels <bold>(A,B)</bold> as follows: <italic><sup>ns</sup></italic>non-significant, **<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.01, ***<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.001, ****<italic>p</italic><sub><italic>Bonf</italic></sub> &lt; 0.0001. <bold>(C)</bold> Average coefficients associated with each <italic>LE</italic> dimension for classifiers trained using <italic>m</italic> = 30 dimensions. For each <italic>m</italic> value, we stack the average coefficients associated with each label, which are colored as follows: blue, 2-back; green, math; yellow, visual attention; gray, rest. <bold>(D)</bold> Same as panel <bold>(C)</bold> but for UMAP.</p>
          </caption>
          <graphic xlink:href="fnhum-17-1134012-g010" position="float"/>
        </fig>
        <p><xref rid="F10" ref-type="fig">Figure 10A</xref> shows average classification accuracy for <italic>LE</italic>. Classification accuracy increased significantly with the number of dimensions up to <italic>m</italic> = 20. Beyond that point, accuracy slightly decreased but remained above that obtained with <italic>m</italic> ≤ 3. <xref rid="F10" ref-type="fig">Figure 10B</xref> shows equivalent results for <italic>UMAP</italic>. In this case, accuracy significantly increased up to <italic>m</italic> = 5, but monotonically decreased beyond that point. For <italic>m</italic> ≥ 15, accuracy was less than that of <italic>m</italic> = 3. <xref rid="F10" ref-type="fig">Figures 10C, D</xref> show the average classifier coefficient values associated with each dimension for classifiers trained with <italic>m</italic> = 30 for <italic>LE</italic> and <italic>UMAP</italic>, respectively. In both instances we can observe that although the higher contributing features are those for the first three dimensions, there are still substantial contributions from higher dimensions.</p>
        <p>When a leave-one-subject-out cross validation scheme is used we always observe, independently of embedding method, a monotonic increase in classification accuracy as m increases up to approximately <italic>m</italic> = 15. Beyond that point accuracy is nearly 1 (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figures 9A, B</xref>). Regarding dimension contribution, results are equivalent for both cross validation schemes.</p>
      </sec>
    </sec>
    <sec sec-type="discussion" id="S5">
      <title>Discussion</title>
      <p>The purpose of this work is to understand why, how, and when <italic>MLTs</italic> can be used to generate informative low dimensional representations of <italic>tvFC</italic> data. In the theory section, we first provide a rationale for why we believe it is reasonable to expect <italic>tvFC</italic> data to lie on a low dimensional manifold embedded within the higher dimensional ambient space of all possible pair-wise ROI connections. We next discuss, at a programmatic level, the inner workings of three state-of-the-art <italic>MLTs</italic> often used to summarize biological data. This theoretical section is accompanied by several empirical observations: (1) the dimensionality of <italic>tvFC</italic> data is several orders of magnitude lower than that of its ambient space, (2) the quality of low dimensional representations varies greatly across methods and also within method as a function of hyper-parameter selection, (3) temporal autocorrelation, which is prominent in <italic>tvFC</italic> data but not on other data modalities commonly used to benchmark manifold learning methods, dominates embedding structure and must be taken into consideration when interpreting results, and (4) while three dimensions suffice to capture first order connectivity-to-behavior relationships (as measured by task separability), keeping additional dimensions up to the <italic>ID</italic> of the data can substantially improve the outcome of subsequent transformation and classification operations.</p>
      <sec id="S5.SS1">
        <title>Intrinsic dimension</title>
        <p>Functional connectivity matrices are often computed using brain parcellations that contain somewhere between 100 and 1,000 different regions of interest. As such, <italic>FC</italic> matrices have dimensionality ranging from 4,950 to 499,500. We used a parcellation with 157 regions, resulting in 12,246 dimensions. Our results indicate that <italic>tvFC</italic> data only occupies a small portion of that immense original space, namely that of a manifold with dimensionality ranging somewhere between 3 and 25. This suggests that the dimensionality of <italic>tvFC</italic> can be aggressively reduced and still preserve salient properties of the data (e.g., task and subject identity). That said, such low <italic>ID</italic> is not evidence of slow dynamics or implies that the brain can be fully characterized using so few <italic>FC</italic> configurations. For example, let us consider a latent space where each dimension ranges between −1 and 1 (as it is the case with Pearson’s correlation) in increments of 0.2 (e.g., −1.0, −0.8, −0.6 … 0.6, 0.8., 1.0). As such, there are only 11 possible values per dimension. A hypothetical 3D space with dimensions defined that way could represent up to 11<sup>3</sup> = 1,331 different <italic>FC</italic> configurations. If we consider five dimensions, the number of configurations goes up to 1.6e<sup>5</sup>. With 25 dimensions, we reach 1.1e<sup>26</sup> different possible FC configurations to be represented in such space. In other words, low <italic>ID</italic> should not be regarded as a sign of <italic>FC</italic> being relatively stable, but as confirmatory evidence that <italic>FC</italic> patterns are highly structured and in agreement with prior observations such as the fact that the brain’s functional connectome adhere to a very specific network topography [e.g., small world network (<xref rid="B81" ref-type="bibr">Sporns and Honey, 2006</xref>)] or that task performance does not lead to radical <italic>FC</italic> re-configurations (<xref rid="B21" ref-type="bibr">Cole et al., 2014</xref>; <xref rid="B58" ref-type="bibr">Krienen et al., 2014</xref>).</p>
        <p>Using the <italic>twoNN</italic> estimator, we found that the <italic>ID<sub><italic>global</italic></sub></italic> of <italic>tvFC</italic> from rest periods is significantly higher than that of the other three tasks (<xref rid="F5" ref-type="fig">Figure 5C</xref>), suggesting that <italic>FC</italic> traverses a larger space of possible configurations during rest compared to when subjects are engaged in tasks with more precise cognitive demands. This agrees with prior work suggesting an overall increase in the stability of <italic>FC</italic> for task as compared to rest (<xref rid="B62" ref-type="bibr">Liu and Duyn, 2013</xref>; <xref rid="B19" ref-type="bibr">Chen et al., 2015</xref>; <xref rid="B28" ref-type="bibr">Elton and Gao, 2015</xref>; <xref rid="B11" ref-type="bibr">Billings et al., 2017</xref>). It also suggests that <italic>ID</italic> can be a valuable summary statistic for <italic>tvFC</italic> data. In machine learning, <italic>ID</italic> is used to evaluate the complexity of object representations at different levels of deep neural networks (<xref rid="B4" ref-type="bibr">Ansuini et al., 2019</xref>) and their robustness against adversarial attacks (<xref rid="B3" ref-type="bibr">Amsaleg et al., 2017</xref>). In biomedical research, <italic>ID</italic> has also been used to characterize the amount of variability present in protein sequence evolution (<xref rid="B30" ref-type="bibr">Facco et al., 2019</xref>), and to explain why humans can learn certain concepts from just a few sensory experiences (i.e., “few-shot” concept learning; <xref rid="B80" ref-type="bibr">Sorscher et al., 2022</xref>). Given that several psychiatric conditions and neurological disorders have been previously associated with altered FC dynamics (<xref rid="B25" ref-type="bibr">Damaraju et al., 2014</xref>; <xref rid="B75" ref-type="bibr">Rashid et al., 2016</xref>; <xref rid="B34" ref-type="bibr">Fiorenzato et al., 2018</xref>; <xref rid="B53" ref-type="bibr">Kaiser et al., 2019</xref>), future research should evaluate the value of <italic>ID</italic> as a marker of clinically relevant aberrant <italic>tvFC</italic>.</p>
        <p>Finally, <italic>ID</italic> for <italic>tvFC</italic> data was estimated to be near to the number of dimensions that can be easily visualized (i.e., two or three dimensions). This explains the success of <italic>MLTs</italic> at summarizing <italic>tvFC</italic> data reported here and elsewhere (<xref rid="B42" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>; <xref rid="B39" ref-type="bibr">Gao et al., 2021</xref>; <xref rid="B77" ref-type="bibr">Rué-Queralt et al., 2021</xref>). Yet for two <italic>ID</italic> estimators (<italic>lPCA</italic> and <italic>twoNN</italic>), <italic>ID</italic> was estimated to be greater than 3 (<xref rid="F5" ref-type="fig">Figures 5A, B</xref>). This suggests that one ought to keep and explore additional dimensions up to <italic>ID</italic> whenever possible. Although visualizing data with more than three dimensions is challenging, tools such as coordinate plots (<xref rid="B50" ref-type="bibr">Inselberg and Dimsdale, 1990</xref>), or star glyphs (<xref rid="B17" ref-type="bibr">Chambers et al., 2018</xref>) should be considered as ways to explore the data beyond three dimensions when needed. Importantly, we empirically demonstrate the value of keeping additional dimensions up to <italic>ID</italic> in two scenarios: across-scan embedding alignment and task classification. For example, in <xref rid="F8" ref-type="fig">Figure 8C</xref> we show how task separability for <italic>Procrustes</italic>-based group embeddings substantially improves when more than three dimensions are used to compute the transformation. The <italic>Procrustes</italic> transformation has several applications in fMRI data analysis including hyper-alignment of voxel-wise responses (<xref rid="B48" ref-type="bibr">Haxby et al., 2011</xref>), alignment of macro-anatomical FC gradients (<xref rid="B65" ref-type="bibr">Margulies et al., 2016</xref>) and generation of bidirectional mappings between fMRI responses and natural language description of scenes in naturalistic experiments (<xref rid="B88" ref-type="bibr">Vodrahalli et al., 2018</xref>). The benefits of using dimensions beyond those being interpreted during alignment has been previously reported for functional gradients by <xref rid="B68" ref-type="bibr">Mckeown et al. (2020)</xref> who showed that alignment toward a template space significantly improves when using 10 dimensions instead of three [the number of gradients often explored and interpreted in studies that rely on this technique (<xref rid="B65" ref-type="bibr">Margulies et al., 2016</xref>; <xref rid="B68" ref-type="bibr">Mckeown et al., 2020</xref>; <xref rid="B85" ref-type="bibr">Tian et al., 2020</xref>; <xref rid="B46" ref-type="bibr">Hardikar et al., 2022</xref>)]. The <xref rid="DS1" ref-type="supplementary-material">Supplementary material</xref> from that study [Supplementary Figure 2 in <xref rid="B68" ref-type="bibr">Mckeown et al. (2020)</xref>] show that by keeping the additional seven dimensions the authors approximately doubled the amount of variance explained available as input to the <italic>Procrustes</italic> transformation, yet no clear heuristic was provided about how to select the optimal number of inputs. Our results suggest that <italic>ID</italic> could help generate such heuristics and help inform how many dimensions ought to be explored and retained during analysis.</p>
      </sec>
      <sec id="S5.SS2">
        <title>Hyper-parameter selection</title>
        <p>Our results demonstrate that although <italic>MLTs</italic> can create low dimensional representations of <italic>tvFC</italic> data that capture structures of interest at different levels (i.e., subject identity and mental state), hyper-parameter selection was critical to their success. Particularly important was the selection of distance function and neighborhood size for single-scan embeddings. For group embeddings, aggregation method and normalization scheme also played a critical role.</p>
        <p>One common theme across explored <italic>MLTs</italic> is the construction of a neighborhood graph in early stages of the algorithm (e.g., <xref rid="F2" ref-type="fig">Figure 2D</xref>). The final form of such graph depends, to a large extent, on how one decides to quantitatively measure dissimilarity between connectivity patterns (distance function) and how big one expects neighborhoods of interest to be (<italic>K</italic><sub><italic>nn</italic></sub> or <italic>PP</italic>). For <italic>LE</italic> and <italic>T-SNE</italic> best results were obtained using <italic>Correlation</italic> distance, which measures the degree of linear relationship between two sets of observations. <italic>Correlation</italic> is often used to quantify similarity in fMRI data, whether it be between timeseries (<xref rid="B12" ref-type="bibr">Biswal et al., 1995</xref>; <xref rid="B33" ref-type="bibr">Finn et al., 2014</xref>), activity maps (<xref rid="B90" ref-type="bibr">Yarkoni et al., 2011</xref>; <xref rid="B66" ref-type="bibr">Matsui et al., 2022</xref>), or connectivity matrices (<xref rid="B21" ref-type="bibr">Cole et al., 2014</xref>). Therefore, <italic>Correlation’s</italic> ability to meaningfully quantify similarity is well accepted and validated in the field. Moreover, previous work with non-imaging data suggest that the <italic>Euclidean</italic> distance fails to accurately capture neighborhood relationships in high dimensional spaces (<xref rid="B10" ref-type="bibr">Beyer et al., 1999</xref>) due to the curse of dimensionality, and that the <italic>Correlation</italic> distance is more appropriate for clustering and classification problems on high dimensional data (<xref rid="B36" ref-type="bibr">France and Carroll, 2009</xref>). Our results confirm that is also the case for <italic>tvFC</italic> data. One exception was <italic>UMAP</italic>, which performed best with the <italic>Euclidean</italic> distance for mid-size neighborhoods and became successively more equivalent to the other distances as neighborhood size increased. As described on the theory section, <italic>UMAP</italic> contains a distance normalization step (<xref rid="F4" ref-type="fig">Figure 4E</xref>) aimed at ensuring that each sample is connected to at least one other sample in the neighboring graph. We believe this step is the reason why the <italic>Euclidean</italic> distance outperforms the <italic>Correlation</italic> distance in <italic>UMAP</italic>. First, as discussed in the original description of the algorithm by <xref rid="B67" ref-type="bibr">McInnes et al. (2018)</xref>, this normalization step increases robustness against the curse of dimensionality and, as such, it helps mitigate some of the undesired outcomes of using <italic>Euclidean</italic> distances with high dimensional data. Second, because <italic>Correlation</italic> distances are in the range [0,2], <italic>rho</italic> (Eq. 12) always takes values near zero (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 8A</xref>) and <italic>sigma</italic> (Eq. 13) is restricted to a narrow range of values (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 8C</xref>). These two circumstances contribute to worsening UMAP’s ability to capture global structure (e.g., same task in temporally distant blocks) when using the <italic>Correlation</italic> distance.</p>
        <p>In summary, when using <italic>MLTs</italic> and <italic>tvFC</italic> data one ought to attempt to mitigate the negative effects derived from the curse of dimensionality—namely the fact that high dimensional spaces tend to be sparse and <italic>Euclidean</italic> distances become progressively meaningless—either by the selection of an alternative distance metric (e.g. <italic>Correlation</italic>) or by relying on algorithms with some built-in level of protection against it (e.g., <italic>UMAP</italic>).</p>
        <p>Neighborhood size, the second key parameter for graph generation, can be thought of as a way of setting the scale at which information of interest is expected. In our test data, subjects engaged with four different tasks during two temporally disconnected 180 s blocks. Given our sliding window parameters (<italic>Window Duration</italic> = 45 s; <italic>Window Step</italic> = 1.5 s) this results in 91 windows per task block and a total of 182 windows per task on each scan. For all methods, we observed bad task separability (low <italic>SI</italic><sub><italic>task</italic></sub>) at the smallest neighborhood sizes (e.g., &lt;60). This is because setting such small values often precluded the graph to capture neighboring relationships beyond those due to large overlap in the number of samples contributing to temporally contiguous windows. As we approach neighborhood values above 70, we start to observe the best <italic>SI</italic><sub><italic>task</italic></sub> values. This is because at this point, the graph can now capture neighborhood relationships between windows corresponding to different blocks of the same task and embeddings start to show structure and clusters that relate to the task. As neighborhood size keeps increasing, <italic>SI</italic><sub><italic>task</italic></sub> slowly degrades because more windows from different tasks end up being marked as neighbors during the construction of the graph.</p>
      </sec>
      <sec id="S5.SS3">
        <title>Challenges for resting-state fMRI</title>
        <p>In the previous paragraph, we were able to explain the relationship between task separability and neighborhood size because we know the scale of the phenomena of interest and have labels for tasks. But what about situations where such information is missing? For example, should we decide to use <italic>MLTs</italic> to explore the dynamics of <italic>tvFC</italic> during resting state, what is the recommended neighborhood size to use? This is quite a difficult question. Initially, one could opt to rely on existing heuristics from machine learning such using a neighborhood size equal to the square root of the number of samples (<xref rid="B47" ref-type="bibr">Hassanat et al., 2014</xref>), but such heuristic would have resulted in a value of 27, which is far from optimal. Similarly, using default values in existing implementations would have also proven sub-optimal here (e.g., <italic>PP</italic> = 30 in <italic>scikit</italic>-<italic>learn</italic> implementation of T-<italic>SNE</italic>). A second approach would be to fine tune neighborhood size using some hyper-parameter optimization scheme, yet those methods require larger datasets and an objective function to optimize. These two requirements are hard to meet when working with <italic>tvFC</italic> data. First, in contrast with other data modalities such as natural images, speech or genomics, fMRI datasets are often of a limited size [although this is changing thanks to recent international efforts such UK’s Biobank (<xref rid="B70" ref-type="bibr">Miller et al., 2016</xref>)]. Second, defining an objective function in this context is quite challenging. Not only it requires labeled data—which is almost nonexistent for resting-state—but as our data shows, it can be misleading (<xref rid="F9" ref-type="fig">Figure 9D</xref> shows an embedding that captures both subject and task identity but has low <italic>SI</italic><sub><italic>task</italic></sub>). A third approach would be to transfer heuristics from studies such as this one. We recently took this approach in a study looking at the temporal dynamics of FC during rest. We used the same multi-task dataset evaluated here to inform our selection of <italic>K</italic><sub><italic>nn</italic></sub> for <italic>LE</italic>. Using this approach, combined with reverse inference via <italic>Neurosynth</italic> (<xref rid="B90" ref-type="bibr">Yarkoni et al., 2011</xref>), we were able to show that resting-state <italic>tvFC</italic> patterns sitting at the corners of <italic>LE</italic> embeddings correspond to mental activities commonly reported as being performed during rest (<xref rid="B42" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>). Finally, an additional alternative would be to use newer versions of the algorithms that do not require a priori selection of neighborhood size such as perplexity-free <italic>T-SNE</italic> (<xref rid="B24" ref-type="bibr">Crecchi et al., 2020</xref>) or optimize concurrently at several scales (<xref rid="B60" ref-type="bibr">Lee et al., 2015</xref>), yet the performance of those algorithmic variants in <italic>tvFC</italic> data should be explored first.</p>
      </sec>
      <sec id="S5.SS4">
        <title>Group-level aggregation</title>
        <p>Functional connectivity is characterized by large inter-subject variability; and subject identification across sessions is possible using both static (<xref rid="B33" ref-type="bibr">Finn et al., 2014</xref>) and time-varying (<xref rid="B9" ref-type="bibr">Betzel et al., 2022</xref>) <italic>FC</italic> patterns as a form of fingerprinting. Here, inter-subject variability is clearly captured by group embeddings computed using the “<italic>Concatenate + Embed</italic>” approach on non-normalized data (see <xref rid="F8" ref-type="fig">Figures 8D</xref>, <xref rid="F9" ref-type="fig">9D</xref>). If data is normalized prior to concatenation, then subject identity is no longer depicted in the embeddings (see <xref rid="F8" ref-type="fig">Figures 8C</xref>, <xref rid="F9" ref-type="fig">9C</xref>). This suggests that one key differentiating aspect across subjects is differences in the mean and/or standard deviation of ROI-to-ROI FC traces. <xref rid="DS1" ref-type="supplementary-material">Supplementary Figures 7A, B</xref> show how the distributions of these two summary metrics vary across subjects, and how those differences are removed by the normalization step (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figures 7C, D</xref>).</p>
        <p>The second way to remove subject identifiability from the group-level embeddings is to generate those using the “<italic>Embed + Procrustes</italic>” approach. This works well independently of whether data are normalized or not. The fact that the <italic>Procrustes</italic> transformation—which only includes translations, rotations and uniform scaling—can bring scan-level embeddings into a common space where windows from the different tasks end up in the same portion of the lower dimensional space suggest that scan-level embeddings share a common geometrical shape and therefore that within-subject relationships between the <italic>FC</italic> patterns associated with the four different tasks are largely equivalent across subjects.</p>
      </sec>
      <sec id="S5.SS5">
        <title>The role of temporal autocorrelation</title>
        <p>While the “<italic>connectivity randomization</italic>” null model always resulted in embeddings with no discernable structure (<xref rid="F7" ref-type="fig">Figure 7D</xref>), that was not the case for the “<italic>phase randomization</italic>” null model (<xref rid="F7" ref-type="fig">Figure 7E</xref>). Although both models remove all neuronally meaningful information, they differ in one critical way. In the <italic>“connectivity randomization”</italic> model, randomization happens after the construction of the <italic>tvFC</italic> matrix. Conversely, in the <italic>“phase randomization”</italic> model, randomization is applied over the ROI timeseries and therefore precedes the sliding window procedure. Because of this, a substantial amount of temporal autocorrelation is reintroduced in this second null model during the sliding window procedure. This results in <italic>FC</italic> patterns from temporally contiguous windows being very similar to each other, even if those patterns are neuronally and behaviorally meaningless. When <italic>MLTs</italic> are then applied to this surrogate data, those temporally contiguous windows appear in proximity. Moreover, because each individual task block spans 91 such windows, one could get the impression that embeddings computed on this second null model were able to recover some task structure. Yet, that is not the case. All they do is to recapitulate the time dimension. They never place together windows from separate task blocks as it is the case with the embeddings computed over real data. In summary, the results from the “<italic>phase randomization</italic>” model do not suggest <italic>MLTs</italic> will show good task separability in data with no neuronally driven <italic>FC</italic>. What they highlight is the importance of considering the role of temporal autocorrelation when interpreting and evaluating embeddings.</p>
        <p>Because the goal of <italic>MLTs</italic> is to preserve local structure over global structure, and temporal autocorrelation is the largest source of local structure in <italic>tvFC</italic> data obtained with sliding window procedures, embeddings will easily recapitulate the time dimension in such data. Also, as mentioned in the discussion about the role of normalization in group-level embeddings above, <italic>MLTs</italic> tend to separate <italic>FC</italic> snapshots that have different mean and/or standard deviation. These two observations should be considered when selecting <italic>FC</italic> datasets for <italic>MLT</italic> benchmarking or interpreting embeddings generated with them. For example, benchmarking datasets should always include multiple temporally distant repetitions of each phenomenon of interest (e.g., mental states). The minimum temporal separation between them should be larger than the intrinsic temporal autocorrelation properties of the data. Moreover, to minimize systematic shifts in mean and standard deviation, such repetitions should occur within the confines of individual scans. In this sense, we believe that long multi-task scans acquired as subjects perform and transition between different tasks or mental states that repeat on several distant occasions might be the optimal type of data for benchmarking <italic>MLTs</italic> and related methods on <italic>tvFC</italic> data. Similarly, when using <italic>MLTs</italic> for summarization and interpretation of <italic>tvFC</italic>, one ought to ensure that observations are not easily explained by the two confounds discussed here: temporal autocorrelation induced during the generation of <italic>tvFC</italic> traces and/or systematic differences in average or volatility values due to factors such as using data from different subjects or different scans.</p>
      </sec>
      <sec id="S5.SS6">
        <title>Heuristics and future work</title>
        <p>One goal of this work was to provide a set of initial heuristics for those looking to apply <italic>MLTs</italic> to <italic>tvFC</italic> data. The following recommendations emerge from our observations. First, while all three evaluated <italic>MLTs</italic> can generate meaningful embeddings, they showed different behaviors. Overall, <italic>LE</italic> resulted in the best task separability. This occurred when using the <italic>Correlation</italic> distance and <italic>K</italic><sub><italic>nn</italic></sub> greater than 50. Although embedding quality is modulated by <italic>K</italic><sub><italic>nn</italic></sub>, and the optimal <italic>K</italic><sub><italic>nn</italic></sub> will be dataset specific, our results seem to suggest that in general the use of larger values is safer as it helps avoid disconnected graphs and embeddings that only capture inter-scan or inter-subject differences. Two additional benefits of <italic>LE</italic> are low computational demands, no optimization phase (which means no additional hyper-parameters to choose from). Additionally, previous applications of <italic>LE</italic> to <italic>FC</italic> data have proven quite successful (<xref rid="B65" ref-type="bibr">Margulies et al., 2016</xref>; <xref rid="B42" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>; <xref rid="B68" ref-type="bibr">Mckeown et al., 2020</xref>; <xref rid="B77" ref-type="bibr">Rué-Queralt et al., 2021</xref>). As such, <italic>LE</italic> might be a good choice for initial choice for those willing to start using <italic>MLTs</italic> on <italic>tvFC</italic> data.</p>
        <p>That said, <italic>LE</italic> was the only method that was not able to simultaneously capture two different levels of information (i.e., task and subject identity). In this regard, <italic>T-SNE</italic> and <italic>UMAP</italic> outperformed <italic>LE</italic>, and therefore if one seeks to obtain such multi-scale representations, these two methods may constitute a better alternative. Between both methods, <italic>UMAP</italic> is initially a better candidate because of its computational efficiency and higher stability across repeated iterations (<xref rid="DS1" ref-type="supplementary-material">Supplementary Figure 5</xref>). This is particularly true if one is willing to explore dimensions beyond three, as T-SNE’s computing times becomes significantly larger as the number of required dimensions increase. For <italic>UMAP</italic> our results suggest the use of the <italic>Euclidean</italic> distance and a preference over larger <italic>K</italic><sub><italic>nn</italic></sub> values in the same manner as just discussed for <italic>LE</italic>.</p>
        <p>Our exploration of <italic>MLTs</italic> is by no means comprehensive. This work is limited not only in terms of data size (20 scans) and evaluation metrics (clustering and classification), but also in terms of the breath of methods being evaluated. Because of that, future research should extend the evaluation presented here to additional datasets, other biological events of interest [e.g., detection of EEG microstates (<xref rid="B69" ref-type="bibr">Michel and Koenig, 2018</xref>)], and also consider other dimensionality reduction methods. For example, manifold estimation can also be accomplished via multidimensional scaling (<xref rid="B59" ref-type="bibr">Kruskal, 1964</xref>), <italic>ISOMAP</italic> (<xref rid="B83" ref-type="bibr">Tenenbaum et al., 2000</xref>), diffusion maps (<xref rid="B20" ref-type="bibr">Coifman et al., 2005</xref>), or T-PHATE (<xref rid="B13" ref-type="bibr">Busch et al., 2022</xref>), to name a few additional MLTs not considered here. All these other methods have been previously applied to fMRI data using either regional levels of activity (<xref rid="B39" ref-type="bibr">Gao et al., 2021</xref>; <xref rid="B13" ref-type="bibr">Busch et al., 2022</xref>) or static FC (<xref rid="B37" ref-type="bibr">Gallos et al., 2021a</xref>,<xref rid="B38" ref-type="bibr">b</xref>) as inputs. Future research should evaluate their efficacy on <italic>tvFC</italic> data. Meaningful dimensionality reduction can also be accomplished in other ways such as linear decomposition methods (e.g., Principal Component Analysis, Independent Component Analysis, Non-negative Matrix Factorization, etc.), using deep neural networks [e.g., autoencoders (<xref rid="B89" ref-type="bibr">Wang et al., 2014</xref>)] or TDA methods [e.g., Mapper (<xref rid="B78" ref-type="bibr">Saggar et al., 2018</xref>)]. All these alternatives should also be considered as valuable candidates for dimensionality reduction of <italic>tvFC</italic> data. Of particular interest for resting-state applications is the case of autoencoders because evaluation of low dimensionality representations in this case do not necessarily require labeled data, and prior work has shown their ability to capture generative factors underlying resting-state activity (<xref rid="B54" ref-type="bibr">Kim B.-H. et al., 2021</xref>; <xref rid="B55" ref-type="bibr">Kim J.-H. et al., 2021</xref>).</p>
        <p>Finally, manifold learning methods applied to <italic>tvFC</italic> matrices is one of the many tools one can use to explore and model time-varying aspects of FC. For example, one can also rely on clustering methods (<xref rid="B2" ref-type="bibr">Allen et al., 2014</xref>) or Hidden Markov models (<xref rid="B87" ref-type="bibr">Vidaurre et al., 2018</xref>) to find meaningful recurrent configurations and their spatiotemporal profiles. Alternatively, one can also explore the dynamics of fMRI signals using tools that do not require the generation of the <italic>tvFC</italic> matrix based on sliding window procedures, such as edge timeseries (<xref rid="B32" ref-type="bibr">Faskowitz et al., 2020</xref>), co-activation patterns (<xref rid="B63" ref-type="bibr">Liu et al., 2018</xref>) or quasi-period patterns (<xref rid="B84" ref-type="bibr">Thompson et al., 2014</xref>). All these should be considered when looking to explore dynamical aspects of the functional connectome, and ultimately, which one to use will depend on the specific scientific question at hand.</p>
      </sec>
    </sec>
    <sec sec-type="conclusion" id="S6">
      <title>Conclusion</title>
      <p>Dimensionality reduction, particularly manifold learning, can play a key role in the summarization and interpretation of <italic>tvFC</italic> data, especially when such data is utilized to study experimentally unconstrained phenomena such as mind wandering, spontaneous memory recall, and naturalistic paradigms. Yet, because most <italic>MLTs</italic> are benchmarked and developed using data modalities with different properties to that of <italic>tvFC</italic>, extreme caution must be exerted when transferring methods and heuristics from these other scientific disciplines. To alleviate this issue, here we evaluated three state-of-the art <italic>MLTs</italic> using labeled <italic>tvFC</italic> data. This evaluation suggests that <italic>LE</italic> and <italic>UMAP</italic> outperform T-<italic>SNE</italic> for this type of data. It also highlights the confounding role of temporal autocorrelation, and how it can artifactually inflate evaluation metrics. While we only test a few methods on one dataset, we hope this report actively contributes to the steady building of a much-needed bridge between the fields of neuroimaging and machine learning. Future steps in this direction should include the generation of neuroimaging-based benchmarking datasets that can be easily added to existing benchmarking efforts (<xref rid="B14" ref-type="bibr">Campadelli et al., 2015</xref>), and the development of <italic>MLTs</italic> tailored to address the specific needs and characteristics of <italic>tvFC</italic> data.</p>
    </sec>
    <sec sec-type="data-availability" id="S7">
      <title>Data availability statement</title>
      <p>The original contributions presented in this study are included in the article/<xref rid="DS1" ref-type="supplementary-material">Supplementary material</xref>. Code used in this work can be accessed at: <ext-link xlink:href="https://github.com/nimh-sfim/manifold_learning_fmri" ext-link-type="uri">https://github.com/nimh-sfim/manifold_learning_fmri</ext-link>. Further inquiries can be directed to the corresponding author.</p>
    </sec>
    <sec sec-type="ethics-statement" id="S8">
      <title>Ethics statement</title>
      <p>The studies involving human participants were reviewed and approved by the National Institute of Mental Health. The patients/participants provided their written informed consent to participate in this study.</p>
    </sec>
    <sec sec-type="author-contributions" id="S9">
      <title>Author contributions</title>
      <p>JG-C: conceptualization, methodology, software, formal analysis, writing—original draft, and visualization. IF: software, formal analysis, writing—review and editing, and visualization. KL: software, methodology, and writing—review and editing. FP: software, methodology, conceptualization, and writing—review and editing. DH: conceptualization, and writing—review and editing. PB: conceptualization, writing—review and editing, and funding acquisition. All authors contributed to the article and approved the submitted version.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>Portions of this study used the high-performance computational capabilities of the <italic>Biowulf</italic> Linux cluster at the National Institutes of Health, Bethesda, MD, United States (<ext-link xlink:href="https://hpc.nih.gov" ext-link-type="uri">hpc.nih.gov</ext-link>).</p>
    </ack>
    <fn-group>
      <fn id="footnote1">
        <label>1</label>
        <p>Some implementations of the <italic>LE</italic> algorithm, like the one in <italic>skicit-learn</italic>, work with the normalized version of the <italic>Laplacian</italic> matrix.</p>
      </fn>
      <fn id="footnote2">
        <label>2</label>
        <p>
<ext-link xlink:href="https://github.com/nimh-sfim/manifold_learning_fmri" ext-link-type="uri">https://github.com/nimh-sfim/manifold_learning_fmri</ext-link>
</p>
      </fn>
    </fn-group>
    <sec sec-type="COI-statement" id="S11">
      <title>Conflict of interest</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
    <sec sec-type="disclaimer" id="S12">
      <title>Publisher’s note</title>
      <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
    </sec>
    <sec sec-type="supplementary-material" id="S13">
      <title>Supplementary material</title>
      <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fnhum.2023.1134012/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fnhum.2023.1134012/full#supplementary-material</ext-link></p>
      <supplementary-material id="DS1" position="float" content-type="local-data">
        <media xlink:href="Data_Sheet_1.docx">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
    <ref-list>
      <title>References</title>
      <ref id="B1">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albergante</surname><given-names>L.</given-names></name><name><surname>Bac</surname><given-names>J.</given-names></name><name><surname>Zinovyev</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>Estimating the effective dimension of large biological datasets using Fisher separability analysis.</article-title>
<source><italic>arXiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1109/ijcnn.2019.8852450</pub-id></mixed-citation>
      </ref>
      <ref id="B2">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>E.</given-names></name><name><surname>Damaraju</surname><given-names>E.</given-names></name><name><surname>Plis</surname><given-names>S.</given-names></name><name><surname>Erhardt</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>Tracking whole-brain connectivity dynamics in the resting state.</article-title>
<source><italic>Cereb. Cortex</italic></source>
<volume>24</volume>
<fpage>663</fpage>–<lpage>676</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs352</pub-id>
<pub-id pub-id-type="pmid">23146964</pub-id></mixed-citation>
      </ref>
      <ref id="B3">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amsaleg</surname><given-names>L.</given-names></name><name><surname>Bailey</surname><given-names>J.</given-names></name><name><surname>Barbe</surname><given-names>D.</given-names></name><name><surname>Erfani</surname><given-names>S.</given-names></name><name><surname>Houle</surname><given-names>M. E.</given-names></name><name><surname>Nguyen</surname><given-names>V.</given-names></name><etal/></person-group> (<year>2017</year>). “<article-title>The Vulnerability of Learning to Adversarial Perturbation Increases with Intrinsic Dimensionality</article-title>,” in <source><italic>Proceedings of the IEEE Workshop on Information Forensics and Security</italic></source>, <publisher-loc>Manhattan, NY</publisher-loc>, <pub-id pub-id-type="doi">10.1109/wifs.2017.8267651</pub-id></mixed-citation>
      </ref>
      <ref id="B4">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ansuini</surname><given-names>A.</given-names></name><name><surname>Laio</surname><given-names>A.</given-names></name><name><surname>Macke</surname><given-names>J. H.</given-names></name><name><surname>Zoccolan</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). “<article-title>Intrinsic dimension of data representations in deep neural networks</article-title>,” in <source><italic>Proceedings of the Advances in Neural Information Processing Systems</italic></source>, <publisher-loc>Red Hook, NY</publisher-loc>.</mixed-citation>
      </ref>
      <ref id="B5">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bac</surname><given-names>J.</given-names></name><name><surname>Mirkes</surname><given-names>E. M.</given-names></name><name><surname>Gorban</surname><given-names>A. N.</given-names></name><name><surname>Tyukin</surname><given-names>I.</given-names></name><name><surname>Zinovyev</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). <article-title>Scikit-dimension: A python package for intrinsic dimension estimation.</article-title>
<source><italic>Entropy</italic></source>
<volume>23</volume>:<issue>1368</issue>. <pub-id pub-id-type="doi">10.3390/e23101368</pub-id>
<pub-id pub-id-type="pmid">34682092</pub-id></mixed-citation>
      </ref>
      <ref id="B6">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname><given-names>M.</given-names></name><name><surname>Lyday</surname><given-names>R. G.</given-names></name><name><surname>Casanova</surname><given-names>R.</given-names></name><name><surname>Burdette</surname><given-names>J. H.</given-names></name><name><surname>Simpson</surname><given-names>S. L.</given-names></name><name><surname>Laurienti</surname><given-names>P. J.</given-names></name></person-group> (<year>2019</year>). <article-title>Using low-dimensional manifolds to map relationships between dynamic brain networks.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>13</volume>:<issue>430</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2019.00430</pub-id>
<pub-id pub-id-type="pmid">31920590</pub-id></mixed-citation>
      </ref>
      <ref id="B7">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname><given-names>D. S.</given-names></name><name><surname>Meyer-Lindenberg</surname><given-names>A.</given-names></name><name><surname>Achard</surname><given-names>S.</given-names></name><name><surname>Duke</surname><given-names>T.</given-names></name><name><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2006</year>). <article-title>Adaptive reconfiguration of fractal small-world human brain functional networks.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>103</volume>
<fpage>19518</fpage>–<lpage>19523</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0606005103</pub-id>
<pub-id pub-id-type="pmid">17159150</pub-id></mixed-citation>
      </ref>
      <ref id="B8">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belkin</surname><given-names>M.</given-names></name><name><surname>Niyogi</surname><given-names>P.</given-names></name></person-group> (<year>2003</year>). <article-title>Laplacian eigenmaps for dimensionality reduction and data representation.</article-title>
<source><italic>Neural Comput.</italic></source>
<volume>15</volume>
<fpage>1373</fpage>–<lpage>1396</lpage>. <pub-id pub-id-type="doi">10.1162/089976603321780317</pub-id></mixed-citation>
      </ref>
      <ref id="B9">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzel</surname><given-names>R. F.</given-names></name><name><surname>Cutts</surname><given-names>S. A.</given-names></name><name><surname>Greenwell</surname><given-names>S.</given-names></name><name><surname>Faskowitz</surname><given-names>J.</given-names></name><name><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2022</year>). <article-title>Individualized event structure drives individual differences in whole-brain functional connectivity.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>252</volume>:<issue>118993</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.118993</pub-id>
<pub-id pub-id-type="pmid">35192942</pub-id></mixed-citation>
      </ref>
      <ref id="B10">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beyer</surname><given-names>K.</given-names></name><name><surname>Goldstein</surname><given-names>J.</given-names></name><name><surname>Ramakrishnan</surname><given-names>R.</given-names></name><name><surname>Shaft</surname><given-names>U.</given-names></name></person-group> (<year>1999</year>). “<article-title>Database Theory — ICDT’99</article-title>,” in <source><italic>Proceedings of the 7th International Conference Jerusalem</italic></source>, <publisher-loc>Jerusalem</publisher-loc>, <pub-id pub-id-type="doi">10.1007/3-540-49257-7_15</pub-id>
</mixed-citation>
      </ref>
      <ref id="B11">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Billings</surname><given-names>J. C. W.</given-names></name><name><surname>Medda</surname><given-names>A.</given-names></name><name><surname>Shakil</surname><given-names>S.</given-names></name><name><surname>Shen</surname><given-names>X.</given-names></name><name><surname>Kashyap</surname><given-names>A.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Instantaneous brain dynamics mapped to a continuous state space.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>162</volume>
<fpage>344</fpage>–<lpage>352</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.042</pub-id>
<pub-id pub-id-type="pmid">28823826</pub-id></mixed-citation>
      </ref>
      <ref id="B12">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswal</surname><given-names>B.</given-names></name><name><surname>Yetkin</surname><given-names>F. Z.</given-names></name><name><surname>Haughton</surname><given-names>V. M.</given-names></name><name><surname>Hyde</surname><given-names>J. S.</given-names></name></person-group> (<year>1995</year>). <article-title>Functional connectivity in the motor cortex of resting human brain using echo-planar MRI.</article-title>
<source><italic>Magnet. Reson. Med.</italic></source>
<volume>34</volume>
<fpage>537</fpage>–<lpage>541</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.1910340409</pub-id>
<pub-id pub-id-type="pmid">8524021</pub-id></mixed-citation>
      </ref>
      <ref id="B13">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busch</surname><given-names>E. L.</given-names></name><name><surname>Huang</surname><given-names>J.</given-names></name><name><surname>Benz</surname><given-names>A.</given-names></name><name><surname>Wallenstein</surname><given-names>T.</given-names></name><name><surname>Lajoie</surname><given-names>G.</given-names></name><name><surname>Wolf</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Multi-view manifold learning of human brain state trajectories.</article-title>
<source><italic>bioRxiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.1101/2022.05.03.490534</pub-id></mixed-citation>
      </ref>
      <ref id="B14">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campadelli</surname><given-names>P.</given-names></name><name><surname>Casiraghi</surname><given-names>E.</given-names></name><name><surname>Ceruti</surname><given-names>C.</given-names></name><name><surname>Rozza</surname><given-names>A.</given-names></name></person-group> (<year>2015</year>). <article-title>Intrinsic Dimension Estimation: Relevant Techniques and a Benchmark Framework.</article-title>
<source><italic>Math. Probl. Eng.</italic></source>
<volume>2015</volume>
<fpage>1</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1155/2015/759567</pub-id></mixed-citation>
      </ref>
      <ref id="B15">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlsson</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>Topology and data B.</article-title>
<source><italic>Am. Math. Soc.</italic></source>
<volume>46</volume>
<fpage>255</fpage>–<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1090/s0273-0979-09-01249-x</pub-id>
<pub-id pub-id-type="pmid">30656504</pub-id></mixed-citation>
      </ref>
      <ref id="B16">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Casanova</surname><given-names>R.</given-names></name><name><surname>Lyday</surname><given-names>R. G.</given-names></name><name><surname>Bahrami</surname><given-names>M.</given-names></name><name><surname>Burdette</surname><given-names>J. H.</given-names></name><name><surname>Simpson</surname><given-names>S. L.</given-names></name><name><surname>Laurienti</surname><given-names>P. J.</given-names></name></person-group> (<year>2021</year>). <article-title>Embedding Functional Brain Networks in Low Dimensional Spaces Using Manifold Learning Techniques.</article-title>
<source><italic>Front. Neuroinform.</italic></source>
<volume>15</volume>:<issue>740143</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2021.740143</pub-id>
<pub-id pub-id-type="pmid">35002665</pub-id></mixed-citation>
      </ref>
      <ref id="B17">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>J. M.</given-names></name><name><surname>Cleveland</surname><given-names>W. S.</given-names></name><name><surname>Kleiner</surname><given-names>B.</given-names></name><name><surname>Tukey</surname><given-names>P. A.</given-names></name></person-group> (<year>2018</year>). <source><italic>Graphical Methods for Data Analysis.</italic></source>
<publisher-loc>Wadsworth</publisher-loc>: <publisher-name>Bellmont</publisher-name>, <pub-id pub-id-type="doi">10.1201/9781351072304-4</pub-id></mixed-citation>
      </ref>
      <ref id="B18">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chazal</surname><given-names>F.</given-names></name><name><surname>Michel</surname><given-names>B.</given-names></name></person-group> (<year>2021</year>). <article-title>An introduction to topological data analysis: Fundamental and practical aspects for data scientists.</article-title>
<source><italic>Front. Artif. Intell.</italic></source>
<volume>4</volume>:<issue>667963</issue>. <pub-id pub-id-type="doi">10.3389/frai.2021.667963</pub-id>
<pub-id pub-id-type="pmid">34661095</pub-id></mixed-citation>
      </ref>
      <ref id="B19">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J. E.</given-names></name><name><surname>Chang</surname><given-names>C.</given-names></name><name><surname>Greicius</surname><given-names>M. D.</given-names></name><name><surname>Glover</surname><given-names>G. H.</given-names></name></person-group> (<year>2015</year>). <article-title>Introducing co-activation pattern metrics to quantify spontaneous brain network dynamics.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>111</volume>
<fpage>476</fpage>–<lpage>488</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.01.057</pub-id>
<pub-id pub-id-type="pmid">25662866</pub-id></mixed-citation>
      </ref>
      <ref id="B20">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coifman</surname><given-names>R. R.</given-names></name><name><surname>Lafon</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>A. B.</given-names></name><name><surname>Maggioni</surname><given-names>M.</given-names></name><name><surname>Nadler</surname><given-names>B.</given-names></name><name><surname>Warner</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2005</year>). <article-title>Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>102</volume>
<fpage>7426</fpage>–<lpage>7431</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0500334102</pub-id>
<pub-id pub-id-type="pmid">15899970</pub-id></mixed-citation>
      </ref>
      <ref id="B21">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>M. W.</given-names></name><name><surname>Bassett</surname><given-names>D. S.</given-names></name><name><surname>Power</surname><given-names>J. D.</given-names></name><name><surname>Braver</surname><given-names>T. S.</given-names></name><name><surname>Petersen</surname><given-names>S. E.</given-names></name></person-group> (<year>2014</year>). <article-title>Intrinsic and Task-Evoked Network Architectures of the Human Brain.</article-title>
<source><italic>Neuron</italic></source>
<volume>83</volume>
<fpage>238</fpage>–<lpage>251</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.014</pub-id>
<pub-id pub-id-type="pmid">24991964</pub-id></mixed-citation>
      </ref>
      <ref id="B22">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>). <article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages 29.</article-title>
<source><italic>Comput. Biomed. Res.</italic></source>
<volume>29</volume>
<fpage>162</fpage>–<lpage>173</lpage>.<pub-id pub-id-type="pmid">8812068</pub-id></mixed-citation>
      </ref>
      <ref id="B23">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craddock</surname><given-names>R. C.</given-names></name><name><surname>James</surname><given-names>G. A.</given-names></name><name><surname>Holtzheimer</surname><given-names>P. E.</given-names></name><name><surname>Hu</surname><given-names>X. P.</given-names></name><name><surname>Mayberg</surname><given-names>H. S.</given-names></name></person-group> (<year>2012</year>). <article-title>A whole brain fMRI atlas generated via spatially constrained spectral clustering.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>33</volume>
<fpage>1914</fpage>–<lpage>1928</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.21333</pub-id>
<pub-id pub-id-type="pmid">21769991</pub-id></mixed-citation>
      </ref>
      <ref id="B24">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crecchi</surname><given-names>F.</given-names></name><name><surname>de Bodt</surname><given-names>C.</given-names></name><name><surname>Verleysen</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>J. A.</given-names></name><name><surname>Bacciu</surname><given-names>D.</given-names></name></person-group> (<year>2020</year>). <article-title>Perplexity-free Parametric t-SNE.</article-title>
<source><italic>arXiv.</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.48550/arxiv.2010.01359</pub-id></mixed-citation>
      </ref>
      <ref id="B25">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damaraju</surname><given-names>E.</given-names></name><name><surname>Allen</surname><given-names>E. A.</given-names></name><name><surname>Belger</surname><given-names>A.</given-names></name><name><surname>Ford</surname><given-names>J. M.</given-names></name><name><surname>McEwen</surname><given-names>S.</given-names></name><name><surname>Mathalon</surname><given-names>D. H.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Dynamic functional connectivity analysis reveals transient states of dysconnectivity in schizophrenia.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>5</volume>
<fpage>298</fpage>–<lpage>308</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2014.07.003</pub-id>
<pub-id pub-id-type="pmid">25161896</pub-id></mixed-citation>
      </ref>
      <ref id="B26">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaz-Papkovich</surname><given-names>A.</given-names></name><name><surname>Anderson-Trocmé</surname><given-names>L.</given-names></name><name><surname>Ben-Eghan</surname><given-names>C.</given-names></name><name><surname>Gravel</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>UMAP reveals cryptic population structure and phenotype heterogeneity in large genomic cohorts.</article-title>
<source><italic>PLoS Genet.</italic></source>
<volume>15</volume>:<issue>e1008432</issue>. <pub-id pub-id-type="doi">10.1371/journal.pgen.1008432</pub-id>
<pub-id pub-id-type="pmid">31675358</pub-id></mixed-citation>
      </ref>
      <ref id="B27">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dini</surname><given-names>H.</given-names></name><name><surname>Sendi</surname><given-names>M. S. E.</given-names></name><name><surname>Sui</surname><given-names>J.</given-names></name><name><surname>Fu</surname><given-names>Z.</given-names></name><name><surname>Espinoza</surname><given-names>R.</given-names></name><name><surname>Narr</surname><given-names>K. L.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Dynamic Functional Connectivity Predicts Treatment Response to Electroconvulsive Therapy in Major Depressive Disorder.</article-title>
<source><italic>Front. Hum. Neurosci.</italic></source>
<volume>15</volume>:<issue>689488</issue>. <pub-id pub-id-type="doi">10.3389/fnhum.2021.689488</pub-id>
<pub-id pub-id-type="pmid">34295231</pub-id></mixed-citation>
      </ref>
      <ref id="B28">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elton</surname><given-names>A.</given-names></name><name><surname>Gao</surname><given-names>W.</given-names></name></person-group> (<year>2015</year>). <article-title>Task-related modulation of functional connectivity variability and its behavioral correlations.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>36</volume>
<fpage>3260</fpage>–<lpage>3272</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.22847</pub-id>
<pub-id pub-id-type="pmid">26015070</pub-id></mixed-citation>
      </ref>
      <ref id="B29">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Facco</surname><given-names>E.</given-names></name><name><surname>d’Errico</surname><given-names>M.</given-names></name><name><surname>Rodriguez</surname><given-names>A.</given-names></name><name><surname>Laio</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Estimating the intrinsic dimension of datasets by a minimal neighborhood information.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>7</volume>:<issue>12140</issue>. <pub-id pub-id-type="doi">10.1038/s41598-017-11873-y</pub-id>
<pub-id pub-id-type="pmid">28939866</pub-id></mixed-citation>
      </ref>
      <ref id="B30">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Facco</surname><given-names>E.</given-names></name><name><surname>Pagnani</surname><given-names>A.</given-names></name><name><surname>Russo</surname><given-names>E. T.</given-names></name><name><surname>Laio</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>The intrinsic dimension of protein sequence evolution.</article-title>
<source><italic>PLoS Comput. Biol.</italic></source>
<volume>15</volume>:<issue>e1006767</issue>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006767</pub-id>
<pub-id pub-id-type="pmid">30958823</pub-id></mixed-citation>
      </ref>
      <ref id="B31">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>M.</given-names></name><name><surname>Gu</surname><given-names>N.</given-names></name><name><surname>Qiao</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>Intrinsic dimension estimation of data by principal component analysis.</article-title>
<source><italic>arXiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.48550/arxiv.1002.2050</pub-id></mixed-citation>
      </ref>
      <ref id="B32">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faskowitz</surname><given-names>J.</given-names></name><name><surname>Esfahlani</surname><given-names>F. Z.</given-names></name><name><surname>Jo</surname><given-names>Y.</given-names></name><name><surname>Sporns</surname><given-names>O.</given-names></name><name><surname>Betzel</surname><given-names>R. F.</given-names></name></person-group> (<year>2020</year>). <article-title>Edge-centric functional network representations of human cerebral cortex reveal overlapping system-level architecture.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>23</volume>
<fpage>1644</fpage>–<lpage>1654</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-020-00719-y</pub-id>
<pub-id pub-id-type="pmid">33077948</pub-id></mixed-citation>
      </ref>
      <ref id="B33">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>E.</given-names></name><name><surname>Shen</surname><given-names>X.</given-names></name><name><surname>Scheinost</surname><given-names>D.</given-names></name><name><surname>Rosenberg</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Functional connectome fingerprinting: identifying individuals using patterns of brain connectivity.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>18</volume>:<issue>4135</issue>. <pub-id pub-id-type="doi">10.1038/nn.4135</pub-id>
<pub-id pub-id-type="pmid">26457551</pub-id></mixed-citation>
      </ref>
      <ref id="B34">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorenzato</surname><given-names>E.</given-names></name><name><surname>Strafella</surname><given-names>A. P.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name><name><surname>Schifano</surname><given-names>R.</given-names></name><name><surname>Weis</surname><given-names>L.</given-names></name><name><surname>Antonini</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Dynamic functional connectivity changes associated with dementia in Parkinson’s disease.</article-title>
<source><italic>Brain J. Neurol.</italic></source>
<volume>142</volume>
<fpage>2860</fpage>–<lpage>2872</lpage>. <pub-id pub-id-type="doi">10.1093/brain/awz192</pub-id>
<pub-id pub-id-type="pmid">31280293</pub-id></mixed-citation>
      </ref>
      <ref id="B35">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>V.</given-names></name><name><surname>Evans</surname><given-names>A.</given-names></name><name><surname>McKinstry</surname><given-names>R.</given-names></name><name><surname>Almli</surname><given-names>C.</given-names></name><name><surname>Collins</surname><given-names>D.</given-names></name></person-group> (<year>2009</year>). <article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>47</volume>:<issue>S102</issue>. <pub-id pub-id-type="doi">10.1016/s1053-8119(09)70884-5</pub-id></mixed-citation>
      </ref>
      <ref id="B36">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>France</surname><given-names>S.</given-names></name><name><surname>Carroll</surname><given-names>D.</given-names></name></person-group> (<year>2009</year>). “<article-title>Machine Learning and Data Mining in Pattern Recognition</article-title>,” in <source><italic>Proceedings of the 6th International Conference</italic></source>, <publisher-loc>Leipzig</publisher-loc>, <pub-id pub-id-type="doi">10.1007/978-3-642-03070-3_21</pub-id></mixed-citation>
      </ref>
      <ref id="B37">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallos</surname><given-names>I. K.</given-names></name><name><surname>Galaris</surname><given-names>E.</given-names></name><name><surname>Siettos</surname><given-names>C. I.</given-names></name></person-group> (<year>2021a</year>). <article-title>Construction of embedded fMRI resting-state functional connectivity networks using manifold learning.</article-title>
<source><italic>Cogn. Neurodyn.</italic></source>
<volume>15</volume>
<fpage>585</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s11571-020-09645-y</pub-id>
<pub-id pub-id-type="pmid">34367362</pub-id></mixed-citation>
      </ref>
      <ref id="B38">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallos</surname><given-names>I. K.</given-names></name><name><surname>Gkiatis</surname><given-names>K.</given-names></name><name><surname>Matsopoulos</surname><given-names>G. K.</given-names></name><name><surname>Siettos</surname><given-names>C.</given-names></name></person-group> (<year>2021b</year>). <article-title>ISOMAP and machine learning algorithms for the construction of embedded functional connectivity networks of anatomically separated brain regions from resting state fMRI data of patients with Schizophrenia.</article-title>
<source><italic>Aims Neurosci.</italic></source>
<volume>8</volume>
<fpage>295</fpage>–<lpage>321</lpage>. <pub-id pub-id-type="doi">10.3934/neuroscience.2021016</pub-id>
<pub-id pub-id-type="pmid">33709030</pub-id></mixed-citation>
      </ref>
      <ref id="B39">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>S.</given-names></name><name><surname>Mishne</surname><given-names>G.</given-names></name><name><surname>Scheinost</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>Nonlinear manifold learning in functional magnetic resonance imaging uncovers a low-dimensional space of brain dynamics.</article-title>
<source><italic>Hum. Brain Mapp.</italic></source>
<volume>42</volume>
<fpage>4510</fpage>–<lpage>4524</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25561</pub-id>
<pub-id pub-id-type="pmid">34184812</pub-id></mixed-citation>
      </ref>
      <ref id="B40">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goñi</surname><given-names>J.</given-names></name><name><surname>van den Heuvel</surname><given-names>M. P.</given-names></name><name><surname>Avena-Koenigsberger</surname><given-names>A.</given-names></name><name><surname>Mendizabal</surname><given-names>N. V., de</given-names></name><name><surname>Betzel</surname><given-names>R. F.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Resting-brain functional connectivity predicted by analytic measures of network communication.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>111</volume>
<fpage>833</fpage>–<lpage>838</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1315529111</pub-id>
<pub-id pub-id-type="pmid">24379387</pub-id></mixed-citation>
      </ref>
      <ref id="B41">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Bandettini</surname><given-names>P. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Task-based dynamic functional connectivity: Recent findings and open questions.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>180</volume>
<fpage>526</fpage>–<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.006</pub-id>
<pub-id pub-id-type="pmid">28780401</pub-id></mixed-citation>
      </ref>
      <ref id="B42">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Caballero-Gaudes</surname><given-names>C.</given-names></name><name><surname>Topolski</surname><given-names>N.</given-names></name><name><surname>Handwerker</surname><given-names>D. A.</given-names></name><name><surname>Pereira</surname><given-names>F.</given-names></name><name><surname>Bandettini</surname><given-names>P. A.</given-names></name></person-group> (<year>2019</year>). <article-title>Imaging the spontaneous flow of thought: Distinct periods of cognition contribute to dynamic functional connectivity during rest.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>202</volume>:<issue>116129</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116129</pub-id>
<pub-id pub-id-type="pmid">31461679</pub-id></mixed-citation>
      </ref>
      <ref id="B43">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Duthie</surname><given-names>K.</given-names></name><name><surname>Saad</surname><given-names>Z.</given-names></name><name><surname>Chu</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Effects of image contrast on functional MRI image registration.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>67</volume>
<fpage>163</fpage>–<lpage>174</lpage>.<pub-id pub-id-type="pmid">23128074</pub-id></mixed-citation>
      </ref>
      <ref id="B44">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Hoy</surname><given-names>C. W.</given-names></name><name><surname>Handwerker</surname><given-names>D.</given-names></name><name><surname>Robinson</surname><given-names>M. E.</given-names></name><name><surname>Buchanan</surname><given-names>L. C.</given-names></name><name><surname>Saad</surname><given-names>Z. S.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Tracking ongoing cognition in individuals using brief, whole-brain functional connectivity patterns.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>112</volume>
<fpage>8762</fpage>–<lpage>8767</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1501242112</pub-id>
<pub-id pub-id-type="pmid">26124112</pub-id></mixed-citation>
      </ref>
      <ref id="B45">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Handwerker</surname><given-names>D.</given-names></name><name><surname>Roopchansingh</surname><given-names>V.</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>). <article-title>Periodic changes in fMRI connectivity.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>63</volume>
<fpage>1712</fpage>–<lpage>1719</lpage>.<pub-id pub-id-type="pmid">22796990</pub-id></mixed-citation>
      </ref>
      <ref id="B46">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hardikar</surname><given-names>S.</given-names></name><name><surname>Mckeown</surname><given-names>B.</given-names></name><name><surname>Schaare</surname><given-names>H. L.</given-names></name><name><surname>Xu</surname><given-names>T.</given-names></name><name><surname>Lauckner</surname><given-names>M. E.</given-names></name><name><surname>Valk</surname><given-names>S. L.</given-names></name><etal/></person-group> (<year>2022</year>). <article-title>Macro-scale patterns in functional connectivity associated with ongoing thought patterns and dispositional traits.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>220</volume>:<issue>117072</issue>. <pub-id pub-id-type="doi">10.1101/2022.10.11.511591</pub-id></mixed-citation>
      </ref>
      <ref id="B47">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassanat</surname><given-names>A. B.</given-names></name><name><surname>Abbadi</surname><given-names>M. A.</given-names></name><name><surname>Altarawneh</surname><given-names>G. A.</given-names></name><name><surname>Alhasanat</surname><given-names>A. A.</given-names></name></person-group> (<year>2014</year>). <article-title>Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach.</article-title>
<source><italic>arXiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.48550/arxiv.1409.0919</pub-id></mixed-citation>
      </ref>
      <ref id="B48">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>J. V.</given-names></name><name><surname>Guntupalli</surname><given-names>J. S.</given-names></name><name><surname>Connolly</surname><given-names>A. C.</given-names></name><name><surname>Halchenko</surname><given-names>Y. O.</given-names></name><name><surname>Conroy</surname><given-names>B. R.</given-names></name><name><surname>Gobbini</surname><given-names>M. I.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex.</article-title>
<source><italic>Neuron</italic></source>
<volume>72</volume>
<fpage>404</fpage>–<lpage>416</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id>
<pub-id pub-id-type="pmid">22017997</pub-id></mixed-citation>
      </ref>
      <ref id="B49">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>W.-T.</given-names></name><name><surname>Rosenberg</surname><given-names>M. D.</given-names></name><name><surname>Scheinost</surname><given-names>D.</given-names></name><name><surname>Constable</surname><given-names>R. T.</given-names></name><name><surname>Chun</surname><given-names>M. M.</given-names></name></person-group> (<year>2018</year>). <article-title>Resting-state functional connectivity predicts neuroticism and extraversion in novel individuals.</article-title>
<source><italic>Soc. Cogn. Affect. Neur.</italic></source>
<volume>13</volume>
<fpage>224</fpage>–<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1093/scan/nsy002</pub-id>
<pub-id pub-id-type="pmid">29373729</pub-id></mixed-citation>
      </ref>
      <ref id="B50">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inselberg</surname><given-names>A.</given-names></name><name><surname>Dimsdale</surname><given-names>B.</given-names></name></person-group> (<year>1990</year>). <article-title>Parallel coordinates: a tool for visualizing multi-dimensional geometry.</article-title>
<source><italic>Proc. First IEEE Conf. Vis. Vis.</italic></source>
<volume>90</volume>
<fpage>361</fpage>–<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1109/visual.1990.146402</pub-id></mixed-citation>
      </ref>
      <ref id="B51">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jangraw</surname><given-names>D. C.</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Handwerker</surname><given-names>D. A.</given-names></name><name><surname>Ghane</surname><given-names>M.</given-names></name><name><surname>Rosenberg</surname><given-names>M. D.</given-names></name><name><surname>Panwar</surname><given-names>P.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>A functional connectivity-based neuromarker of sustained attention generalizes to predict recall in a reading task.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>166</volume>
<fpage>99</fpage>–<lpage>109</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.019</pub-id>
<pub-id pub-id-type="pmid">29031531</pub-id></mixed-citation>
      </ref>
      <ref id="B52">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jo</surname><given-names>H.</given-names></name><name><surname>Saad</surname><given-names>Z. S.</given-names></name><name><surname>Simmons</surname><given-names>K. W.</given-names></name><name><surname>Milbury</surname><given-names>L. A.</given-names></name><name><surname>Cox</surname><given-names>R. W.</given-names></name></person-group> (<year>2010</year>). <article-title>Mapping sources of correlation in resting state FMRI, with artifact detection and removal.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>52</volume>
<fpage>571</fpage>–<lpage>582</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.246</pub-id>
<pub-id pub-id-type="pmid">20420926</pub-id></mixed-citation>
      </ref>
      <ref id="B53">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>R. H.</given-names></name><name><surname>Kang</surname><given-names>M. S.</given-names></name><name><surname>Lew</surname><given-names>Y.</given-names></name><name><surname>Feen</surname><given-names>J. V. D.</given-names></name><name><surname>Aguirre</surname><given-names>B.</given-names></name><name><surname>Clegg</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Abnormal frontoinsular-default network dynamics in adolescent depression and rumination: a preliminary resting-state co-activation pattern analysis.</article-title>
<source><italic>Neuropsychopharmacol</italic></source>
<volume>44</volume>
<fpage>1604</fpage>–<lpage>1612</lpage>. <pub-id pub-id-type="doi">10.1038/s41386-019-0399-3</pub-id>
<pub-id pub-id-type="pmid">31035283</pub-id></mixed-citation>
      </ref>
      <ref id="B54">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>B.-H.</given-names></name><name><surname>Ye</surname><given-names>J. C.</given-names></name><name><surname>Kim</surname><given-names>J.-J.</given-names></name></person-group> (<year>2021</year>). <article-title>Learning dynamic graph representation of brain connectome with spatio-temporal attention.</article-title>
<source><italic>arXiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.48550/arxiv.2105.13495</pub-id></mixed-citation>
      </ref>
      <ref id="B55">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J.-H.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Han</surname><given-names>K.</given-names></name><name><surname>Wen</surname><given-names>Z.</given-names></name><name><surname>Choi</surname><given-names>M.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name></person-group> (<year>2021</year>). <article-title>Representation learning of resting state fMRI with variational autoencoder.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>241</volume>:<issue>118423</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118423</pub-id>
<pub-id pub-id-type="pmid">34303794</pub-id></mixed-citation>
      </ref>
      <ref id="B56">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D.</given-names></name><name><surname>Berens</surname><given-names>P.</given-names></name></person-group> (<year>2019</year>). <article-title>The art of using t-SNE for single-cell transcriptomics.</article-title>
<source><italic>Nat. Commun.</italic></source>
<volume>10</volume>:<issue>5416</issue>. <pub-id pub-id-type="doi">10.1038/s41467-019-13056-x</pub-id>
<pub-id pub-id-type="pmid">31780648</pub-id></mixed-citation>
      </ref>
      <ref id="B57">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kollmorgen</surname><given-names>S.</given-names></name><name><surname>Hahnloser</surname><given-names>R. H. R.</given-names></name><name><surname>Mante</surname><given-names>V.</given-names></name></person-group> (<year>2020</year>). <article-title>Nearest neighbours reveal fast and slow components of motor learning.</article-title>
<source><italic>Nature</italic></source>
<volume>577</volume>
<fpage>526</fpage>–<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-019-1892-x</pub-id>
<pub-id pub-id-type="pmid">31915383</pub-id></mixed-citation>
      </ref>
      <ref id="B58">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krienen</surname><given-names>F. M.</given-names></name><name><surname>Yeo</surname><given-names>T. B.</given-names></name><name><surname>Buckner</surname><given-names>R. L.</given-names></name></person-group> (<year>2014</year>). <article-title>Reconfigurable task-dependent functional coupling modes cluster around a core functional architecture.</article-title>
<source><italic>Philos. Trans. R. Soc. B Biol. Sci.</italic></source>
<volume>369</volume>:<issue>20130526</issue>. <pub-id pub-id-type="doi">10.1098/rstb.2013.0526</pub-id>
<pub-id pub-id-type="pmid">25180304</pub-id></mixed-citation>
      </ref>
      <ref id="B59">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruskal</surname><given-names>J. B.</given-names></name></person-group> (<year>1964</year>). <article-title>Nonmetric multidimensional scaling: A numerical method.</article-title>
<source><italic>Psychometrika</italic></source>
<volume>29</volume>
<fpage>115</fpage>–<lpage>129</lpage>. <pub-id pub-id-type="doi">10.1007/bf02289694</pub-id></mixed-citation>
      </ref>
      <ref id="B60">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J. A.</given-names></name><name><surname>Peluffo-Ordóñez</surname><given-names>D. H.</given-names></name><name><surname>Verleysen</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Multi-scale similarities in stochastic neighbour embedding: Reducing dimensionality while preserving both local and global structure.</article-title>
<source><italic>Neurocomputing</italic></source>
<volume>169</volume>
<fpage>246</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2014.12.095</pub-id></mixed-citation>
      </ref>
      <ref id="B61">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardi</surname><given-names>N.</given-names></name><name><surname>Ville</surname><given-names>V. D.</given-names></name></person-group> (<year>2014</year>). <article-title>On spurious and real fluctuations of dynamic functional connectivity during rest.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>104</volume>
<fpage>430</fpage>–<lpage>436</lpage>.<pub-id pub-id-type="pmid">25234118</pub-id></mixed-citation>
      </ref>
      <ref id="B62">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Duyn</surname><given-names>J. H.</given-names></name></person-group> (<year>2013</year>). <article-title>Time-varying functional network information extracted from brief instances of spontaneous brain activity.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>110</volume>:<issue>1216856110</issue>. <pub-id pub-id-type="doi">10.1073/pnas.1216856110</pub-id>
<pub-id pub-id-type="pmid">23440216</pub-id></mixed-citation>
      </ref>
      <ref id="B63">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>N.</given-names></name><name><surname>Chang</surname><given-names>C.</given-names></name><name><surname>Duyn</surname><given-names>J. H.</given-names></name></person-group> (<year>2018</year>). <article-title>Co-activation patterns in resting-state fMRI signals.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>180</volume>
<fpage>485</fpage>–<lpage>494</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.041</pub-id>
<pub-id pub-id-type="pmid">29355767</pub-id></mixed-citation>
      </ref>
      <ref id="B64">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname><given-names>L. V. D.</given-names></name></person-group> (<year>2014</year>). <article-title>Accelerating T-SNE Using Tree-Based Algorithms.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>15</volume>
<fpage>3221</fpage>–<lpage>3245</lpage>.</mixed-citation>
      </ref>
      <ref id="B65">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Margulies</surname><given-names>D. S.</given-names></name><name><surname>Ghosh</surname><given-names>S. S.</given-names></name><name><surname>Goulas</surname><given-names>A.</given-names></name><name><surname>Falkiewicz</surname><given-names>M.</given-names></name><name><surname>Huntenburg</surname><given-names>J. M.</given-names></name><name><surname>Langs</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Situating the default-mode network along a principal gradient of macroscale cortical organization.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>113</volume>
<fpage>12574</fpage>–<lpage>12579</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1608282113</pub-id>
<pub-id pub-id-type="pmid">27791099</pub-id></mixed-citation>
      </ref>
      <ref id="B66">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsui</surname><given-names>T.</given-names></name><name><surname>Pham</surname><given-names>T. Q.</given-names></name><name><surname>Jimura</surname><given-names>K.</given-names></name><name><surname>Chikazoe</surname><given-names>J.</given-names></name></person-group> (<year>2022</year>). <article-title>On co-activation pattern analysis and non-stationarity of resting brain activity.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>249</volume>:<issue>118904</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.118904</pub-id>
<pub-id pub-id-type="pmid">35031473</pub-id></mixed-citation>
      </ref>
      <ref id="B67">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L.</given-names></name><name><surname>Healy</surname><given-names>J.</given-names></name><name><surname>Melville</surname><given-names>J.</given-names></name></person-group> (<year>2018</year>). <article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.</article-title>
<source><italic>arXiv</italic></source> [<comment>Preprint</comment>]. <pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></mixed-citation>
      </ref>
      <ref id="B68">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mckeown</surname><given-names>B.</given-names></name><name><surname>Strawson</surname><given-names>W. H.</given-names></name><name><surname>Wang</surname><given-names>H.-T.</given-names></name><name><surname>Karapanagiotidis</surname><given-names>T.</given-names></name><name><surname>de Wael</surname><given-names>R. V.</given-names></name><name><surname>Benkarim</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>The relationship between individual variation in macroscale functional gradients and distinct aspects of ongoing thought.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>220</volume>:<issue>117072</issue>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117072</pub-id>
<pub-id pub-id-type="pmid">32585346</pub-id></mixed-citation>
      </ref>
      <ref id="B69">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>C. M.</given-names></name><name><surname>Koenig</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>EEG microstates as a tool for studying the temporal dynamics of whole-brain neuronal networks: A review.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>180</volume>
<fpage>577</fpage>–<lpage>593</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.062</pub-id>
<pub-id pub-id-type="pmid">29196270</pub-id></mixed-citation>
      </ref>
      <ref id="B70">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>K. L.</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F.</given-names></name><name><surname>Bangerter</surname><given-names>N. K.</given-names></name><name><surname>Thomas</surname><given-names>D. L.</given-names></name><name><surname>Yacoub</surname><given-names>E.</given-names></name><name><surname>Xu</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Multimodal population brain imaging in the UK Biobank prospective epidemiological study.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>19</volume>
<fpage>1523</fpage>–<lpage>1536</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4393</pub-id>
<pub-id pub-id-type="pmid">27643430</pub-id></mixed-citation>
      </ref>
      <ref id="B71">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>R. L.</given-names></name><name><surname>Vergara</surname><given-names>V. M.</given-names></name><name><surname>Pearlson</surname><given-names>G. D.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name></person-group> (<year>2022</year>). <article-title>Multiframe Evolving Dynamic Functional Connectivity (EVOdFNC): A Method for Constructing and Investigating Functional Brain Motifs.</article-title>
<source><italic>Front. Neurosci.</italic></source>
<volume>16</volume>:<issue>770468</issue>. <pub-id pub-id-type="doi">10.3389/fnins.2022.770468</pub-id>
<pub-id pub-id-type="pmid">35516809</pub-id></mixed-citation>
      </ref>
      <ref id="B72">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mokhtari</surname><given-names>F.</given-names></name><name><surname>Mayhugh</surname><given-names>R. E.</given-names></name><name><surname>Hugenschmidt</surname><given-names>C. E.</given-names></name><name><surname>Rejeski</surname><given-names>W. J.</given-names></name><name><surname>Laurienti</surname><given-names>P. J.</given-names></name></person-group> (<year>2018a</year>). <article-title>Tensor-based vs. matrix-based rank reduction in dynamic brain connectivity.</article-title>
<source><italic>Med. Imag. 2018 Image Process</italic></source>
<volume>10574</volume>:<issue>105740Z</issue>. <pub-id pub-id-type="doi">10.1117/12.2293014</pub-id></mixed-citation>
      </ref>
      <ref id="B73">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mokhtari</surname><given-names>F.</given-names></name><name><surname>Rejeski</surname><given-names>W. J.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Wu</surname><given-names>G.</given-names></name><name><surname>Simpson</surname><given-names>S. L.</given-names></name><name><surname>Burdette</surname><given-names>J. H.</given-names></name><etal/></person-group> (<year>2018b</year>). <article-title>Dynamic fMRI networks predict success in a behavioral weight loss program among older adults.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>173</volume>
<fpage>421</fpage>–<lpage>433</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.025</pub-id>
<pub-id pub-id-type="pmid">29471100</pub-id></mixed-citation>
      </ref>
      <ref id="B74">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Michel</surname><given-names>V.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name><name><surname>Grisel</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Scikit-learn: Machine Learning in Python.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>12</volume>
<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
      </ref>
      <ref id="B75">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashid</surname><given-names>B.</given-names></name><name><surname>Arbabshirani</surname><given-names>M. R.</given-names></name><name><surname>Damaraju</surname><given-names>E.</given-names></name><name><surname>Cetin</surname><given-names>M. S.</given-names></name><name><surname>Miller</surname><given-names>R.</given-names></name><name><surname>Pearlson</surname><given-names>G. D.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Classification of schizophrenia and bipolar patients using static and dynamic resting-state fMRI brain connectivity.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>134</volume>
<fpage>645</fpage>–<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.051</pub-id>
<pub-id pub-id-type="pmid">27118088</pub-id></mixed-citation>
      </ref>
      <ref id="B76">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousseeuw</surname><given-names>P. J.</given-names></name></person-group> (<year>1987</year>). <article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis.</article-title>
<source><italic>J. Comput. Appl. Math.</italic></source>
<volume>20</volume>
<fpage>53</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1016/0377-0427(87)90125-7</pub-id></mixed-citation>
      </ref>
      <ref id="B77">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rué-Queralt</surname><given-names>J.</given-names></name><name><surname>Stevner</surname><given-names>A.</given-names></name><name><surname>Tagliazucchi</surname><given-names>E.</given-names></name><name><surname>Laufs</surname><given-names>H.</given-names></name><name><surname>Kringelbach</surname><given-names>M. L.</given-names></name><name><surname>Deco</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Decoding brain states on the intrinsic manifold of human brain dynamics across wakefulness and sleep.</article-title>
<source><italic>Commun. Biol.</italic></source>
<volume>4</volume>:<issue>854</issue>. <pub-id pub-id-type="doi">10.1038/s42003-021-02369-7</pub-id>
<pub-id pub-id-type="pmid">34244598</pub-id></mixed-citation>
      </ref>
      <ref id="B78">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saggar</surname><given-names>M.</given-names></name><name><surname>Sporns</surname><given-names>O.</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J.</given-names></name><name><surname>Bandettini</surname><given-names>P. A.</given-names></name><name><surname>Carlsson</surname><given-names>G.</given-names></name><name><surname>Glover</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Towards a new approach to reveal dynamical organization of the brain using topological data analysis.</article-title>
<source><italic>Nat. Commun.</italic></source>
<volume>9</volume>:<issue>1399</issue>. <pub-id pub-id-type="doi">10.1038/s41467-018-03664-4</pub-id>
<pub-id pub-id-type="pmid">29643350</pub-id></mixed-citation>
      </ref>
      <ref id="B79">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sizemore</surname><given-names>A. E.</given-names></name><name><surname>Phillips-Cremins</surname><given-names>J. E.</given-names></name><name><surname>Ghrist</surname><given-names>R.</given-names></name><name><surname>Bassett</surname><given-names>D. S.</given-names></name></person-group> (<year>2019</year>). <article-title>The importance of the whole: Topological data analysis for the network neuroscientist.</article-title>
<source><italic>Netw. Neurosci.</italic></source>
<volume>3</volume>
<fpage>656</fpage>–<lpage>673</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00073</pub-id>
<pub-id pub-id-type="pmid">31410372</pub-id></mixed-citation>
      </ref>
      <ref id="B80">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorscher</surname><given-names>B.</given-names></name><name><surname>Ganguli</surname><given-names>S.</given-names></name><name><surname>Sompolinsky</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>Neural representational geometry underlies few-shot concept learning.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>119</volume>:<issue>e2200800119</issue>. <pub-id pub-id-type="doi">10.1073/pnas.2200800119</pub-id>
<pub-id pub-id-type="pmid">36251997</pub-id></mixed-citation>
      </ref>
      <ref id="B81">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O.</given-names></name><name><surname>Honey</surname><given-names>C. J.</given-names></name></person-group> (<year>2006</year>). <article-title>Small worlds inside big brains.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U. S. A.</italic></source>
<volume>103</volume>
<fpage>19219</fpage>–<lpage>19220</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0609523103</pub-id>
<pub-id pub-id-type="pmid">17159140</pub-id></mixed-citation>
      </ref>
      <ref id="B82">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>P. A.</given-names></name><name><surname>Saad</surname><given-names>Z. S.</given-names></name></person-group> (<year>2013</year>). <article-title>FATCAT: (An Efficient) Functional And Tractographic Connectivity Analysis Toolbox.</article-title>
<source><italic>Brain Connect.</italic></source>
<volume>3</volume>
<fpage>523</fpage>–<lpage>535</lpage>. <pub-id pub-id-type="doi">10.1089/brain.2013.0154</pub-id>
<pub-id pub-id-type="pmid">23980912</pub-id></mixed-citation>
      </ref>
      <ref id="B83">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname><given-names>J. B.</given-names></name><name><surname>de Silva</surname><given-names>V.</given-names></name><name><surname>Langford</surname><given-names>J. C.</given-names></name></person-group> (<year>2000</year>). <article-title>A Global Geometric Framework for Nonlinear Dimensionality Reduction.</article-title>
<source><italic>Science</italic></source>
<volume>290</volume>
<fpage>2319</fpage>–<lpage>2323</lpage>. <pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id>
<pub-id pub-id-type="pmid">11125149</pub-id></mixed-citation>
      </ref>
      <ref id="B84">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>G. J.</given-names></name><name><surname>Pan</surname><given-names>W.-J. J.</given-names></name><name><surname>Magnuson</surname><given-names>M. E.</given-names></name><name><surname>Jaeger</surname><given-names>D.</given-names></name><name><surname>Keilholz</surname><given-names>S. D.</given-names></name></person-group> (<year>2014</year>). <article-title>Quasi-periodic patterns (QPP): large-scale dynamics in resting state fMRI that correlate with local infraslow electrical activity.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>84</volume>
<fpage>1018</fpage>–<lpage>1031</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.09.029</pub-id>
<pub-id pub-id-type="pmid">24071524</pub-id></mixed-citation>
      </ref>
      <ref id="B85">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Y.</given-names></name><name><surname>Margulies</surname><given-names>D. S.</given-names></name><name><surname>Breakspear</surname><given-names>M.</given-names></name><name><surname>Zalesky</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <article-title>Topographic organization of the human subcortex unveiled with functional connectivity gradients.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>23</volume>
<fpage>1421</fpage>–<lpage>1432</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-020-00711-6</pub-id>
<pub-id pub-id-type="pmid">32989295</pub-id></mixed-citation>
      </ref>
      <ref id="B86">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing Data using t-SNE.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>9</volume>
<fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
      </ref>
      <ref id="B87">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D.</given-names></name><name><surname>Abeysuriya</surname><given-names>R.</given-names></name><name><surname>Becker</surname><given-names>R.</given-names></name><name><surname>Quinn</surname><given-names>A. J.</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Discovering dynamic brain networks from big data in rest and task.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>180</volume>
<fpage>646</fpage>–<lpage>656</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.077</pub-id>
<pub-id pub-id-type="pmid">28669905</pub-id></mixed-citation>
      </ref>
      <ref id="B88">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vodrahalli</surname><given-names>K.</given-names></name><name><surname>Chen</surname><given-names>P.-H.</given-names></name><name><surname>Liang</surname><given-names>Y.</given-names></name><name><surname>Baldassano</surname><given-names>C.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Yong</surname><given-names>E.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Mapping between fMRI responses to movies and their natural language annotations.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>180</volume>
<fpage>223</fpage>–<lpage>231</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.042</pub-id>
<pub-id pub-id-type="pmid">28648889</pub-id></mixed-citation>
      </ref>
      <ref id="B89">
        <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name></person-group> (<year>2014</year>). “<article-title>Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction</article-title>,” in <source><italic>Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Work</italic></source>, <publisher-loc>Manhattan, NY</publisher-loc>, <pub-id pub-id-type="doi">10.1109/cvprw.2014.79</pub-id></mixed-citation>
      </ref>
      <ref id="B90">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T.</given-names></name><name><surname>Poldrack</surname><given-names>R. A.</given-names></name><name><surname>Nichols</surname><given-names>T. E.</given-names></name><name><surname>Essen</surname><given-names>D. C.</given-names></name><name><surname>Wager</surname><given-names>T. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data.</article-title>
<source><italic>Nat. Methods</italic></source>
<volume>8</volume>:<issue>665</issue>.</mixed-citation>
      </ref>
      <ref id="B91">
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeisel</surname><given-names>A.</given-names></name><name><surname>Hochgerner</surname><given-names>H.</given-names></name><name><surname>Lönnerberg</surname><given-names>P.</given-names></name><name><surname>Johnsson</surname><given-names>A.</given-names></name><name><surname>Memic</surname><given-names>F.</given-names></name><name><surname>van der Zwan</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Molecular Architecture of the Mouse Nervous System.</article-title>
<source><italic>Cell</italic></source>
<volume>174</volume>
<fpage>999</fpage>–<lpage>1014.e22</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.06.021</pub-id>
<pub-id pub-id-type="pmid">30096314</pub-id></mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
